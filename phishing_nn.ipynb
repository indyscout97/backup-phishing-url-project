{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "phishing_nn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "SMsmB1OqoQO6",
        "pxhItR4mUlyi",
        "ZUuo04VLzBkB",
        "Vi_xm1LgUlOg",
        "XEf-TNa9UueX",
        "AD-tbKBauWhl",
        "nWW3TtqRoQPH",
        "fEE8ajVx5v0-",
        "khptwsq3oQPJ",
        "Jq7E6v5l56Zn",
        "14HsrzL2oQPK",
        "AVrPsUyf5_lf",
        "uYKWx22aoQPL",
        "-zpUqUyS6EGW",
        "Ld35dvSMoQPN",
        "-FIKr-RX6M8V",
        "h_xzBJWRoQPO",
        "UL2R3T7WdCN0",
        "rrn-vv5GfHiG",
        "fyBh_Ph_dXnT",
        "PO5Qlco9oVi7",
        "iCvgAZwFdiUw",
        "NvD349puz8EZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMsmB1OqoQO6"
      },
      "source": [
        "---\n",
        "\n",
        "# Keras Sequential Neural Network to predict the probability of a phishing URL\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7UjcFFKd7XM2",
        "outputId": "b320499b-fb46-4568-e56f-0f05d9ff51de"
      },
      "source": [
        "#load packages and libraries\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, cross_val_predict\n",
        "import sklearn as sk\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import *\n",
        "from keras.utils.np_utils import *\n",
        "from sklearn.pipeline import Pipeline\n",
        "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.models import model_from_json\n",
        "\n",
        "\n",
        "print(\"Done Loading\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done Loading\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZJiz6GaoQO9"
      },
      "source": [
        "## load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "_p0zv7mq7p3R",
        "outputId": "a43c42a0-1cf7-4406-e6ea-26ac2e736279"
      },
      "source": [
        "full_df = pd.read_csv(\"https://raw.githubusercontent.com/jwaldroop/phishing-url-project/main/dataset_full.csv\")\n",
        "\n",
        "full_df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_underline_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>qty_questionmark_url</th>\n",
              "      <th>qty_equal_url</th>\n",
              "      <th>qty_at_url</th>\n",
              "      <th>qty_and_url</th>\n",
              "      <th>qty_exclamation_url</th>\n",
              "      <th>qty_space_url</th>\n",
              "      <th>qty_tilde_url</th>\n",
              "      <th>qty_comma_url</th>\n",
              "      <th>qty_plus_url</th>\n",
              "      <th>qty_asterisk_url</th>\n",
              "      <th>qty_hashtag_url</th>\n",
              "      <th>qty_dollar_url</th>\n",
              "      <th>qty_percent_url</th>\n",
              "      <th>qty_tld_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_hyphen_domain</th>\n",
              "      <th>qty_underline_domain</th>\n",
              "      <th>qty_slash_domain</th>\n",
              "      <th>qty_questionmark_domain</th>\n",
              "      <th>qty_equal_domain</th>\n",
              "      <th>qty_at_domain</th>\n",
              "      <th>qty_and_domain</th>\n",
              "      <th>qty_exclamation_domain</th>\n",
              "      <th>qty_space_domain</th>\n",
              "      <th>qty_tilde_domain</th>\n",
              "      <th>qty_comma_domain</th>\n",
              "      <th>qty_plus_domain</th>\n",
              "      <th>qty_asterisk_domain</th>\n",
              "      <th>qty_hashtag_domain</th>\n",
              "      <th>qty_dollar_domain</th>\n",
              "      <th>qty_percent_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>domain_in_ip</th>\n",
              "      <th>server_client_domain</th>\n",
              "      <th>...</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>qty_dot_params</th>\n",
              "      <th>qty_hyphen_params</th>\n",
              "      <th>qty_underline_params</th>\n",
              "      <th>qty_slash_params</th>\n",
              "      <th>qty_questionmark_params</th>\n",
              "      <th>qty_equal_params</th>\n",
              "      <th>qty_at_params</th>\n",
              "      <th>qty_and_params</th>\n",
              "      <th>qty_exclamation_params</th>\n",
              "      <th>qty_space_params</th>\n",
              "      <th>qty_tilde_params</th>\n",
              "      <th>qty_comma_params</th>\n",
              "      <th>qty_plus_params</th>\n",
              "      <th>qty_asterisk_params</th>\n",
              "      <th>qty_hashtag_params</th>\n",
              "      <th>qty_dollar_params</th>\n",
              "      <th>qty_percent_params</th>\n",
              "      <th>params_length</th>\n",
              "      <th>tld_present_params</th>\n",
              "      <th>qty_params</th>\n",
              "      <th>email_in_url</th>\n",
              "      <th>time_response</th>\n",
              "      <th>domain_spf</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "      <th>url_google_index</th>\n",
              "      <th>domain_google_index</th>\n",
              "      <th>url_shortened</th>\n",
              "      <th>phishing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>25</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.207316</td>\n",
              "      <td>0</td>\n",
              "      <td>60781</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>892</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>223</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0.499566</td>\n",
              "      <td>-1</td>\n",
              "      <td>36024</td>\n",
              "      <td>579</td>\n",
              "      <td>150</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>9540</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>15</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.935901</td>\n",
              "      <td>0</td>\n",
              "      <td>4766</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>589</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>81</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.410021</td>\n",
              "      <td>0</td>\n",
              "      <td>20454</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>292</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0.410761</td>\n",
              "      <td>0</td>\n",
              "      <td>53831</td>\n",
              "      <td>6998</td>\n",
              "      <td>306</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3597</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 112 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   qty_dot_url  qty_hyphen_url  ...  url_shortened  phishing\n",
              "0            3               0  ...              0         1\n",
              "1            5               0  ...              0         1\n",
              "2            2               0  ...              0         0\n",
              "3            4               0  ...              0         1\n",
              "4            2               0  ...              0         0\n",
              "\n",
              "[5 rows x 112 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9kCXX7hrQ1x",
        "outputId": "6c8ee41f-46d3-49ab-873d-3953fa1c2dfd"
      },
      "source": [
        "len(full_df[full_df.phishing == 1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30647"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2PSfxIrrngk",
        "outputId": "a6bd3742-3035-481a-8a96-d933f1ac010d"
      },
      "source": [
        "len(full_df[full_df.phishing != 1])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "58000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxhItR4mUlyi"
      },
      "source": [
        "---\n",
        "\n",
        "# unit testing to remove negative values (there cannot be a negative url length, etc.)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KXuFC-sUk-e"
      },
      "source": [
        "# Noticed a discrepancy in the data, some values are recorded as -1 even though it makes not practical sense, i.e. you can't have a negative quantity of a character\n",
        "# This changes all -1 to 0\n",
        "full_df.dtypes == 'int64'\n",
        "\n",
        "def remove_negatives(full_df):\n",
        "    full_df[full_df == -1] = 0\n",
        "remove_negatives(full_df)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYB4m2v1Uk7i"
      },
      "source": [
        "# unit test \n",
        "is_it_working = []\n",
        "def data_cleaning_unit_test(column):\n",
        "    did_it_work =  {'Yes':0 , 'No':0}\n",
        "    for i in column:\n",
        "        if i >= 0:\n",
        "            did_it_work['Yes'] += 1\n",
        "        elif i <0:\n",
        "            did_it_work['No'] += 1\n",
        "    if did_it_work['No'] > 0:\n",
        "        print(column.name,'=', 'Not working')\n",
        "    else:\n",
        "        print(column.name,'=', 'It worked!')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gce-FFzDU9vL",
        "outputId": "b2dd8c84-660e-4cee-a0a2-5c8ea8b90586"
      },
      "source": [
        "for col in full_df.columns.tolist():\n",
        "    data_cleaning_unit_test(full_df[col])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "qty_dot_url = It worked!\n",
            "qty_hyphen_url = It worked!\n",
            "qty_underline_url = It worked!\n",
            "qty_slash_url = It worked!\n",
            "qty_questionmark_url = It worked!\n",
            "qty_equal_url = It worked!\n",
            "qty_at_url = It worked!\n",
            "qty_and_url = It worked!\n",
            "qty_exclamation_url = It worked!\n",
            "qty_space_url = It worked!\n",
            "qty_tilde_url = It worked!\n",
            "qty_comma_url = It worked!\n",
            "qty_plus_url = It worked!\n",
            "qty_asterisk_url = It worked!\n",
            "qty_hashtag_url = It worked!\n",
            "qty_dollar_url = It worked!\n",
            "qty_percent_url = It worked!\n",
            "qty_tld_url = It worked!\n",
            "length_url = It worked!\n",
            "qty_dot_domain = It worked!\n",
            "qty_hyphen_domain = It worked!\n",
            "qty_underline_domain = It worked!\n",
            "qty_slash_domain = It worked!\n",
            "qty_questionmark_domain = It worked!\n",
            "qty_equal_domain = It worked!\n",
            "qty_at_domain = It worked!\n",
            "qty_and_domain = It worked!\n",
            "qty_exclamation_domain = It worked!\n",
            "qty_space_domain = It worked!\n",
            "qty_tilde_domain = It worked!\n",
            "qty_comma_domain = It worked!\n",
            "qty_plus_domain = It worked!\n",
            "qty_asterisk_domain = It worked!\n",
            "qty_hashtag_domain = It worked!\n",
            "qty_dollar_domain = It worked!\n",
            "qty_percent_domain = It worked!\n",
            "qty_vowels_domain = It worked!\n",
            "domain_length = It worked!\n",
            "domain_in_ip = It worked!\n",
            "server_client_domain = It worked!\n",
            "qty_dot_directory = It worked!\n",
            "qty_hyphen_directory = It worked!\n",
            "qty_underline_directory = It worked!\n",
            "qty_slash_directory = It worked!\n",
            "qty_questionmark_directory = It worked!\n",
            "qty_equal_directory = It worked!\n",
            "qty_at_directory = It worked!\n",
            "qty_and_directory = It worked!\n",
            "qty_exclamation_directory = It worked!\n",
            "qty_space_directory = It worked!\n",
            "qty_tilde_directory = It worked!\n",
            "qty_comma_directory = It worked!\n",
            "qty_plus_directory = It worked!\n",
            "qty_asterisk_directory = It worked!\n",
            "qty_hashtag_directory = It worked!\n",
            "qty_dollar_directory = It worked!\n",
            "qty_percent_directory = It worked!\n",
            "directory_length = It worked!\n",
            "qty_dot_file = It worked!\n",
            "qty_hyphen_file = It worked!\n",
            "qty_underline_file = It worked!\n",
            "qty_slash_file = It worked!\n",
            "qty_questionmark_file = It worked!\n",
            "qty_equal_file = It worked!\n",
            "qty_at_file = It worked!\n",
            "qty_and_file = It worked!\n",
            "qty_exclamation_file = It worked!\n",
            "qty_space_file = It worked!\n",
            "qty_tilde_file = It worked!\n",
            "qty_comma_file = It worked!\n",
            "qty_plus_file = It worked!\n",
            "qty_asterisk_file = It worked!\n",
            "qty_hashtag_file = It worked!\n",
            "qty_dollar_file = It worked!\n",
            "qty_percent_file = It worked!\n",
            "file_length = It worked!\n",
            "qty_dot_params = It worked!\n",
            "qty_hyphen_params = It worked!\n",
            "qty_underline_params = It worked!\n",
            "qty_slash_params = It worked!\n",
            "qty_questionmark_params = It worked!\n",
            "qty_equal_params = It worked!\n",
            "qty_at_params = It worked!\n",
            "qty_and_params = It worked!\n",
            "qty_exclamation_params = It worked!\n",
            "qty_space_params = It worked!\n",
            "qty_tilde_params = It worked!\n",
            "qty_comma_params = It worked!\n",
            "qty_plus_params = It worked!\n",
            "qty_asterisk_params = It worked!\n",
            "qty_hashtag_params = It worked!\n",
            "qty_dollar_params = It worked!\n",
            "qty_percent_params = It worked!\n",
            "params_length = It worked!\n",
            "tld_present_params = It worked!\n",
            "qty_params = It worked!\n",
            "email_in_url = It worked!\n",
            "time_response = It worked!\n",
            "domain_spf = It worked!\n",
            "asn_ip = It worked!\n",
            "time_domain_activation = It worked!\n",
            "time_domain_expiration = It worked!\n",
            "qty_ip_resolved = It worked!\n",
            "qty_nameservers = It worked!\n",
            "qty_mx_servers = It worked!\n",
            "ttl_hostname = It worked!\n",
            "tls_ssl_certificate = It worked!\n",
            "qty_redirects = It worked!\n",
            "url_google_index = It worked!\n",
            "domain_google_index = It worked!\n",
            "url_shortened = It worked!\n",
            "phishing = It worked!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "jMYAbOAXPXiu",
        "outputId": "789ad4e1-0806-42ff-929a-f473b10e03ae"
      },
      "source": [
        "full_df.describe()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_underline_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>qty_questionmark_url</th>\n",
              "      <th>qty_equal_url</th>\n",
              "      <th>qty_at_url</th>\n",
              "      <th>qty_and_url</th>\n",
              "      <th>qty_exclamation_url</th>\n",
              "      <th>qty_space_url</th>\n",
              "      <th>qty_tilde_url</th>\n",
              "      <th>qty_comma_url</th>\n",
              "      <th>qty_plus_url</th>\n",
              "      <th>qty_asterisk_url</th>\n",
              "      <th>qty_hashtag_url</th>\n",
              "      <th>qty_dollar_url</th>\n",
              "      <th>qty_percent_url</th>\n",
              "      <th>qty_tld_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_hyphen_domain</th>\n",
              "      <th>qty_underline_domain</th>\n",
              "      <th>qty_slash_domain</th>\n",
              "      <th>qty_questionmark_domain</th>\n",
              "      <th>qty_equal_domain</th>\n",
              "      <th>qty_at_domain</th>\n",
              "      <th>qty_and_domain</th>\n",
              "      <th>qty_exclamation_domain</th>\n",
              "      <th>qty_space_domain</th>\n",
              "      <th>qty_tilde_domain</th>\n",
              "      <th>qty_comma_domain</th>\n",
              "      <th>qty_plus_domain</th>\n",
              "      <th>qty_asterisk_domain</th>\n",
              "      <th>qty_hashtag_domain</th>\n",
              "      <th>qty_dollar_domain</th>\n",
              "      <th>qty_percent_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>domain_in_ip</th>\n",
              "      <th>server_client_domain</th>\n",
              "      <th>...</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>qty_dot_params</th>\n",
              "      <th>qty_hyphen_params</th>\n",
              "      <th>qty_underline_params</th>\n",
              "      <th>qty_slash_params</th>\n",
              "      <th>qty_questionmark_params</th>\n",
              "      <th>qty_equal_params</th>\n",
              "      <th>qty_at_params</th>\n",
              "      <th>qty_and_params</th>\n",
              "      <th>qty_exclamation_params</th>\n",
              "      <th>qty_space_params</th>\n",
              "      <th>qty_tilde_params</th>\n",
              "      <th>qty_comma_params</th>\n",
              "      <th>qty_plus_params</th>\n",
              "      <th>qty_asterisk_params</th>\n",
              "      <th>qty_hashtag_params</th>\n",
              "      <th>qty_dollar_params</th>\n",
              "      <th>qty_percent_params</th>\n",
              "      <th>params_length</th>\n",
              "      <th>tld_present_params</th>\n",
              "      <th>qty_params</th>\n",
              "      <th>email_in_url</th>\n",
              "      <th>time_response</th>\n",
              "      <th>domain_spf</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "      <th>url_google_index</th>\n",
              "      <th>domain_google_index</th>\n",
              "      <th>url_shortened</th>\n",
              "      <th>phishing</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.00000</td>\n",
              "      <td>88647.0</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "      <td>88647.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2.191343</td>\n",
              "      <td>0.328810</td>\n",
              "      <td>0.113879</td>\n",
              "      <td>1.281781</td>\n",
              "      <td>0.009329</td>\n",
              "      <td>0.205861</td>\n",
              "      <td>0.022133</td>\n",
              "      <td>0.140885</td>\n",
              "      <td>0.002944</td>\n",
              "      <td>0.001015</td>\n",
              "      <td>0.003226</td>\n",
              "      <td>0.002166</td>\n",
              "      <td>0.002786</td>\n",
              "      <td>0.004535</td>\n",
              "      <td>0.000508</td>\n",
              "      <td>0.001895</td>\n",
              "      <td>0.107505</td>\n",
              "      <td>1.047480</td>\n",
              "      <td>36.347615</td>\n",
              "      <td>1.870622</td>\n",
              "      <td>0.114578</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.440590</td>\n",
              "      <td>18.560820</td>\n",
              "      <td>0.002267</td>\n",
              "      <td>0.004501</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.037192</td>\n",
              "      <td>3.279727</td>\n",
              "      <td>0.105181</td>\n",
              "      <td>0.037666</td>\n",
              "      <td>0.054023</td>\n",
              "      <td>0.028145</td>\n",
              "      <td>0.008449</td>\n",
              "      <td>0.189019</td>\n",
              "      <td>0.018027</td>\n",
              "      <td>0.127449</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>0.001636</td>\n",
              "      <td>0.00009</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000384</td>\n",
              "      <td>0.054170</td>\n",
              "      <td>6.189459</td>\n",
              "      <td>0.024536</td>\n",
              "      <td>0.156531</td>\n",
              "      <td>0.018331</td>\n",
              "      <td>0.859983</td>\n",
              "      <td>0.147642</td>\n",
              "      <td>31131.225783</td>\n",
              "      <td>3389.951391</td>\n",
              "      <td>352.338917</td>\n",
              "      <td>1.186797</td>\n",
              "      <td>2.772412</td>\n",
              "      <td>1.742428</td>\n",
              "      <td>6159.927758</td>\n",
              "      <td>0.506447</td>\n",
              "      <td>0.422293</td>\n",
              "      <td>0.002425</td>\n",
              "      <td>0.003012</td>\n",
              "      <td>0.005482</td>\n",
              "      <td>0.345720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.235636</td>\n",
              "      <td>1.119286</td>\n",
              "      <td>0.657767</td>\n",
              "      <td>1.893929</td>\n",
              "      <td>0.112568</td>\n",
              "      <td>0.954272</td>\n",
              "      <td>0.279652</td>\n",
              "      <td>0.924864</td>\n",
              "      <td>0.087341</td>\n",
              "      <td>0.072653</td>\n",
              "      <td>0.078127</td>\n",
              "      <td>0.075968</td>\n",
              "      <td>0.110904</td>\n",
              "      <td>0.301651</td>\n",
              "      <td>0.061655</td>\n",
              "      <td>0.099730</td>\n",
              "      <td>1.722625</td>\n",
              "      <td>0.254755</td>\n",
              "      <td>46.191590</td>\n",
              "      <td>0.705607</td>\n",
              "      <td>0.421957</td>\n",
              "      <td>0.032381</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003359</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.541244</td>\n",
              "      <td>6.598694</td>\n",
              "      <td>0.047564</td>\n",
              "      <td>0.066939</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.335428</td>\n",
              "      <td>13.432864</td>\n",
              "      <td>0.830999</td>\n",
              "      <td>0.505227</td>\n",
              "      <td>0.504926</td>\n",
              "      <td>0.411940</td>\n",
              "      <td>0.105608</td>\n",
              "      <td>0.900987</td>\n",
              "      <td>0.139828</td>\n",
              "      <td>0.849584</td>\n",
              "      <td>0.047019</td>\n",
              "      <td>0.014640</td>\n",
              "      <td>0.007510</td>\n",
              "      <td>0.063444</td>\n",
              "      <td>0.064147</td>\n",
              "      <td>0.01502</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032561</td>\n",
              "      <td>0.963131</td>\n",
              "      <td>34.773196</td>\n",
              "      <td>0.154706</td>\n",
              "      <td>0.722705</td>\n",
              "      <td>0.134147</td>\n",
              "      <td>1.407413</td>\n",
              "      <td>0.354747</td>\n",
              "      <td>45261.452420</td>\n",
              "      <td>3043.859735</td>\n",
              "      <td>598.090471</td>\n",
              "      <td>0.796456</td>\n",
              "      <td>1.322999</td>\n",
              "      <td>1.706705</td>\n",
              "      <td>11465.556814</td>\n",
              "      <td>0.499961</td>\n",
              "      <td>0.689952</td>\n",
              "      <td>0.049188</td>\n",
              "      <td>0.054799</td>\n",
              "      <td>0.073841</td>\n",
              "      <td>0.475605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>14.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.241038</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>13335.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>292.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.466492</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>20013.000000</td>\n",
              "      <td>3046.000000</td>\n",
              "      <td>168.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>2029.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.874495</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>34922.000000</td>\n",
              "      <td>6423.000000</td>\n",
              "      <td>354.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>10798.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>24.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>44.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>12.000000</td>\n",
              "      <td>4165.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>61.000000</td>\n",
              "      <td>231.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>174.000000</td>\n",
              "      <td>1232.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>43.000000</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>11.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>4.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>4094.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>23.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>38.402411</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>395754.000000</td>\n",
              "      <td>17775.000000</td>\n",
              "      <td>22574.000000</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>604800.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 112 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        qty_dot_url  qty_hyphen_url  ...  url_shortened      phishing\n",
              "count  88647.000000    88647.000000  ...   88647.000000  88647.000000\n",
              "mean       2.191343        0.328810  ...       0.005482      0.345720\n",
              "std        1.235636        1.119286  ...       0.073841      0.475605\n",
              "min        1.000000        0.000000  ...       0.000000      0.000000\n",
              "25%        2.000000        0.000000  ...       0.000000      0.000000\n",
              "50%        2.000000        0.000000  ...       0.000000      0.000000\n",
              "75%        2.000000        0.000000  ...       0.000000      1.000000\n",
              "max       24.000000       35.000000  ...       1.000000      1.000000\n",
              "\n",
              "[8 rows x 112 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUuo04VLzBkB"
      },
      "source": [
        "---\n",
        "\n",
        "# NN on entire dataset\n",
        "\n",
        "### (this can take a while to run depending on your computer resources)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "Uyt5-sN-7p1H",
        "outputId": "84d0abc1-bab5-49ae-89de-0f68874238ef"
      },
      "source": [
        "y = full_df.iloc[:,-1]\n",
        "\n",
        "X = full_df.iloc[:,0:111]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.25, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_underline_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>qty_questionmark_url</th>\n",
              "      <th>qty_equal_url</th>\n",
              "      <th>qty_at_url</th>\n",
              "      <th>qty_and_url</th>\n",
              "      <th>qty_exclamation_url</th>\n",
              "      <th>qty_space_url</th>\n",
              "      <th>qty_tilde_url</th>\n",
              "      <th>qty_comma_url</th>\n",
              "      <th>qty_plus_url</th>\n",
              "      <th>qty_asterisk_url</th>\n",
              "      <th>qty_hashtag_url</th>\n",
              "      <th>qty_dollar_url</th>\n",
              "      <th>qty_percent_url</th>\n",
              "      <th>qty_tld_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_hyphen_domain</th>\n",
              "      <th>qty_underline_domain</th>\n",
              "      <th>qty_slash_domain</th>\n",
              "      <th>qty_questionmark_domain</th>\n",
              "      <th>qty_equal_domain</th>\n",
              "      <th>qty_at_domain</th>\n",
              "      <th>qty_and_domain</th>\n",
              "      <th>qty_exclamation_domain</th>\n",
              "      <th>qty_space_domain</th>\n",
              "      <th>qty_tilde_domain</th>\n",
              "      <th>qty_comma_domain</th>\n",
              "      <th>qty_plus_domain</th>\n",
              "      <th>qty_asterisk_domain</th>\n",
              "      <th>qty_hashtag_domain</th>\n",
              "      <th>qty_dollar_domain</th>\n",
              "      <th>qty_percent_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>domain_in_ip</th>\n",
              "      <th>server_client_domain</th>\n",
              "      <th>...</th>\n",
              "      <th>qty_asterisk_file</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>qty_dot_params</th>\n",
              "      <th>qty_hyphen_params</th>\n",
              "      <th>qty_underline_params</th>\n",
              "      <th>qty_slash_params</th>\n",
              "      <th>qty_questionmark_params</th>\n",
              "      <th>qty_equal_params</th>\n",
              "      <th>qty_at_params</th>\n",
              "      <th>qty_and_params</th>\n",
              "      <th>qty_exclamation_params</th>\n",
              "      <th>qty_space_params</th>\n",
              "      <th>qty_tilde_params</th>\n",
              "      <th>qty_comma_params</th>\n",
              "      <th>qty_plus_params</th>\n",
              "      <th>qty_asterisk_params</th>\n",
              "      <th>qty_hashtag_params</th>\n",
              "      <th>qty_dollar_params</th>\n",
              "      <th>qty_percent_params</th>\n",
              "      <th>params_length</th>\n",
              "      <th>tld_present_params</th>\n",
              "      <th>qty_params</th>\n",
              "      <th>email_in_url</th>\n",
              "      <th>time_response</th>\n",
              "      <th>domain_spf</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "      <th>url_google_index</th>\n",
              "      <th>domain_google_index</th>\n",
              "      <th>url_shortened</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999934</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.011480</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.730210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.680199</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.698307</td>\n",
              "      <td>0.269674</td>\n",
              "      <td>0.016112</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.662860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.859057</td>\n",
              "      <td>0.261792</td>\n",
              "      <td>0.005926</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.439823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.795440</td>\n",
              "      <td>0.178787</td>\n",
              "      <td>0.007212</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.579014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 111 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_url  qty_hyphen_url  ...  domain_google_index  url_shortened\n",
              "5676      0.000004             0.0  ...                  0.0            0.0\n",
              "39002     0.000777             0.0  ...                  0.0            0.0\n",
              "1732      0.000092             0.0  ...                  0.0            0.0\n",
              "39668     0.000122             0.0  ...                  0.0            0.0\n",
              "82035     0.000054             0.0  ...                  0.0            0.0\n",
              "\n",
              "[5 rows x 111 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8L1OV6fg7py7",
        "outputId": "c0b6f0bf-129f-49dc-d2e3-af42ab1a9986"
      },
      "source": [
        "tf.keras.backend.clear_session() #clear tensorflow backend to help with memory leakage\n",
        "\n",
        "nn_mod_1 = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[111]),                   # first layer - input - number of columns in X\n",
        "                          layers.Dense(units=64, activation='relu'),              # second layer - first dense layer - 64 output neurons w/ rectified linear unit activation\n",
        "                          layers.Dropout(0.2),                                    # third layer - first dropout layer - drop 20% of neurons\n",
        "                          layers.Dense(units=64, activation='relu'),              # fourth layer - second dense layer - 64 output neurons w/ ReLU activation\n",
        "                          layers.Dropout(0.2),                                    # fifth layer - second dropout layer - drop 20% of neurons \n",
        "                          layers.Dense(units=50, activation='relu'),              # sixth layer -  third dense layer - 50 output neurons w/ ReLU activation\n",
        "                          layers.Dropout(0.20),                                   # seventh layer - third dropout layer - drop 20% of neurons\n",
        "                          layers.Dense(units=32, activation='relu'),              # eighth layer - fourth dense layer - 32 output neurons w/ ReLU activation \n",
        "                          layers.Dropout(0.2),                                    # ninth layer - fourth dropout layer - drop 20% of neurons\n",
        "                          layers.Dense(units=32, activation='relu'),              # tenth layer - fifth dense layer - 32 output neurons w/ ReLU activation\n",
        "                          layers.Dropout(0.2),                                    # eleventh layer - fifth dropout layer - drop 20% of neurons\n",
        "                          layers.Dense(units=16, activation='relu'),              # twelfth layer - sixth dense layer - 16 output neurons w/ ReLU activation\n",
        "                          layers.Dropout(0.40),                                   # thirteenth layer - sixth dropout layer - drop 40% of neurons\n",
        "                          layers.Dense(units=16, activation='relu'),              # fourteenth layer - seventh dense layer - 16 output neurons w/ ReLU activation\n",
        "                          layers.Dropout(0.40),                                   # fifteenth layer - seventh dropout layer - drop 40% of neurons\n",
        "                          layers.Dense(units=111, activation='relu'),             # sixteenth layer - eighth dense layer - 111 output neurons w/ ReLU activation\n",
        "                          layers.Flatten(),                                       # seventeenth layer - flatten - adds column to X for predictions to fit\n",
        "                          layers.Dense(units=1, activation='sigmoid')             # eighteenth layer - output layer - 1 output neuron w/ sigmoid activation (preds <> 0, 1)\n",
        "])\n",
        "\n",
        "nn_mod_1.compile(\n",
        "    optimizer='adam', #best optimizer for noise, easy computation\n",
        "    loss='binary_crossentropy', #crossentropy between preds and val_y; preds are between 0 & 1 so we use binary\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), #binary accuracy between preds and val_y with 0.5 threshold\n",
        "             tf.keras.metrics.AUC(), # measuring AUC (obviously) through internal measures of TP, TN, FP, FN\n",
        "             ]\n",
        ")\n",
        "\n",
        "# early stopping callback will monitor the validation data's binary accuracy for it's maximum value\n",
        "# & 25 epochs after reaching the maximum value, the model will stop running and restore the best layer weights measured.\n",
        "# I don't actually want this to run for anywhere near 500 epochs but this gives it the flexibility to learn as long as needed\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "# this will output a table of the layers, each layer output shape, and the parameters measured by the model\n",
        "nn_mod_1.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                7168      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 20,065\n",
            "Trainable params: 20,065\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umxFYgEP7pwr",
        "scrolled": true,
        "outputId": "0dddeca1-ba32-41d9-e67b-fb5578b662c0"
      },
      "source": [
        "history1 = nn_mod_1.fit(train_X, train_y, validation_split=0.30, shuffle=True, batch_size= 175, epochs=500, callbacks = [earlystopping], workers=8)\n",
        "\n",
        "# this fit method does not use the original validation split (25%) during model fitting to allow for more accurate evaluation and preditions later on\n",
        "# the 75% training split (train_X, train_y) is split 70/30 during model fitting and each epoch the 70/30 split is, well, shuffled\n",
        "# and there are 8 workers to *hopefully* help it run faster, in addition to the early stopping callback being applied"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 4s 9ms/step - loss: 0.6275 - binary_accuracy: 0.6548 - auc: 0.6108 - val_loss: 0.5662 - val_binary_accuracy: 0.7166 - val_auc: 0.7632\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5572 - binary_accuracy: 0.7092 - auc: 0.7493 - val_loss: 0.5633 - val_binary_accuracy: 0.7282 - val_auc: 0.7710\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5441 - binary_accuracy: 0.7189 - auc: 0.7582 - val_loss: 0.5480 - val_binary_accuracy: 0.7372 - val_auc: 0.7884\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5215 - binary_accuracy: 0.7340 - auc: 0.7840 - val_loss: 0.4894 - val_binary_accuracy: 0.7762 - val_auc: 0.8533\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4658 - binary_accuracy: 0.7745 - auc: 0.8357 - val_loss: 0.4422 - val_binary_accuracy: 0.8233 - val_auc: 0.9033\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4227 - binary_accuracy: 0.8091 - auc: 0.8788 - val_loss: 0.5386 - val_binary_accuracy: 0.7603 - val_auc: 0.8851\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3907 - binary_accuracy: 0.8351 - auc: 0.9016 - val_loss: 0.4385 - val_binary_accuracy: 0.8168 - val_auc: 0.9356\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3734 - binary_accuracy: 0.8458 - auc: 0.9110 - val_loss: 0.3809 - val_binary_accuracy: 0.8553 - val_auc: 0.9417\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3391 - binary_accuracy: 0.8662 - auc: 0.9269 - val_loss: 0.3938 - val_binary_accuracy: 0.9024 - val_auc: 0.9568\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3180 - binary_accuracy: 0.8746 - auc: 0.9336 - val_loss: 0.3824 - val_binary_accuracy: 0.8956 - val_auc: 0.9564\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3086 - binary_accuracy: 0.8793 - auc: 0.9360 - val_loss: 0.4854 - val_binary_accuracy: 0.8047 - val_auc: 0.9548\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3018 - binary_accuracy: 0.8808 - auc: 0.9377 - val_loss: 0.3574 - val_binary_accuracy: 0.8911 - val_auc: 0.9564\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2922 - binary_accuracy: 0.8856 - auc: 0.9425 - val_loss: 0.3676 - val_binary_accuracy: 0.9024 - val_auc: 0.9613\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2927 - binary_accuracy: 0.8841 - auc: 0.9406 - val_loss: 0.2862 - val_binary_accuracy: 0.8990 - val_auc: 0.9614\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2775 - binary_accuracy: 0.8929 - auc: 0.9463 - val_loss: 0.3057 - val_binary_accuracy: 0.9077 - val_auc: 0.9609\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2758 - binary_accuracy: 0.8907 - auc: 0.9484 - val_loss: 0.3730 - val_binary_accuracy: 0.9079 - val_auc: 0.9619\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2856 - binary_accuracy: 0.8878 - auc: 0.9435 - val_loss: 0.2793 - val_binary_accuracy: 0.9168 - val_auc: 0.9625\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2734 - binary_accuracy: 0.8913 - auc: 0.9476 - val_loss: 0.3538 - val_binary_accuracy: 0.9119 - val_auc: 0.9599\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2815 - binary_accuracy: 0.8865 - auc: 0.9450 - val_loss: 0.3157 - val_binary_accuracy: 0.8965 - val_auc: 0.9627\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2750 - binary_accuracy: 0.8911 - auc: 0.9470 - val_loss: 0.3131 - val_binary_accuracy: 0.9064 - val_auc: 0.9663\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2653 - binary_accuracy: 0.8931 - auc: 0.9517 - val_loss: 0.3525 - val_binary_accuracy: 0.9125 - val_auc: 0.9616\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2756 - binary_accuracy: 0.8910 - auc: 0.9471 - val_loss: 0.2706 - val_binary_accuracy: 0.9118 - val_auc: 0.9644\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2590 - binary_accuracy: 0.8976 - auc: 0.9527 - val_loss: 0.2921 - val_binary_accuracy: 0.9140 - val_auc: 0.9608\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2650 - binary_accuracy: 0.8966 - auc: 0.9492 - val_loss: 0.2816 - val_binary_accuracy: 0.9031 - val_auc: 0.9642\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2589 - binary_accuracy: 0.8974 - auc: 0.9523 - val_loss: 0.3350 - val_binary_accuracy: 0.9042 - val_auc: 0.9641\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2560 - binary_accuracy: 0.9003 - auc: 0.9537 - val_loss: 0.4118 - val_binary_accuracy: 0.8911 - val_auc: 0.9640\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2517 - binary_accuracy: 0.9011 - auc: 0.9558 - val_loss: 0.3070 - val_binary_accuracy: 0.9144 - val_auc: 0.9639\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2489 - binary_accuracy: 0.9017 - auc: 0.9564 - val_loss: 0.2547 - val_binary_accuracy: 0.9132 - val_auc: 0.9637\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2554 - binary_accuracy: 0.9000 - auc: 0.9536 - val_loss: 0.2771 - val_binary_accuracy: 0.9171 - val_auc: 0.9669\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2483 - binary_accuracy: 0.9033 - auc: 0.9566 - val_loss: 0.2983 - val_binary_accuracy: 0.9117 - val_auc: 0.9638\n",
            "Epoch 31/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2511 - binary_accuracy: 0.9030 - auc: 0.9557 - val_loss: 0.2903 - val_binary_accuracy: 0.9145 - val_auc: 0.9671\n",
            "Epoch 32/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2523 - binary_accuracy: 0.9013 - auc: 0.9548 - val_loss: 0.3012 - val_binary_accuracy: 0.9095 - val_auc: 0.9660\n",
            "Epoch 33/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2553 - binary_accuracy: 0.8993 - auc: 0.9544 - val_loss: 0.2345 - val_binary_accuracy: 0.9081 - val_auc: 0.9676\n",
            "Epoch 34/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2472 - binary_accuracy: 0.9038 - auc: 0.9565 - val_loss: 0.2597 - val_binary_accuracy: 0.9218 - val_auc: 0.9676\n",
            "Epoch 35/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2530 - binary_accuracy: 0.8992 - auc: 0.9548 - val_loss: 0.2536 - val_binary_accuracy: 0.9193 - val_auc: 0.9697\n",
            "Epoch 36/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2502 - binary_accuracy: 0.9012 - auc: 0.9566 - val_loss: 0.2505 - val_binary_accuracy: 0.9116 - val_auc: 0.9670\n",
            "Epoch 37/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2438 - binary_accuracy: 0.9043 - auc: 0.9586 - val_loss: 0.2782 - val_binary_accuracy: 0.8993 - val_auc: 0.9664\n",
            "Epoch 38/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2478 - binary_accuracy: 0.9029 - auc: 0.9570 - val_loss: 0.3531 - val_binary_accuracy: 0.9076 - val_auc: 0.9657\n",
            "Epoch 39/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2469 - binary_accuracy: 0.9036 - auc: 0.9564 - val_loss: 0.2347 - val_binary_accuracy: 0.9142 - val_auc: 0.9684\n",
            "Epoch 40/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2394 - binary_accuracy: 0.9057 - auc: 0.9594 - val_loss: 0.3647 - val_binary_accuracy: 0.8856 - val_auc: 0.9688\n",
            "Epoch 41/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2479 - binary_accuracy: 0.9034 - auc: 0.9570 - val_loss: 0.2714 - val_binary_accuracy: 0.9160 - val_auc: 0.9699\n",
            "Epoch 42/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2449 - binary_accuracy: 0.9042 - auc: 0.9570 - val_loss: 0.2173 - val_binary_accuracy: 0.9118 - val_auc: 0.9691\n",
            "Epoch 43/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2527 - binary_accuracy: 0.9014 - auc: 0.9551 - val_loss: 0.2397 - val_binary_accuracy: 0.9232 - val_auc: 0.9712\n",
            "Epoch 44/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2510 - binary_accuracy: 0.9011 - auc: 0.9560 - val_loss: 0.2492 - val_binary_accuracy: 0.9183 - val_auc: 0.9668\n",
            "Epoch 45/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2443 - binary_accuracy: 0.9043 - auc: 0.9582 - val_loss: 0.2461 - val_binary_accuracy: 0.9175 - val_auc: 0.9729\n",
            "Epoch 46/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2423 - binary_accuracy: 0.9059 - auc: 0.9600 - val_loss: 0.2914 - val_binary_accuracy: 0.9034 - val_auc: 0.9717\n",
            "Epoch 47/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2370 - binary_accuracy: 0.9078 - auc: 0.9615 - val_loss: 0.3225 - val_binary_accuracy: 0.9172 - val_auc: 0.9693\n",
            "Epoch 48/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2405 - binary_accuracy: 0.9063 - auc: 0.9603 - val_loss: 0.2557 - val_binary_accuracy: 0.9194 - val_auc: 0.9701\n",
            "Epoch 49/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2347 - binary_accuracy: 0.9087 - auc: 0.9616 - val_loss: 0.2337 - val_binary_accuracy: 0.9221 - val_auc: 0.9718\n",
            "Epoch 50/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2400 - binary_accuracy: 0.9071 - auc: 0.9594 - val_loss: 0.2229 - val_binary_accuracy: 0.9232 - val_auc: 0.9713\n",
            "Epoch 51/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2412 - binary_accuracy: 0.9047 - auc: 0.9597 - val_loss: 0.2531 - val_binary_accuracy: 0.9133 - val_auc: 0.9686\n",
            "Epoch 52/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2368 - binary_accuracy: 0.9059 - auc: 0.9613 - val_loss: 0.2379 - val_binary_accuracy: 0.9030 - val_auc: 0.9698\n",
            "Epoch 53/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2395 - binary_accuracy: 0.9053 - auc: 0.9601 - val_loss: 0.3397 - val_binary_accuracy: 0.8685 - val_auc: 0.9679\n",
            "Epoch 54/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2436 - binary_accuracy: 0.9027 - auc: 0.9590 - val_loss: 0.2721 - val_binary_accuracy: 0.9128 - val_auc: 0.9699\n",
            "Epoch 55/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2373 - binary_accuracy: 0.9056 - auc: 0.9597 - val_loss: 0.2189 - val_binary_accuracy: 0.9139 - val_auc: 0.9663\n",
            "Epoch 56/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2387 - binary_accuracy: 0.9064 - auc: 0.9603 - val_loss: 0.2501 - val_binary_accuracy: 0.9154 - val_auc: 0.9689\n",
            "Epoch 57/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2443 - binary_accuracy: 0.9043 - auc: 0.9589 - val_loss: 0.2310 - val_binary_accuracy: 0.9137 - val_auc: 0.9663\n",
            "Epoch 58/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2287 - binary_accuracy: 0.9122 - auc: 0.9629 - val_loss: 0.2355 - val_binary_accuracy: 0.9128 - val_auc: 0.9721\n",
            "Epoch 59/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2337 - binary_accuracy: 0.9077 - auc: 0.9619 - val_loss: 0.2683 - val_binary_accuracy: 0.9163 - val_auc: 0.9702\n",
            "Epoch 60/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2398 - binary_accuracy: 0.9059 - auc: 0.9608 - val_loss: 0.2270 - val_binary_accuracy: 0.9214 - val_auc: 0.9743\n",
            "Epoch 61/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2320 - binary_accuracy: 0.9106 - auc: 0.9626 - val_loss: 0.2699 - val_binary_accuracy: 0.9056 - val_auc: 0.9724\n",
            "Epoch 62/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2341 - binary_accuracy: 0.9092 - auc: 0.9613 - val_loss: 0.2888 - val_binary_accuracy: 0.9178 - val_auc: 0.9678\n",
            "Epoch 63/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2332 - binary_accuracy: 0.9075 - auc: 0.9628 - val_loss: 0.2772 - val_binary_accuracy: 0.9205 - val_auc: 0.9742\n",
            "Epoch 64/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2385 - binary_accuracy: 0.9064 - auc: 0.9607 - val_loss: 0.2453 - val_binary_accuracy: 0.9201 - val_auc: 0.9734\n",
            "Epoch 65/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2282 - binary_accuracy: 0.9103 - auc: 0.9634 - val_loss: 0.2811 - val_binary_accuracy: 0.8918 - val_auc: 0.9731\n",
            "Epoch 66/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2346 - binary_accuracy: 0.9083 - auc: 0.9618 - val_loss: 0.3173 - val_binary_accuracy: 0.9098 - val_auc: 0.9725\n",
            "Epoch 67/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2242 - binary_accuracy: 0.9137 - auc: 0.9648 - val_loss: 0.2184 - val_binary_accuracy: 0.9194 - val_auc: 0.9719\n",
            "Epoch 68/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2315 - binary_accuracy: 0.9076 - auc: 0.9624 - val_loss: 0.2400 - val_binary_accuracy: 0.9204 - val_auc: 0.9735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "muIvYULG7pum",
        "outputId": "4b190e22-1084-4dac-94c5-703f3cdc87bb"
      },
      "source": [
        "#turn model fit metric measures into a pandas dataframe\n",
        "\n",
        "history_df1 = pd.DataFrame(history1.history)\n",
        "\n",
        "history_df1.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>60.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>60.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.298340</td>\n",
              "      <td>0.876291</td>\n",
              "      <td>0.930071</td>\n",
              "      <td>0.306239</td>\n",
              "      <td>0.892638</td>\n",
              "      <td>0.949530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.079031</td>\n",
              "      <td>0.050985</td>\n",
              "      <td>0.052563</td>\n",
              "      <td>0.082248</td>\n",
              "      <td>0.051828</td>\n",
              "      <td>0.045229</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.247882</td>\n",
              "      <td>0.653989</td>\n",
              "      <td>0.693303</td>\n",
              "      <td>0.224036</td>\n",
              "      <td>0.659180</td>\n",
              "      <td>0.764524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.258570</td>\n",
              "      <td>0.885585</td>\n",
              "      <td>0.941027</td>\n",
              "      <td>0.253032</td>\n",
              "      <td>0.901710</td>\n",
              "      <td>0.959590</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.267412</td>\n",
              "      <td>0.894776</td>\n",
              "      <td>0.948218</td>\n",
              "      <td>0.282801</td>\n",
              "      <td>0.909430</td>\n",
              "      <td>0.963853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.288674</td>\n",
              "      <td>0.897903</td>\n",
              "      <td>0.951484</td>\n",
              "      <td>0.315000</td>\n",
              "      <td>0.916587</td>\n",
              "      <td>0.965639</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.594974</td>\n",
              "      <td>0.903178</td>\n",
              "      <td>0.955421</td>\n",
              "      <td>0.570593</td>\n",
              "      <td>0.922741</td>\n",
              "      <td>0.970417</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  60.000000        60.000000  ...            60.000000  60.000000\n",
              "mean    0.298340         0.876291  ...             0.892638   0.949530\n",
              "std     0.079031         0.050985  ...             0.051828   0.045229\n",
              "min     0.247882         0.653989  ...             0.659180   0.764524\n",
              "25%     0.258570         0.885585  ...             0.901710   0.959590\n",
              "50%     0.267412         0.894776  ...             0.909430   0.963853\n",
              "75%     0.288674         0.897903  ...             0.916587   0.965639\n",
              "max     0.594974         0.903178  ...             0.922741   0.970417\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPDWBW-zoQPA"
      },
      "source": [
        "## evaluating the model and getting the overall model metrics following model fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDRvjAls7psK",
        "outputId": "23ba4ee3-323f-4d37-c95e-88e6c4d04712"
      },
      "source": [
        "#tf.keras function to evaluate the model & record best metrics\n",
        "\n",
        "train_acc = nn_mod_1.evaluate(train_X, train_y)  # evaluation w/ best measures of training data -- aka model fit\n",
        "test_acc = nn_mod_1.evaluate(val_X, val_y)       # evaluation w/ best measures of validation/testing data -- the model hasn't seen this data before so super important!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 1ms/step - loss: 0.2500 - binary_accuracy: 0.9210 - auc: 0.9659\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2511 - binary_accuracy: 0.9205 - auc: 0.9650\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahig1f-Z7ppm",
        "outputId": "3e9c369b-1770-4002-f0df-fd8edc5d256f"
      },
      "source": [
        "# display validation/testing evaluation metrics in a different way\n",
        "\n",
        "dict(zip(nn_mod_1.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.9650113582611084,\n",
              " 'binary_accuracy': 0.9205396771430969,\n",
              " 'loss': 0.25112584233283997}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4V9I4_yoQPB"
      },
      "source": [
        "### graphs of the training vs. validation metrics across model fitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "2D__R-yv7plw",
        "outputId": "1a83c4e3-9c71-4496-e880-970996150806"
      },
      "source": [
        "history_df1.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df1['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.22403639554977417\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOydd3iUVdr/P2fSe4eQRhIIPRggVAXsshZQUcGyioq+Fmy7r6+6trWtrltc/S0W7B2xo6LYC9ISagg1BNIIIb2RPuf3x5lJJsMMmSSTZBLO57pyTZ7ztDMp37mf+9xFSCnRaDQazcDF0NcT0Gg0Gk3PooVeo9FoBjha6DUajWaAo4Veo9FoBjha6DUajWaA497XE7AmPDxcxsfH9/U0NBqNpl+xadOmEillhK19Lif08fHxpKen9/U0NBqNpl8hhMixt0+7bjQajWaAo4Veo9FoBjgOCb0QYo4QYo8QIksIca+dYy4TQuwUQmQKId6zGL9GCLHP9HWNsyau0Wg0Gsfo0EcvhHADlgJnAflAmhBipZRyp8UxScB9wMlSynIhxCDTeCjwMJAKSGCT6dxy578VjUbTn2lqaiI/P5/6+vq+nopL4+3tTUxMDB4eHg6f48hi7BQgS0qZDSCEWA7MA3ZaHHMDsNQs4FLKI6bxc4DvpJRlpnO/A+YA7zs8Q41Gc0KQn59PQEAA8fHxCCH6ejouiZSS0tJS8vPzSUhIcPg8R1w30UCexXa+acySEcAIIcTvQoj1Qog5nThXo9FoqK+vJywsTIv8cRBCEBYW1umnHmeFV7oDScCpQAzwqxAi2dGThRA3AjcCxMXFOWlKGo2mv6FFvmO68jNyxKIvAGIttmNMY5bkAyullE1SygPAXpTwO3IuUsplUspUKWVqRITNeP8OqTjayHM/7CMjv7JL52s0Gs1AxRGhTwOShBAJQghPYCGw0uqYz1DWPEKIcJQrJxtYDZwthAgRQoQAZ5vGnI6bQfCf7/fy/a6inri8RqM5AfD39+/rKfQIHbpupJTNQoglKIF2A16TUmYKIR4F0qWUK2kT9J1AC3C3lLIUQAjxGOrDAuBR88Ksswnw9mBkZCCbcnRAj0aj0VjiUBy9lHKVlHKElHKYlPIJ09hDJpFHKv4kpRwjpUyWUi63OPc1KeVw09frPfM2FJPjQ9iSW05zi7Enb6PRaAY4Ukruvvtuxo0bR3JyMh988AEAhYWFzJo1i5SUFMaNG8dvv/1GS0sLixYtaj32mWee6ePZH4vL1brpDpOGhvDWuhx2H65mXHRQX09Ho9F0kUe+yGTnoSqnXnNMVCAPXzDWoWM/+eQTtm7dyrZt2ygpKWHy5MnMmjWL9957j3POOYf777+flpYWjh49ytatWykoKGDHjh0AVFRUOHXezmBAlUCYHB8KQPrBHvEOaTSaE4Q1a9Zw+eWX4+bmxuDBg5k9ezZpaWlMnjyZ119/nb/+9a9kZGQQEBBAYmIi2dnZ3HbbbXzzzTcEBgb29fSPYUBZ9FHBPkQFeZOWU86ikx1PJtBoNK6Fo5Z3bzNr1ix+/fVXvvrqKxYtWsSf/vQnrr76arZt28bq1at58cUXWbFiBa+99lpfT7UdA8qiB0iNDyX9YBlSyr6eikaj6afMnDmTDz74gJaWFoqLi/n111+ZMmUKOTk5DB48mBtuuIHFixezefNmSkpKMBqNzJ8/n8cff5zNmzf39fSPYUBZ9ACp8SGs3HaI/PI6YkN9+3o6Go2mH3LRRRexbt06TjrpJIQQPP3000RGRvLmm2/yj3/8Aw8PD/z9/XnrrbcoKCjg2muvxWhUQSBPPvlkH8/+WISrWb6pqamyO41Hdh6q4tznfuM/C1K4cIKutqDR9Bd27drF6NGj+3oa/QJbPyshxCYpZaqt4wec62ZkZAABXu6k6QVZjUajAQag0LsZBClxwTpxSqPRaEwMOKEHFWa5p6iayrqmvp6KRqPR9DkDUuhTh4YgJWzOLYc9X0NJVl9PSaPRaPqMARd1A5ASF4ybQZCTmQbbr4TgWLjpd/B2vUQGjUaj6WkGpEXv6+nO2KhAkvc8C55+UJkP39zX19PSaDSaPmFACj3ARWF5TGrYQPPJd8HMP8PWd2CndXVljUajGfgMTKGXknklyzgig8mMuRxm3wNDUuCLO6Ba16vXaDTd53i16w8ePMi4ceN6cTbHZ2AK/b5vCS3dzHPNF5F2qAHcPODiZdB0FFYuARdLEtNoNJqeZOAtxhqN8P0jEJLA7/XnUXywjMUzEyFiJJz1GHx9N6S/BpOv7+uZajQae3x9LxzOcO41I5PhD0/Z3X3vvfcSGxvLrbfeCsBf//pX3N3d+emnnygvL6epqYnHH3+cefPmdeq29fX13HzzzaSnp+Pu7s6///1vTjvtNDIzM7n22mtpbGzEaDTy8ccfExUVxWWXXUZ+fj4tLS08+OCDLFiwoFtvGwai0O/4CI5kwvxXmbArgl/3FiOlVA11Jy+GvV/Dtw9AwmwIH97Xs9VoNC7CggULuPPOO1uFfsWKFaxevZrbb7+dwMBASkpKmDZtGnPnzu1Ug+6lS5cihCAjI4Pdu3dz9tlns3fvXl588UXuuOMOrrzyShobG2lpaWHVqlVERUXx1VdfAVBZ6Zwe2ANL6Jsb4cfHYXAyjL2Y1KP5fLK5gIOlR0kI9wODAeYtheenw/cPw8J3+3rGGo3GFsexvHuKCRMmcOTIEQ4dOkRxcTEhISFERkZy11138euvv2IwGCgoKKCoqIjIyEiHr7tmzRpuu+02AEaNGsXQoUPZu3cv06dP54knniA/P5+LL76YpKQkkpOT+fOf/8w999zD+eefz8yZM53y3hzy0Qsh5ggh9gghsoQQ99rYv0gIUSyE2Gr6Wmyxr8VivGfDXja/CRU5cObDYDAwOT4EsGpEEhgFiadC8e4enYpGo+l/XHrppXz00Ud88MEHLFiwgHfffZfi4mI2bdrE1q1bGTx4MPX19U651xVXXMHKlSvx8fHh3HPP5ccff2TEiBFs3ryZ5ORkHnjgAR599FGn3KtDi14I4QYsBc4C8oE0IcRKKeVOq0M/kFIusXGJOillSven2gENNfDL0zD0ZBh+JgDDIvwJ9/fkw/R85k+MwWAwPW4Fx8Leb9SibCcewTQazcBmwYIF3HDDDZSUlPDLL7+wYsUKBg0ahIeHBz/99BM5OTmdvubMmTN59913Of3009m7dy+5ubmMHDmS7OxsEhMTuf3228nNzWX79u2MGjWK0NBQrrrqKoKDg3nllVec8r4cseinAFlSymwpZSOwHOjcakRv0FAFUSlwxsOt4m0wCP7vnFFsPFjGOxssfkHBQ6G5HmqO9NFkNRqNKzJ27Fiqq6uJjo5myJAhXHnllaSnp5OcnMxbb73FqFGjOn3NW265BaPRSHJyMgsWLOCNN97Ay8uLFStWMG7cOFJSUtixYwdXX301GRkZTJkyhZSUFB555BEeeOABp7yvDuvRCyEuAeZIKRebtv8ITLW03oUQi4AngWJgL3CXlDLPtK8Z2Ao0A09JKT+zcY8bgRsB4uLiJnXlU9MeUkqufm0jm3LKWX3nLNWMZM838P4CWPwDxNgs36zRaHoZXY/ecfqqHv0XQLyUcjzwHfCmxb6hpptfAfxHCDHM+mQp5TIpZaqUMjUiIsJJU1IIIXhq/ngEcO8n21WLweA4tbPCeR8oGo1G46o4IvQFQKzFdoxprBUpZamUssG0+QowyWJfgek1G/gZmNCN+XaJ6GAf7jt3NL9nlbI8LU/56AEqcnt7KhqNZgCRkZFBSkpKu6+pU6f29bSOwZHwyjQgSQiRgBL4hSjrvBUhxBApZaFpcy6wyzQeAhyVUjYIIcKBk4GnnTX5znDFlDi+2l7IE1/tYvaIWUT5hEBFXl9MRaPR2KE156WfkJyczNatW3v1nl1p/9qhRS+lbAaWAKtRAr5CSpkphHhUCDHXdNjtQohMIcQ24HZgkWl8NJBuGv8J5aO3jtbpFQwGwd/nj6fFKLnvkwxkcJy26DUaF8Lb25vS0tIuCdmJgpSS0tJSvL29O3XegGsO3hFv/H6Av36xkzXxrxLTUgC3buixe2k0GsdpamoiPz/faXHqAxVvb29iYmLw8PBoN368xdiBlRnrAFdPj+fTrYdYX+rHJeTqWHqNxkXw8PAgISGhr6cxIBmY1SuPg8EguCglisyjQaqa5dGyjk/SaDSafswJJ/QAZ44ZTL40hXHqEEuNRjPAOSGFPibEF6+weLWhF2Q1Gs0A54QUeoAxY1T3l9ojB/p4JhqNRtOznLBCP2v8cKqkD4dy9vb1VDQajaZHOWGFfmxUIEWGQdqi12g0A54TVuiFEDQHxuJTW0B9U0tfT0ej0Wh6jBNW6AECIxOJopg1e4v7eioajUbTY5zQQj84dgQBoo41O/b39VQ0Go2mxzihhd49VJUr3rd3B0aja5WC0Gg0GmdxQgu9uS69X10hW/Iq+ngyGo1G0zOc2EIfpIQ+zlDCdzuL+ngyGo1G0zOc2ELvGwoefkwKruG7nYf7ejYajUbTI5zYQi8EBMcxxqeC/cW1ZBfX9PWMNBqNxumc2EIPEBzLEFR45fe7tPtGo9EMPLTQB8fhWZ3PmCGB2k+v0WgGJFrog2KhvoIzE33YmldBQ7POktVoNAMLh4ReCDFHCLFHCJElhLjXxv5FQohiIcRW09dii33XCCH2mb6ucebknYIpxHJySA1NLZLdhdV9PCGNRqNxLh22EhRCuAFLgbOAfCBNCLHSRpPvD6SUS6zODQUeBlIBCWwynVvulNk7A5PQj/apADzYXlDJSbHBfTsnjUajcSKOWPRTgCwpZbaUshFYDsxz8PrnAN9JKctM4v4dMKdrU+0hTEIf1nSYMD9PtuvEKY1GM8BwROijgTyL7XzTmDXzhRDbhRAfCSFiO3OuEOJGIUS6ECK9uLiXC4z5RYC7N6Iyj+SYIDIKKnv3/hqNRtPDOGsx9gsgXko5HmW1v9mZk6WUy6SUqVLK1IiICCdNyUGEUAuyFbmMjw5ib1E1dY16QVaj0QwcHBH6AiDWYjvGNNaKlLJUStlg2nwFmOTouS5BcBxU5pEcE4xRQuYhbdVrNJqBgyNCnwYkCSEShBCewEJgpeUBQoghFptzgV2m71cDZwshQoQQIcDZpjHXIthk0ccEAbA9Xwu9RqMZOHQYdSOlbBZCLEEJtBvwmpQyUwjxKJAupVwJ3C6EmAs0A2XAItO5ZUKIx1AfFgCPSinLeuB9dI/gODhaymDvFgYHemk/vUajGVB0KPQAUspVwCqrsYcsvr8PuM/Oua8Br3Vjjj2PqYolFXkkRwezPV9H3mg0moGDzoyF1hBLKnI5KSaI7JJaquub+nZOGo1G4yS00EOb0FfmkhwThJSwo6Cqb+ek0Wg0TkILPYD/YHDzhIpckqPVgmxGgXbfaDSagYEWegCDAYJioCKPMH8vooN92KYjbzQazQBBC70ZU9IUwEmxQWRooddoNAMELfRmQhOgNAuMRpKjg8ktO0rF0ca+npVGo9F0Gy30ZmKnQX0FHNmpE6c0Gs2AQgu9mfiT1evBNYxrXZDVQq/RaPo/WujNBMdB8FA4+BtBPh4khPvpxCmNRjMg0EJvScJMOLjG5KcP0q4bjUYzINBCb0n8TJOfPpPxMUEUVtZzpLq+r2el0Wg03UILvSXxp6jXA78xPka1E9zRkZ/+8A747Bb4WzQc/L2HJ6jRaDSdRwu9JUExEJIAB9cwNioQIWBbng2hNxph77fw5lx48WTY8TE01kBRZu/PWaPRaDpAC7018adAzhr8PATDI/yPjbypyIPnp8F7l0LJPjjzEbhzh9pXrxdvNRqN6+FQmeITiviZsOVtKNrB+JhgftlbjJQSIYTav/Y5KD8AF78MYy8CNw817uEHdVroNRqN66EtemvMfvqDa0iJDaKkpoG8sjo1Vl8JW9+DcfNh/GVtIg/gE6wteo1G45JoobcmKBpCE+HAb0xLDANgXXaJ2rf1PeWLn/o/x57nHaw+CDQajcbFcEjohRBzhBB7hBBZQoh7j3PcfCGEFEKkmrbjhRB1Qoitpq8XnTXxHiX+FMhZy/BwH8L9vVi7v1QtwG54CWKnQtSEY8/xCdauG41G45J0KPRCCDdgKfAHYAxwuRBijI3jAoA7gA1Wu/ZLKVNMXzc5Yc49T/wsaKhEFO1g+rAw1u4vRe5brXzzU+28BW/tutFoNK6JIxb9FCBLSpktpWwElgPzbBz3GPB3oP9nGLXWvfmNGcPCKK5uoO635yEgCkZfYPsc7yBt0Ws0GpfEEaGPBvIstvNNY60IISYCsVLKr2ycnyCE2CKE+EUIMbPrU+1FAqMgdBgcXMOMYWEMF/n45v8Kk69vvwBriV6M1Wg0Lkq3wyuFEAbg38AiG7sLgTgpZakQYhLwmRBirJSyyuoaNwI3AsTFxXV3Ss4hYSbs+IS4YE9u8fmBJumBx6RF9o/3DlYLtS1N9j8MNBqNpg9wxKIvAGIttmNMY2YCgHHAz0KIg8A0YKUQIlVK2SClLAWQUm4C9gMjrG8gpVwmpUyVUqZGRER07Z04m/iZ0FCFOLiG8+QvfCVPwegTZv94H1UygXrdVFyj0bgWjgh9GpAkhEgQQngCC4GV5p1SykopZbiUMl5KGQ+sB+ZKKdOFEBGmxVyEEIlAEpDt9HfRE5jj6b+8Cy9Zz7KGs9h9uNr+8d5modfuG41G41p0KPRSymZgCbAa2AWskFJmCiEeFULM7eD0WcB2IcRW4CPgJillWXcn3SsEREJYEpQfoCF6GjtlPOuyS+0f762alegFWY1G42o45KOXUq4CVlmNPWTn2FMtvv8Y+Lgb8+tb4k+B0n14nXwr8RW+rNtfwvWnJNg+ttV1U95789NoNBoH0LVujkfqdWBwh5HnMn33Lr7cdojmFiPubjYehMyuG23RazQaF0OXQDgeQ8bDef8EN3dmDAujuqGZzEN2Flt9tI9eo9G4JlroHcRc92btfjt++tbFWF3vRqPRuBZa6B0kIsCLEYP97S/IeniDu7d23Wg0GpdDC30nmDEsnLQDZTQ2G20f4B2kXTcajcbl0ELfCaYlhlHX1MK2fDti7q0rWGo0GtdDC30nmJYYihCwzp6fXte70Wg0LogW+k4Q7OvJ2KhA1u4vsX2Abj6i0WhcEC30nWR6Yhibcyqob2o5dqduPqLRaFwQLfSdZMawcBpbjGzOsZEBq5uPaDQaF0QLfSdJjQ/BIGD9ARsle7yDVPVKo52oHI1Go+kDtNB3kgBvD8ZGBbHBVjy9TzAgoUH76TUajeughb4LTEsMZUueDT+9rnej0WhcEC30XWBqQhiNzUa25lkJuo8ug6DRaFwPLfRdYHKCiqffkG3lp9fNRzQajQuihb4LBPl4MDoykA0HrPz0uvmIRqNxQbTQd5FpiWFsyimnodnCT69LFWs0GhdEC30XmZoYSkOzke35Fv54vRir0WhcEIeEXggxRwixRwiRJYS49zjHzRdCSCFEqsXYfabz9gghznHGpF2BKfGhAO3DLD39VEcqbdFrNBoXokOhF0K4AUuBPwBjgMuFEGNsHBcA3AFssBgbAywExgJzgOdN1+v3hPh5MioygA2WiVNC6Ho3Go3G5XDEop8CZEkps6WUjcByYJ6N4x4D/g7UW4zNA5ZLKRuklAeALNP1BgTTEsNIP1hOU4tFJqyud6PRaFwMR4Q+Gsiz2M43jbUihJgIxEopv+rsuabzbxRCpAsh0ouLix2auCswNSGUuqYWKz+9bj6i0Whci24vxgohDMC/gT939RpSymVSylQpZWpERER3p9RrTEkw+ektwyx18xGNRuNiOCL0BUCsxXaMacxMADAO+FkIcRCYBqw0Lch2dG6/Jsxf9ZFdb5k41RfNR/QHi0ajOQ6OCH0akCSESBBCeKIWV1ead0opK6WU4VLKeCllPLAemCulTDcdt1AI4SWESACSgI1Ofxd9yNSEMDYdLKPZ7KfvbYu+PAeeToQDv/bePTUaTb+iQ6GXUjYDS4DVwC5ghZQyUwjxqBBibgfnZgIrgJ3AN8CtUkobHTv6L1MTQ6ltbGHHoSo14GOKupGydyZQth9kCxza0jv302g0/Q53Rw6SUq4CVlmNPWTn2FOttp8Anuji/FyeVj99dikpscFqMVa2QGMNeAX0/ARqjqjXsuyev5dGo+mX6MzYbjIowJthEX6sNydO9XZ2bE2Rei070Dv302g0/Q4t9E5gqimevsUoe7/eTatFr4Veo9HYRgu9E5iaEEp1QzM7D1X1nUVflQ/NDb1zT41G06/QQu8Epg8Lw80g+HL7ob6z6KURKnJ7554ajaZfoYXeCQwK8GbO2Eje35hLnZtpAba36t3UHAG/Qep77b7RaDQ20ELvJK47JZ6q+mZW7qlVA73puombqr7XkTcajcYGWuidxMS4EMbHBPHyhmIkondcN82NUFcGg8eBpz+Ua4teo9EcixZ6JyGE4LqTE8gqqaPZM7B3LPpaUwE4/8EQkqAteo1GYxMt9E7k3OQhDArwotzo2zsWvTnixn8whCZoH/2JTOWAKSGl6QG00DsRT3cDf5w2lMON3tRWlnZ8QndptegHKaEvPwjGAVVhQuMIR3bDM2Mgd31fz0TjomihdzJXTI2jGn9KS470/M1aLfpBEJoIxiao0pbdCUfpPtPr/r6dh8Zl0ULvZML8vQgIDqOptozKo009ezOz0PsNUj566F9++g0vqS9N96g+rF6PlvTtPDQuixb6HiA2KopAalme1sMJTDVHVBE1D29l0UP/8tNvegM2vtzXs+j/VB1Sr7Va6DW20ULfA4SERRAsjvLm7wfa6tT3BDVFaiEWIDAK3Dz7j0UvJVTkqZDQlh5+8hnomC16LfQaO2ih7wm8g/GgibKqKlZnFvXcfWqOtAm9wQ1C4vtPLH19BTRWg7FZLSJruk51oXrVrhuNHbTQ9wSmejfjQiVLf8pC9lQTkpoitRBrJiQByg72zL2cTYVFz/iSfX03j4GAWei1Ra+xgxb6nsA7CIAbp4Sxs7CK73b2kFVfU9xm0YPy05dl9153q+5QaSH0pVrou4W26DUdoIW+JzCVKj59qAfxYb785/t9zrfqG2uV68Mvom0sNAGaatvi610Zs0Xv7gMle/t2Lv2ZxqOqgJ4waIteYxeHhF4IMUcIsUcIkSWEuNfG/puEEBlCiK1CiDVCiDGm8XghRJ1pfKsQ4kVnvwGXxOS6cW+s4rbTk3rGqjeXJ7a26KF/LMhW5CqRj54IJVl9PZv+S41pITZ0GDQdVcKv0VjRodALIdyApcAfgDHA5WYht+A9KWWylDIFeBr4t8W+/VLKFNPXTc6auEtj0XxkXkoU8WG+PPuDk616W0LfGkvfDxZkK3MhOBbCk7TrpjtUmdw2kcnqVbtvNDZwxKKfAmRJKbOllI3AcmCe5QFSyiqLTT+gHziJexCfEPVaX4G7m4ElpyeReaiK73c5MVvWMivWTHCceoTvFxZ9HgTFQlgSHC2Fo2V9PaP+SbWV0Gv3jcYGjgh9NGCxcka+aawdQohbhRD7URb97Ra7EoQQW4QQvwghZtq6gRDiRiFEuhAivbi4H/iXO8IrUL2amo9cmBLF0DBf/vP9XudZ9ZYFzcy4e0JQTP8IsazMa7PoQUfedBVzDL0Wes1xcNpirJRyqZRyGHAP8IBpuBCIk1JOAP4EvCeECLRx7jIpZaqUMjUiIsJ6d//DzR08A1pLFbu7GbjN2VZ9bbGy3v3C24+bI29cmcZaZcUHWQi9dt90jepCtdZhXp/RrhuNDRwR+gIg1mI7xjRmj+XAhQBSygYpZanp+03AfmBE16baz/AJbleq2GzVP/uDk6z6miLwDVeJUpaE9INyxZX56jU4DoKHqoxeHXnTNaoLISCyLfpKW/QaGzgi9GlAkhAiQQjhCSwEVloeIIRIstg8D9hnGo8wLeYihEgEkgAXNzedhHdwu+Yj7m4Glpw2nB0FVfzgDKu+5kh7/7yZ0ETVdaq3Whl2BXNoZVCs+qAKTdSRN12l+rAqf+EVoD4wtUWvsUGHQi+lbAaWAKuBXcAKKWWmEOJRIcRc02FLhBCZQoitKBfNNabxWcB20/hHwE1SyhNj1c3Koge4aEI08WG+PLwyk7Laxu5d3zor1kyoKfLGlf30laZib8GmB8Ww4dp101XMFr0Q6glPW/QaGzjko5dSrpJSjpBSDpNSPmEae0hKudL0/R1SyrGmEMrTpJSZpvGPLcYnSim/6Lm34mJ4Bx1jVbu7GXh24QSKaxq45d1NNHWn4JllnRtL+kMsfUUeGNwhYIjaDh+h5quLm3UOKVV4pfnn6KeFXmMbnRnbU3gHt0bdWHJSbDB/n5/M+uwyHv1iZ9euLaV9iz4kXr26sp++Mk+5G8zrC+FJpuJmOX07r/5GfSU01ymLHpTQa9eNxgZa6HsKG64bMxdNiOHGWYm8vT6H9zZ0oWZ9fQW0NNq26D39wD/StYW+Ik8twpoJc+HIm6pDsP3Dvp6FbcyhlWaL3je8f5S/0PQ6Wuh7Cu9glZLebNsXf8+cUcweEcFDn+9g44FOLlvUmHvF2hB6MPWPdWGhrzQlS5kJH65eXTHy5vdn4ZPFbT9zV8KcLNXOddMLvYo1/Q4t9D2Fqd6NPavezSB47vIJxIX6cvM7m8gv70SNEltZsZa4cix9c6OykoMthN4nRIUHumLSVM7v6rUoo2/nYYtWobdw3TTVQlNd381J45Jooe8pLOrd2CPIx4OXr0mlsdnIotfTOFhS69i1bWXFWhKSoETAFQtcVRUAsr1FD8p9U+piIZb1lXB4h/r+sCsLvYXrBvSCrOYYtND3FB1Y9GaGRfjz8jWplNQ0MPe/a/hpjwMx9uaCZn52sohbQywPOjbX3sRchz7YSujDk1zPdZO3kdayTS4p9IdVdJenr9o2Z0lrP73GCi30PYWp+YityBtrpiWG8cWSU4gO8eW6N9I67kpVUwQGj7biada4ciy9ZbKUJeEuWNwsZ60KA02Y7aJCbxFaCW0f/Ee1n17THi30PYUDrhtLYkN9+eTmGVwwPop/rN7DLWcj1msAACAASURBVO9upqah2fbB5hh6IWzvN8fSu5orBNos+qCY9uOtkTcuNOfcdTAkBWKnqqcNV/N9VxW2+ecBfMPUq3bdaKzQQt9TOOi6aXeKpxvPLkzhgfNGszrzMJe8sJZyWxm0tXbKH7ReKERZzIe2dHLSvUBFngr/dPdqP95axdJF3DdN9VCwCYbOgMhxII1wpIt5Dz1F9WEri97kutGx9BortND3FGaLvuxAp3q4CiFYPDORN66dQnZJLde/mUZdY0v7g2qK7C/EmomZDHlpnZx0L2BuOGJN8FDljnKVyJuCTSpXYeiMthLAruS+MRpVdylLofcKVD9D7aPXWKGFvqdw94SoCbB+Kbx8GmR+BsaWjs8zMWtEBM8tTGFLXgW3vb+FZstyCfYKmlkSOwWq8qHyeIVG+4CKvGP986BKO4cmuo7rJneteo2dCsHxquy0OQLHFThaorKJLYVeCOWn17H0Giu00Pck134D5/9HLch+eA38dzKkv67cAg4wZ9wQHpk7lu93FfHg55lqgdbYoiy2Di36Keo1f2M334QTMRpVeGVwnO394UmuY9HnrINBY8A3FAwG5b5xJYveOobejF9Y51033z4AXx/TCnrg8819sP7EaGOthb4n8fCG1GthSTpc+iZ4B8KXd8I/R8Bnt8L+H6HFzoKriaunx3PLqcN4f2Muz/2QpSIqpLFjiz4yGdy9neu+MXajCBsol1NLo23XDSihL8vu8GfS47Q0q9DKuOltY5HJULSj+z8DZ2Fd/sBMZytYSglb34ddJ069QUC97y3vnjDv272vJ3BCYHCDsRfCmHlw8Df1j7Xzc9j6jnrUHnsRTL4BImz3ZLn7nJEUVTXwzPd7GYknc6BjoXf3VBEjXbHoCzbBuqWmcEdTyOPRUvXBsSSt43vbozXixo5FH5YExiaoyIGwYV27hzMoyoDGauWfNxOZDI01KmS1L+dmxmzRB1oJvV9457Kiy7LbngAaasDL3znzc3Vqi6GhUrk3TwC0Rd+bCAEJs+CiF+DuLLjsbWU1bnoT3jgPmhvsnCZ4an4yp46M4P0flYWeXefX8f1ip0DhNrvXtcsvT8Oer1VmbWAMJJ4GKVeoCKLdX3buWpZUWNWht8ZV+sfmrFOv1kIPruO+qTIJvbULzy+ic3H0eRaGgKusj/QG5vdadahTwRL9FS30fYWHN4yZCwvehsvfVyGTOz+3f7ibgeevnMhlo70BWPRRLte8tvH4BdFipyhXSeE2x+fVUAP7f4KJ18Di7+CK5XDhUjjv32qxtDuPumaht7UYC6oBCfR9iGXuWhUFFBjVNhYxGoSb6wh9daESdTeP9uO+YerJw8F1IPI2AKZ8jL7+gO1NzO+1pfGEyDvQrhtXIPE0CB0GG1+G8ZfZPczX053zEgywH646I5WX1hVx2UvrmBAXTEKYH24GgbubwM0g8HJ346pxY0gAZbXFTnFsLvt/gJYGGHVe+3EhYPQFyqVTV24/K/d4VOap8+y5B3xDlY+5L8sVS6ks+qSz2o97eEPESBcS+sPHLsRC+1h666Q0W+SnQfwpqnhbX3/A9iaWf2NV+eBvp5zIAMEhi14IMUcIsUcIkSWEOGZ5XghxkxAiQwixVQixRggxxmLffabz9gghznHm5AcMBgNMXqz86R1Z3zVHwNOfG88cz5p7TueRuWOpbzKSllPGmqwSvt91hFUZh3l7XQ6XvJ1NU0Bs5/z0u79SYmy5EGlm9FwV0rd3defenxl7oZWWhCf1bf/Y0iwlkrbe/2AXirypLoSAqGPHW5uEOxBLX18FRZkQP1M9wZxQQr9flbcA1wtB7gE6tOhNzb2XAmcB+UCaEGKllNIyTfA9KeWLpuPnAv8G5pgEfyEwFogCvhdCjJBSOh5QfqKQcgX88Kiy6uf91/5xFp2lfDzduGZGPNfMiD/msOziGi57aT0/1g7ljJyNjj26tTTB3m9g5Hkqrt2aqIlKXHZ9ASctdOhttaMyr809Y4/wEcqFZTSqD8DexlyW2NI/byYyGTJWqEd9s+XcV1QXqjwNa1orWDrgpy9IB6R62ivYdOK5bmImqzIXVQNf6B35T5oCZEkps6WUjcByYJ7lAVLKKotNP1pL/jEPWC6lbJBSHgCyTNfTWOMTDOMvhYyPlGvEHjVF4Ndx1EtihD/vLJ7CFjkC99pCDuc5YCXn/K5i/q3dNmYMBhh9PmT9AI0OllQ2I6VjFv3QGWrRt6/qv+esU1axrQ8kV1mQbWlSFrt1aCV0rgxC3kYQBoiepJ6kSrM6ldTXb2lpUtFTsVPBzVMLvYloIM9iO9801g4hxK1CiP3A08DtnTz3RiFEuhAivbj4BE7fnnyD6gG69T37xziSFWtiVGQg8+deBMCyd96nuLqD6JvdX4G7Dww73f4xoy9Qc8z6waE5tFJXrppi2Iu4MZMwW71m/9K56zuL3LXKbWOrYJxZ6Iv6OEPW3I/Alo++tbCZA/9HeRtUUph3oHqSamloC4EdyFTkKhdk+Ai14H4CuG6c9mwspVwqpRwG3AM80Mlzl0kpU6WUqRERA3tR5LgMGa+sjLRX7Cfm1B7pOCvWgqTx02lx8ya+LpOrXtlgu0gaKIt791dK5M31zW0RNwN8QjsffdNRxI2ZwCEQPhIO9IHQVxaoedpy24CylgOi+t6iN4dWBtrw0XsHmerddGDRG1sgP71tkT7clMNxIrhvzO8xPAkCo7VFb6IAsPzvjDGN2WM5cGEXz9VMvkElsWT/eOy+5gZlGXdC6HHzwC16IhdFHOJAaS1Xv7aRqvqmY48r3Kr+4Eef38H13GHUucqXb6cfrk1aG47YSZayJHG2qgXfmes7g1xT/LythVgzkcl9L/T2yh+Aqd5NeMeum+Ld0FClDAuwEPoeXpBtqoNDW1VW6ur74e2LYMOynr2nNeaIm7DhSui1RQ9AGpAkhEgQQniiFldXWh4ghEiy2DwPMJsFK4GFQggvIUQCkAS4UPEVF2TMXOUj3vjKsfvMj+OdzUyNnUxA+U6WXT6WXYVVXPd6GkcbrcoM7P5K+WtHzOn4eqPnKpE48Kvjc6johNAnzFaN1fN7ufpm7jpVvMzsorFFZDIU73E8Tr0nsFf+wIyvA03C8zaoV7NF7xemntR6QuiNRlXUb9lp8LcoWDYbPr9FPbnmb1KF/3qT0iz1Xn1DISgaqg8N+LWJDoVeStkMLAFWA7uAFVLKTCHEo6YIG4AlQohMIcRW4E/ANaZzM4EVwE7gG+BWHXHTAe5eKllp7zdQntN+X0e9Yu0RMwWMTZwacIhnF05gc245N7yVTn2Txa9i15cw9GT1x98RCbOVIO5a2fGxZirzwMPPsfj7+FPUh072z45f3xkc2QWDx6qSFfaITAbZAsW7Or5ec6MStx+fcG72ZXWhCg30tRP54xfWsY8+L00ZFCEJbWPhI5zrujG2qOCCF2aoon4N1TDzf1XdpyXp8JdDMPv/VMtL84dXb1CS1ZaFHRit/PUDvLSzQ1F3UspVwCqrsYcsvr/jOOc+ATzR1QmekExaBGv+DRteUmGMxXuUsOSuV/s7m9xhttryNnLeydOoazqJ//1wG7e+u5kXrpqEZ+UBdf1JTzl2PQ9vGHG2ego4/5njC6OZClMdentdsSzxCVahgwd+Ae53bE7OoGSfel/HwzLyxlZ4oyU5a+DQZvVlcINTnVQhsrpQNW+xF37qF9Fxv+C8DcptY/n7CE9SBoYz2Pk5/Pi4ekIIHwnzX1U1naz/VuKmqdfc9aoeVG9Qug+GmxLizElllQW2XWEDBF0CwRUJjoWR56pH2pdmwieL4fdn1QLbSZfDoLGdu57/IJUQY0qcumRSDI9dOI4fdh/hrg+20rLLVL9m5LmOX3P0BcoPbP7w6YiK3I4XYi1JPFUtFtZXdXSkc6ivVAvdYUnHPy4kATz9HfPT714FHr4wfiH8/CSs+Y9z5lpdeHxR6sh1U1sCZfuPzZYOH6Es2+727a3IhRXXqKeyS16HW9ZD8iW2DYLI8aYqqxu6d09Hqa9ST8bmwnTmBe0BXtxMl0BwVc5+TFWfDEtUdVbChquKlF0ldgoc+E25EITgj9OGUt/YwhOrdnHrgeUEeyfx0eZGooPziQ7xwdPdwP4jNWQV16jXIzXUNbXw0PljOW/8EGURuXmp6Jv4k49/7yO71OJf/CmOzzdhNvz2L7UoO9KBdYPuYs7GDe9A6A0G5d7pSOilhD2rVBTThc+rqpzfPwwePjD1f7o31+rDx0888wtT1Teb6tXTlzXmQmYx1kJv0bfXtxvpLoe2AhLmPQ8xk45/rLuniuPvyGAoyYLdX8DJdzr2VGiPUqvfc6CFRT+A0ULvqoQmwuy7nXe9mCmQ8aHylZsWRG+YlUhAcxmjft3Fy8bL+Pd3xy7EebgJEsL9GBMVSF5ZHbe+t5l12XE8cN4YvIedroR+zpP2//ma6uCj61TY3yl3OT7f2KnK0jvwS/eF/vAOteh2vPWB1kiMDoQelPtm2wfHz941RzGd/oCyZC96SUVNff1/ah1m0qJOv41WqgtVFVR7mMsg2Kt3k7dBhWBGpbQft4y8cbQ2ki0Ob1fW/OAxHR8L6ne99jlVLdVeaO+aZ1RZ7zEXQmiC7WMcwSz05t+zb6j6OxvgIZZa6E8UYier17yNSuiNRmiqZaFvGiD5nxtv45qwMRRW1lNQXkd9UwuJEX7Ehfri7qbErLHZyD+/3cOyX7NJP1jOmxPPYvDer2HnZ8r/aotvH1RNta/6uHPRQh7eyn/b3QXZ5kZ49SwlrHOetH9cyT5VnTIkvuNrRiariJHSfarQmS3MUUxJpvJObh5wyWuw/Er44k7VU7grPunGo8rN1JHrBpSLxqbQb4QhJ6mnC0uc1bf3cIb60LC+vj3ipqk1qYJNkDDz2P1GI+wz1VfKT+++0AtD2zWEOCFi6bWP/kRh8DiV9frFnfBkHDwaCk/GwDf3qn/wwePw9nAjIdyPU5LCOXPMYBIj/FtFHsDT3cBfzh3N64smU1RVz7nfh1EWNBb50fUqLtqa3V9B2sswfQkMP9Ohaa7ZV8Kraw6ojYTZ6kOiuqjr77tohwrVPLT1+MeV7lMi74h7LOlsFfWy6Q37x+xepeLx/cLaxty9VFnqQaPh9y7661tj6O2EVsLxyyA0N6rFYXP8vCVu7sp33V2hL9yufO+OEmM2Quy4bw5tbouKKUjv3txK9ilDx92rbSxo4MfSa4v+RMHNQ/n9CzarlHevQPAKUN9bR190wGmjBvH1HbO4Y/kWZh74Ey96PMPMz2/h/Z83szvxWoYN8ufM6BaiPr9VWY5nPOzQddMPlnHdm2k0NhuJDvZhTuKp8MMjKl5//KVde9+HNqvXoszW9QmbWIbcdURgFIy9GDa/pSJpvIPa7y87AEcy4Zy/HXuuhw+Mu1hFpHSinEUrrTH0jlj0NhZkD2dAc71910x4EhzZbefeRarxTOp19n+OtSUqLv14uQjHzDdUrUPl2lmQ3fO1etqKGNX93IrSfce65wJj+iYTuxfRFv2JxJQbVHerP/wdTr8fTr5duTQGje70pSKDvHnvhmn866pT2DZzGVsCTuPyyleI3/wUj3y+nZxXrqK+vo61E56mxeDR4fWyjtSw+K10YoJ9GBUZwIOf76AyaIwS0QM/d/69minYol4bKu3XcTEaVRRKR5U1LZl+i2rwsfntY/ftMUUi24tiMof2dbZeELSVknDEorcVG24uWW3Logflcik/oAp/WfPzk/DVn9RTlj3Mi9RDOmHRA8RNVS4lW6U/9n6j3DtJZ5o+qDrZMc2MlKo8sfXvOTBKPSn1da/iHkQLvabLuBkEc8ZFsuTssUy462OYvJhrxRfsinqc6YadPGVYzBWflDLr6Z9Y+lMWpTW2/0GPVNez6PWNuBsEb143hX9eehJltY387Zu9atEx+5euJxwd2tJm4R62U4ysMk9ZuY5a9KBi6IeeDBtePFYgdq9SxcLs+ZIjx6ukt33fOn4/M5mfqhj64y0am+vd2HLd5G1QPXute82aCR+hEojKDrQfb6hWi/mgIqHscXi7eu2M6wYgdpr6MLZORKvIVe63EXMgOtXUMW17565tpuqQcuOFWwl9UDRIY1tC4gBEC73GORjc4Nx/wqn34Vm2B8Zdwv33PcrzV05kaJgv/1i9h5P//iOPfJFJYWVd62m1Dc1c90YaZbWNvLZoMrGhvoyLDuKGmYl8kJ7Hfv9UJcSdaXhtprFWCcf4BYCwX3XSgYgbKSVltY2kHyxjRXoeL/6yn9yR16q5WWYIHy1TFTDtlXoGFakz/CxTN69OWJGVBZD1HUy40na/ADNCqCqW1oXNmupUiO3xImpa+/ZaRWBtX6GeYNx9OhD6DOUKcSTD2pI40xOGdZilucnNyD+0+fK76qe393s2h1gO4AVZ7aPXOA8hlM965LkQMQoPdzfOTR7CuclDyDpSzQs/Z/PWuhzeWZ/D/IkxLJ6ZyGNf7mRXYTWvXJ3K+Jjg1kvdeWYSqzMP82BGGO+B8qGak1wcpXC7stQSZsLer6FoB1JKVmce5j/f70MIwdXThzK/aS+ecIxFn19+lI83FfDL3iPsL66lsq69O+NpvPnVZwhu3/yL2kFnM3yQv3IzSGPHyWdJZ6pwwYL0tuzQjtj6nrr2hKs6PtZWk/C0V5WVn3qd/fPCbAi9lJD+ulpvCRuuhN7eekfh9s75582EJKg+C3kbYPL1beN7vlZtNi1LFuSnATd3/h6toZU2LHqAyvzuhZW6MFroNc7Hhn92+KAA/nXZSdx5ZhLLfs3mg/Q8lqcpn/lTFydz2qj2i5LeHm48dXEyC5bVUBkwiKDsn2HC1VC4lYrM7ynL/B7f6oP8JeBJWoKHEu7vRXiAJ4MCvBkzJJCU2GB8zAuxURNg8Fjq8rax8Pm1bMurYPggfzzdBPd9koHB+2fmuflT1uRPaFMLqzMP82F6Pr/vL0FKmDQ0hPPGDyEx3I9hEf4kRvjh6+nOdzuL+HbtxVxXuZSLn3mZushUXvf5hMEBUYiOyiMknqYWGPd92yr0UkqEvUVOoxG2vKVcWaGJHf8OrOvdNNSoEMbE046f4OYdqPz/lpE3+emqEcwFzyq3zo6PlR/feh6NR5XV3JWwUSGUVW9p0TdUw8HfYMqNbWMxqWo+XaEkS9Vbsi7vHGgSem3RazTOITbUl8cuHMdtZwznzbUHiQz0ZuEU2xUtpyaGcdW0oXy3aRQX7/ka45NxuDfXEgwUGWOJNBQzx20D7xwdwr6iakpqGmlsUYt57gbBqwHfMsEjgu/2NkNBCBdV5VDdUMHTl4xn/sQYDAI2Higj8MN/srt2MPP/8TO+Hm5UNzQTHezDHWckMX9iDLGhtpN4rpgaBxPux/ivd/nXoDUsqRtLYMGv/BhwNomlR0kI97P/g/AJVgK/7zsqp9/H7cu3UFHXxCtXpxIR4HXs8Qd/Vf5qByOY8A1vXxRvw4vKwj/dgVYR4UntLfr0V1URu3GXtC0G56yD0EQ2HijjsS93ctvpwzk7KF89cXTFogcVjrrri7bG5/t/Uj55y4qq0amqjk5NcedrPpXuU0+F1h+m3kHqA2AAh1hqodf0CYMCvLn7nFEdHnfPnFHck3kmoxty2WocRobHScRNOpuLZqbA+2dxmddOLrv2n0CbH31bfgXpB8tJStvHhqZ4/vzhNi70iWS+kHx9RThe8W01d6YmhoFHEbVjprM4KIHy2kYuTIlmWmIYBoMDIaeefhhSF5Gw9jlWnn0mbqsb+KB6PD8/8yv/MzuRW04djo+nnaJvw8+EHx7hhqUr2VLhg5tBcNlL63j7+inEhFh9uGx+SyVZjeqgX4AZv/A2101dhco8HTFHWcRWNDYbqWtsIcjXFB0VlgQ7PlLumbpy2PEJTPwjePmrEEefEMhZy45B53P9G2kcbWrhxrc38fKY7ZwFXRf6WKsCZ3u/USJs6doyz78gXfntO0Nplup7bI0Qyn2jLXqNpm8I8Pbgjwuv4qmfp3FhSjR/HT8Ebw+TcCado1Lj68rBJwQhBGH+Xpw+ajCnD/WC9QUMOv1avhpxCkMNI+HFp/Eq2QnxFqGFjbVQVYBf6ijum9X5MFNAuRbW/Re37x8Cr0Aev+kmnvw2m//3YxafbC7g1tOGMzclCn+v9v9uuwKmMRoYXZvGn66/Gw83wbWvp3Hpi+t4+/qpyucPaoF31xfKt26rdg1gNMr2H0x+4apnQHMDrH9eZdOe9pd250gp+XrHYR75IpOiqgbC/DwZNsifPwp/LqivZMe+LMaVfqtaDE66Vp1kMEDcDBoPrOHqjI0E+njwxW2n8N+fsija9ipHPfxo8orCKrPALlJKpETNfch4tdibu14Vzdu72lRTySI8d0iKcnnlp3VO6Jsb1NPI+AUAVNU38enmApIG+zNjWPiAz47VUTcal2f6sDDevn4q8yfFtIk8qAxV2QL7bXTjOqTi591jJjI2Kgj/wcNUkph15I117ZOuEBStEqhaGmH4mQwKCeSZBSksv3Eawb4e/OXTDKY+8T1/+TSDHQWVAKzKKOTCjyo4IsK4e1gu0xLDmDQ0lA/+ZzpNLZLLXlpHRr46lu0fqGtP+OMxt95VWMXt728h6YGvmf/CWt5ad5CSmoa2kNKSvbDueRgzTy2mmsgrO8p1b6Rxy7ubCfPz4p45ozhz9GCMRsmqwgAAHn/jMw7/+DwNQyZD5LjWcysHpeJZeZAIKnhn8VTiw/34xyXjOSfsCBktcVz4/Fr2FVUf90cmpeTL7YeY9Y+fuPiFtaqfsZuHKnCWt16VQzha0irmRqOkuLoB6eGjisp11k9fdgCkkVLvOB77cicznvyRh1dmcv0b6ewqrBrw2bHaotf0X2JSVaegvd/CuPnt95mEvrVmvBCmqpNWQm/ZP7Q7zFgCmZ+ocrwmpiWG8eVtp7A5t4L3N+byyeZ83tuQy4jB/uwtqmHS0BACB/0B770rVYKSmwejhwTy4U3TueqVDVz+8nqeXXASp256E7foSe3EdlNOOc//lMUPu4/g7+XOJRNj2JZfwUOfZ/LIFzu5PaqcO4DqL+7Dv7GGyin/i2+zESHg5d+yee6HfRiE4IHzRrNoRny7UhdUxMJ/HufR6A1EFufzf3lzCftmN7eeNpz6phYeTA/gBeDVUxuJMa1DCGkkojaLlnGXU723mXlLf+eKKXHMGRfJxLiQdk8bW/MqePzLnaTnlDNisD97Dldz0fO/88a1UxgeN1WVc97xibLch59BXtlR7vpgK+k55UQGevNP3wSmFn9PzuFKhg0OtL+AbaKmoZmCzM2MBK77opxMDnLe+CFcPDGG//toGze+nc53KUPwrilSJSK6UyXWRdFCr+m/GNyUnzvrO9XNyLLe+aHNKmTPMp578FhVddIyNLA0CxCORbIcjyEnwf/uOyZ+XAjBpKEhTBoawoPnj+GzLQWsSM/j4onR/O2iZLyzSiHjHRVWaCrjnBDux8c3z+CqVzfw/97+gDO8dvFXeSNrn/mFIUE+1DY0k55TToivB38+awRXT49v9a/vOVzNym0F7Nmk8g4CCn7j05aTuevFPCAPDzdBU4vknLGDefiCsUQF2yg8FhgNHr6MKP4Oo1cwYtSFvPDzfj5MzyfY14PCmiG0ePkSU7UFuLLt59hcR+SIyXx5zik8vHIHb63L4ZU1B4gI8OLsMYM5deQgVmUU8umWAsL9vXjq4mQuTY0l81Al172RxvwX1vLBaWMYJVsg/TXk0Ol8lFnDX1duwGAQ3H5GEtnFNXy/P5ZTWmq5+dnllPgkEBfqy5AgHyKDvIkK9ibMz4u88qPsKqxi9+FqckqPcrPbL9zjAbOmT+eFmW3v+4WrJrHwpfW8s6uZxUiVIRsytHt/Cy6IFnpN/2bEOZCxQtXwMVfoBFX6wDomevA4aHwFKnLaqlSW7FONXhyttHg8OkgSCvLx4JoZ8VwzI75tMGG2ymLd9127ev2RQd58cssMyt9/i6Z8HzzGXsLQGjcKK+toaDLy4PljuHxKLL6e7f+FR0YGcHfkKOQkN/jv/RiFG35n388jbtFU1TVR3dDM9MSwY8JZ22EwqFjzw9sxTLiSv8+ZyuUnV/DoF5nsKKji5Wum4rZ+alszdWgrfRA5nsggb176YyrV9U38tKeY1TsO8+mWAt7dkIunu4FbTh3GLacNb12zGB8TzKe3nMw1r2/kqtWSNA+BaGng4+pk7v5oO1MTQvnXZSe1LlDLYn9Y+l8eT63nUyIpqKhjf3ENa7JKqGlQCWhCQEKYH+OigrhkYgzz85owlgzmzxe0X4yeGBfCo/PGsuqzLSz2RGXPnqhCL4SYAzwLuAGvSCmfstr/J2Ax0AwUA9dJKXNM+1oAc5eGXCnlXDQaZzHsdFV2dt+3bUJfc0R1DIq+qf2xrW0Ad7QJva0iV72Jd2BrmCVnPdJuV6CoJ/DQ1zB+PvfPs1Obxg7CfxAIA4aUyzl7ZgeNYWwRPkKVM0hVi7ApscF8fPMMahqaCfD2gMIZ8NPf1EKxb6g61s2zXdnmAG8P5p4UxdyToqhvamHjgTKGDfIn2sZTRGyoL5/cPIMb39rEnkMxjDLk8eLhJO79wyhumJmIm4XrR4QNB+8gpnrsZ+rc9l1Mq+qbKK5uICrIp32006v5dn/PC6fEcXj/ONgDmzJ2MGno9M7/vFycDhdjhRBuwFLgD8AY4HIhhHVHgS1AqpRyPPAR8LTFvjopZYrpS4u8xrn4hqoCXeZ65WDhn7cKpRs0GlUKIVNtm4tcddc/312SzlbVList2tmVH4Rv7oOmWtUsvrN4B8E1X8Kcv3dtTlNvgjlPtfvZCCGUyAMMnQHIthaAhdvVz9cyQsZyOh5uzBoRYVPkzQT7evLW9VPIGXQ6O9zH8uytl3DT7GHtRB5QTxzRqWrB1opAbw+GRfi3F/nGBr+x+gAAC/NJREFUWtV32brGjQW3zJ0NwE8bN7PxQBlHquspr22kur6J+qYWmluMSBv1lqSU1De1UFzdwIGSWjIPVVJQUUdDc4vdex0PW/dwBo5Y9FOALCllNoAQYjkwD2gtYSel/Mni+PWAAznaGo2TSDoLfni0LdGmYDMg2kWZAODpp3zxRaYHzOpCVb+lM1Ure4Kks+G7B1W6v1+EqnOf/ZN6Ukm5qq3GS2fpqMXj8Yid3N4VZk30JOVyylmr4vMPZzil5aO3hxvnLHmu4wNjUuHXf6iMXy9/+8c1VMO7l6lQ09EX2D3M0y8Io1cgcU3lXPbSOrvHebgJ3A0GPNwEQgiONjbT1GJbnAO83Ynw9yI8wIvRkQGkxoeSGh/CkCAf9STk4UOD8GRrbgVr95eyLruUEF8PXvrjsbkO3cURoY8GLOu75gPHe468HvjaYttbCJGOcus8JaX8zPoEIcSNwI0AcXG2syQ1GrsknaOEft+3MPFqtRAbMdK2AESOa6t+6KyIm+4SMVI1Tl/1v2o7MAZO/YuqaWOuw+JqePgosc9Zqz4wj5Z0vmJld4hOVVm4h7bY7koFqhH4u5eoUMz5r3TY/MYQGM0FgRLjyGSajJLmFiPNLZImo3ptbjG2jje1SKSU+Hm54+/tToDp1dvdjcq6JkpqGiipaaS4uoGiqnpWpOfz5jqVqRwX5MmnLbey13M011bfRH2TioZKjg5izJBwZ/+kACcvxgohrgJSgdkWw0OllAVCiETgRyFEhpRyv+V5UsplwDKA1NTUnnl20QxcBo9VkSJ7V6tY84LNykq2eWyySqFvqO5cn9ieRAiY+WdVuO2kK2D4Ge0jiFyVodNh7f9rq0/Tm0JvzpDNT7Mt9PWV8PbFqnfvpa+rPIKOCIrGp/aw3ZIc3aGpxciuwirSD5bTkPkVYYVFTGku5paTbmHU6AlMTQhry0zuARwR+gIg1mI7xjTWDiHEmcD9wGwpZWvhcSllgek1WwjxMzAB2G99vkbTZYRQwp7xoSpnfLQEom2kuoP6UAA4sst+kau+IPXa1oXPfsPQk1Vmcvpratv8s+0NfEOVG86Gn566cnj7IrXoftlbxy8ZbUlgNBRuO/4xRqPqXdBcrxafj+c2ssDDzcD4mGBVoTVvLVSE4tZQze2+38PYcxybXzdwROjTgCQhRAJK4BcCV1geIISYALwEzJFSHrEYDwGOSikbhBDhwMm0X6jVaJzDiHNg0+uwbqnatlXTBNqSjg5n2C9ypXGM2CmAUBUmQxNVBFFvEjNZNY9vblQf8MW71aJr5qeqY9iCdzq3bhAUoyp+Nje07ym78WXV+rHpqMpQNuPuDZe/ryK/HKW6SNXwmX4LHC2HLe/Aqfd1vn5/J+lQ6KWUzUKIJcBqVHjla1LKTCHEo0C6lHIl8A/AH/jQlKVmDqMcDbwkhDCiInyeklIepw+ZRtNFEmaBm5cq/mVwt29dBsWCV5AqhVCyz2aRL42DeAepkNXDXaxB312iU1V5iCciVSkMoDX5beH7quZ/Z7AsV2xOoMtdD1/foyK74qYqcXf3UjV5Nr8JKxbB4u/ahZUel+3L1VwnXK1KPm99R1UHnXV35+baSRzy0UspVwGrrMYesvje5k9USrkW6IO/AM0Jh6efSjja/4OKtrFT/Ku1FELBJlXk6qTLe3eeA42hM0xC34v+eTNjL1Q++IAhSmgjRqr1Fk/bZaU7xOzCqzqkhP5oGXx0HQTHwRXLj20CP+o8ePl0ePdSuOHHtl699pBS9RiOnQoRI9TY8DNhwzKYfpv9v1knoIuaaQYOI0y+TntuGzOR40y+WNn3ETf9naGmEM4hKb1/b/9BcOHzcMaDMP4y9QHfVZEH5boBVdzMaIRPb1KunEvfOFbkQWVUX75c9ZpdfmXHTcvzNih3oWVxuhm3Qe2Rtn68PYQWes3AYcQctUBmUUrAJoPbioP1eQx9f2fU+bDg3c75qV2VVtdNPqz7fyoJ75y/QdRxPsRiJsGFL/z/9u4vRsrqjOP49wfrQhFlURCxEKEpigtRtKnVatRKFLSEqvFCY5QLxRtjtakxEhNNG2+88F/VNDEtbWIaNYoiIcZWKBc2JCAi6gKiGDFgtKtJicYLrfXx4pwNA7LL7M7Qd97D75NMdt4zu5vnyZ595p3znvectOLmqluH3sR+85PQPR7mXLmvbeaFadhr/aPpzeUw8Vo3Vo6JJ8Nvth36I/SJLvRtM2oUnNbkZiidrntc2lTlnZfS/PzeX8FPbzr0z829Kt1hve6+NHR04UHG27/6Il0knnvV/jN1JPj5r+H5pbBzDZwyyLTgFvmM3soyfvKhZ9FMPi3ddXrMSU1Pj7MjxLHT0u5VPdNh8aPNz8i64A44/ZpU7P/18PfP7Le+kJezuOH7PzvnyvRpYn0TdwSPkAu9HXm6x6VFu5qdKWFHjp7pafhvsHH5wUiw+A/QewWsuRdW3JQ2Sx+w+cnU5w62nMXoo9LaQrte3bdOU5u50NuR6erl8MsHqo7COs38e+H6lfs2rBmOrjHpDWL+PdC3ApYvgL2709z+PRvTRdjBPiH8ZEnagH39Yy2FP2hoh+W3mnW6/+ddnFYfJxx6w/ohDSxnMWVuOqt/4qJ0l/aorqGn8o6dkG6i+vrL/TfGaRMXejOzdjtlQZpb/9S1abG92YvS9aOhHLB5ezu50JuZHQ6TZsHStfDqA2mxugq50JuZHS5jJ8Alv686Cl+MNTMrnQu9mVnhXOjNzArnQm9mVjgXejOzwrnQm5kVzoXezKxwLvRmZoVTDLVQfgUkfQp82MKvmAR81qZwqlZSLlBWPiXlAs6nkzWby8kRcdB1Fjqu0LdK0qaIKGLH55JygbLyKSkXcD6drB25eOjGzKxwLvRmZoUrsdA/UXUAbVRSLlBWPiXlAs6nk7WcS3Fj9GZmtr8Sz+jNzKyBC72ZWeGKKfSSFkraIWmnpLuqjme4JC2X1C+pr6HtOEmvSHovf51YZYzNkjRd0jpJ2yRtlXRbbq9rPmMlbZT0Zs7nd7l9pqQNuc89I6m76libJWm0pDckrc7Hdc5ll6S3JW2RtCm31bKvAUjqkfScpHckbZd0bqv5FFHoJY0GHgcuA3qBayX1VhvVsP0VWHhA213A2oiYBazNx3XwDfDbiOgFzgFuyX+PuubzFXBxRJwBzAMWSjoHuB94KCJ+DPwHuLHCGIfrNmB7w3GdcwH4RUTMa5hvXte+BvAI8HJEzAbOIP2dWssnImr/AM4F/t5wvAxYVnVcI8hjBtDXcLwDmJqfTwV2VB3jCPN6EbikhHyAccBm4GekuxW7cvt+fbCTH8C0XCwuBlYDqmsuOd5dwKQD2mrZ14AJwAfkiTLtyqeIM3rgh8DuhuM9ua3upkTEx/n5J8CUKoMZCUkzgDOBDdQ4nzzUsQXoB14B3gf2RsQ3+Vvq1OceBu4Evs3Hx1PfXAAC+Iek1yXdnNvq2tdmAp8Cf8lDa3+SdDQt5lNKoS9epLfyWs2FlTQeWAHcHhGfN75Wt3wi4n8RMY90Nnw2MLvikEZE0iKgPyJerzqWNjo/Is4iDd3eIumCxhdr1te6gLOAP0bEmcCXHDBMM5J8Sin0HwHTG46n5ba6+7ekqQD5a3/F8TRN0lGkIv+3iHg+N9c2nwERsRdYRxre6JHUlV+qS587D1gsaRfwNGn45hHqmQsAEfFR/toPvEB6I65rX9sD7ImIDfn4OVLhbymfUgr9a8CsPHOgG7gGWFVxTO2wCliSny8hjXV3PEkC/gxsj4gHG16qaz6TJfXk5z8gXW/YTir4V+dvq0U+EbEsIqZFxAzS/8k/I+I6apgLgKSjJR0z8By4FOijpn0tIj4Bdks6NTfNB7bRaj5VX3xo40WMy4F3SWOnd1cdzwjifwr4GPgv6V39RtLY6VrgPWANcFzVcTaZy/mkj5ZvAVvy4/Ia53M68EbOpw+4J7f/CNgI7ASeBcZUHesw87oIWF3nXHLcb+bH1oH//br2tRz7PGBT7m8rgYmt5uMlEMzMClfK0I2ZmQ3Chd7MrHAu9GZmhXOhNzMrnAu9mVnhXOjNzArnQm9mVrjvAA+4YiDEJ4+UAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "fo_odY_Z-Fc-",
        "outputId": "151dfc5a-1714-4ece-d52f-ee7aab88d502"
      },
      "source": [
        "history_df1.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df1['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.970416784286499\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj3cRv7MoQPB"
      },
      "source": [
        "##### wow that is a nice looking curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "sAvi3T0k-FaZ",
        "outputId": "28854671-7e9a-47dc-c7f1-ba7259c2ecba"
      },
      "source": [
        "history_df1.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df1['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.9227414131164551\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3yV5dnA8d+dPckgIQQSEvYm7Km4QK1bEScqqFit61Vbq7WtVGvrW33raK0bEbfiABUXCnUhe+8RCEkIWZAEsnOu94/7JISQcbI4yeH6fj7nc3Keda4nObnO/dzrMSKCUkopz+Xl7gCUUkq1Lk30Sinl4TTRK6WUh9NEr5RSHk4TvVJKeTgfdwdQU1RUlCQmJro7DKWUaldWrVqVLSLRta1rc4k+MTGRlStXujsMpZRqV4wxe+tap1U3Sinl4TTRK6WUh9NEr5RSHk4TvVJKeThN9Eop5eE00SullIfTRK+UUh5OE71SbcHu/8L2r9wdhfJQbW7AlFInlZIC+OohWP26fX3u/8LYW90bk/I4muiVagkVZbDsRRh6DQRFurZP8vcw/3bIS4UJ/wM5O+HL30PpYZj429aNV7WuwlxIWw1pK2H/evANhPB4CIuH8G72ObQzBISBMa0ejiZ61T7l7IJf/gOjfw3RfdwdDWxZAF8/BEeyYPJf6t+2tBC+/QssewEie8KNX0H8aKgoh/m/ge8etcn+rIdPSBLgcKZNRhUlUO58VJRAdH/oNqb137+mggw4kg2dB534926O/HT47jFIWQq5u5wLDUT1hopS2DwfHGXH7mO8Iajj0UdsEpz7txYPTRO9an/SVsFbV0BhNqx5C85+FEbdfGKSYl1WzbHPq1+H0x+wJbjaiMAbl8C+ZTDmVpvM/YLsOm8fuOQF8A2CH5+C0iO2KserlZvS3r4S0lcfv9x4w41f2i+huhzcY6ueygrBJwC8/eyzf4g9v6jejYulMBdePRsOH4BblkCn/o3bvzkcDshYB9k7bWJ2lNkrtYoyCI6CgZeCt2/t++7+L3x4k/2b9TwThk2DuJHQZRj4hzqPX2G/xPL2waF9cCQTCnOqPXKh6GCrnJometW+7PgG3r/e/uNdsRB+/Ccs/C1s/xIufs5eDrviwCbb+Hkk25bCj2TZn+NHwQVPNS6mnF22GqbXJNi5CDbMg+HX1b7t9i9tkr/gKRh54/HrvbzsOr9gWPpvm0Av+nfrfYntX2eT/Km/hf4Xgo+/fYjA3Evgo1vg1h+OJqvqyorgvWmQu8deVZVnOa8Kiu3vctMncP1810vmFeXwwQ1QsB/8QuDDm2Hmdzae1lJaCMn/hW1f2M/D4Yy6t13yOJz1Zxhw8dG/h8MBPz0F3/0VOvaG6Z9DdN/a9/fyhrCu9tFtbMufSz000auWJQK7voUuw12vq3bVmrdgwZ0QMxCunQehMZAwHla8Al//Ef4zDi561ias+pQchjkXQFGuTSjBURAcbZPqmjfhV/+ou+RWm9Wv29Lvxc/BG5fZuvph045PziLw3/+F8AQYVscXAdj9zv6rLdl//w8I6wan/971eBpj9Vzw9ofxd0BgxLHrLnsRXjsPvnwQLv738fsu/C1kbIBrPoA+Zx+7LnsnzL0IXr8ArvvYlmwb8s2f7Rfmxf+xf4+3p8K3j8A5j7l+Pvn77RfEpIfrvxIB+PpPsPwl+8XkFwq9zoQ+50LXEfbLxcvXXqF4+0DKMlg0y34RxY2CyY/Yq42Pb7Vf3oOmwIXP2iuZtkhE2tRjxIgRotqpshKRj38j8nAHkSf7iST/2DLHdThE/vsPe9zXLxYpzj9+m8xtIi+cardZ83b9x/vpWbvd3qXHLl/7rl1+YLPrsZWViPxvD5F3rrGvV75mj7Hnp+O33f6NXbfyNdeO7XCIfPRru8/Gj1yPyVUlR0T+Fi8y7+a6t/lmln3/zZ8eu3zV63b5t3+te9/cZJGnBon8LU5k7y/1x7LmbXu8hfcfXfbZfXbZzm8bPJUqH99m93nxdPv7q8veX+x2715rj19W0vCxK8pFVs21n+2HO4j8PV7kLx1Ffnmh/vc6QYCVUkde1X707Ulemi2FHM5ydyTHK8yFNy+DtW/aBlLfQFuaW/K/tm7SVY4KSF0Fq16Hhffbkvc/uttL4yFXwjXv116NEN0HblpkS1uLZtm60tqUFcHP/4Lupx1/+Rwz0D5nbHQ93m2f27aCETPs68FXQEC4LdVXJ2JL5x3iIOka145tDFz4DMSPgY9vg/Q1rsflii0LoCQPhl9f9zanP2gbCBfcaeuXwVb3fP5b6HGGbY+oS0QizPjCls7fuBSSf6h9u7RV8OndkHiqvZKpdPajENXXnvuRnIbPZ/96WPs2dBpoq6O2fFr7diL2MxLcCS590dap+/g1fHwvb1sld+cq27YS3Q9mLIQxv3Zv+5ALNNG3J5/fBz8/C69OtvXCLeHHp2HeTbZxqKlydtmY9i2DS1+C8/4Bv/6vTXpL/gZzL7aX1A1xVMC718ArZ8Knd9lqlPJiWyd6yQv2n7K+f0gfPzj7MVvP+vO/at9mzZu2oW/i745fF9XHXq4f2ODaeYNthA3rBj3PsK/9gmzi3PKp/WKulPy9/f2c8j+uJZVKPv5w5Vu2eumda1z7Pbpq9VyI7AGJp9Tz/n5w2Sv2C3L+7fYL/b3rbPKe8qpNfvUJi7PJMDwe3rrcNtwue8nWiWdstJ+dd6fZariprx9bZeYbCFNesVVsC+60CbouIrbXU2AETP/MfkF896it969pxzeQ8jOcdr9tC2ksvyA49V646euGq4faCE307cWORbD9Cxh6LZTk28Sa2sw7ceWl2pLyxnnw3BhY+p/aS9+H9tkriRcn2n/y7x6zDY4HNtneBq+cZRPA9Qsg6Uq7j3+oreO95HlbYnthAuxaXH88S/5u6ztP/wPctQYeTIWbF9lS7dCrXSs1dRtjvxh+euZoCbRSealdHj+29uTm42dLaQc2Nfw+ALm7YfcSm9irJ7xRNwMCK2cfXfb9ExDSuf66+bqERMPV70Jxnv0iLCtq/DFqyt4Be3+ysTf0e43uY0vXOxfBS6fZboRXvA7BHV17r9DOtpEyfjQsfxm++B28c5X9TPxrOBQfgqverv14sUNs6Xnb50cHldVmx9f2y/T0B2zb0Jl/hOztsP7dY7dzOGzX1ojuMGK6a/F7AE307UF5KXz5gC19XfAU3PSNTaRzLrAlo6b60dm7ZPpCSJwAXz0IL59pL81FIOUX28PlmSRY+pxtsMrcDD88abuSPT/eNrgFRcHMbyFh3PHvMfQa+PX3EBIDb18BWz6rPZbNC2wyHDbNlrQiezS9W+FZD9sucYtrNOKtf892bZv4u7qTW8xA16tuVs+1jbDDrj12eUQC9PmVLe2XFcPen2HPDzDhbvANaPTpALbnypSXbfXN/NvrL926GruXj+vVSKNuhl6T4VAKnPt323WwMYKj4IZP4aEMuG873PwtXP4aTH4UbvgMOg+ue9+xv4Eep8MXv6/981NRbgsikT2PVqH1v9B2CFjyuB0XUGnDB3Bgo/0iaEyDe3tXV+W9ux7aGFuLysbDbV8eXVZwQOTF00RmhYuseLXxxzyUKvJIlMiCu+xrh0Nkw4ci/+hlj/nv0UcbnL7+k8jBlKP7lhaJZGwUWf+ByNLnRQpzG36/wlyRl88SmRUhsu69Y9cd2CLyWBeRl86wx24JXzxgzyNjo31dXibyzFCRFybW33BW+bs+nFX/8ctKRP7RU+Ttq2tfv2uxs2H4LduA/I+etvGzuSobpXctbvoxKmOvbEB2VdEhkW1fuafh8XCW/Xw8HCay9D/Hrlv+cu0Nxju/s8srty8rsY3Dz58iUlFxYuI+gdDG2DYkc6stFbmq4IBt0Ox9NvQ55+jykE62JNRrEnx2j616WTQL9q2wl6cN+fEpEAecep99bQwMugzuWG5LRX4hcP4/4d4ttitZePzRfX0DbMl38OV2Xpaa3fJqExhhu9kljLd9s1e+ZpcXHbLVEb6BcMUbTS/x1jTxd/aq55s/29ebPrZVLfWV5gFinH2+DzRQqt/+he17X9flf/fTbDXQt4/A7sUw/s6jA6OaY9yd9ne54tX6tysvhY9+DavfOL70v/1LG/vwGxr33gFhthulOxoeg6Ps573f+fbqduH9tpqxOB8W/x0SJth11fU8A7pPhO+ftHMKrXrN/u9Nerj1B6G1MdqP/kSqKLcNk+Hd4OZvXNvn27/YBslz/n78Ov8QuOod+wHesgB+etYm8JAY2x/4lHsgsvvx++Wl2frOodfaWKoLjIAL/tn4c3OFfyhc+wG8fwN89j/2n2/Pj3Bor72sD+vacu8VFGmT+td/tHXLPzxph/T3Pa/+/SoTfcZGW11Ql1VzbA+aXmfVvt4YGH0LfH4vBEbCyJuacBK18A2w9fxLn7N15R261L7d+nePPnZ8bds5Ksc1rH4dOnStO/a2yi8Irphrv7yX/tsm7YhE2+vp7A9q/wI662HbhvTff9geOYmnQk/Xz7u8wkFZhVBa4cDhECpEqp79vL3oGNL4wVwiwpHSCny8DAG+DTRmtxCXEr0x5lzgGcAbeEVEHq+xPgGYDUQDucA0EUl1rrsB+KNz07+KSD0tKh5u92LbI+RwBuQm156Eq0tdBWvfsnW7Ub1q38bbB0bPtI+ig7ZHwdbPbWPp9i9tAq05Uu+np48tzZ9IvoFw5Zvw0Uz45k922XlP2pJ+Sxt9i238m3ejbcic8mrDJbmQaPtFWV+J/uAe2PWdbTSur9dJ0lXwwz9h3O0tO5Bm5Azbq2jV63DGg8evd1TY3lSxSTDwMtv7JG2V7bUUkQg7v7XtIA31mGmLvLztAKqIRPjifvs5HnwFdB1e+/ZxI6HfBba3GsCkv9R5RVJSXsFDH2/kq00ZlJY7KKtw4GigKaRHdDATe0dzSq8oxvbsSIi/TakFxWVsP3CYHQcK2Jl5mP15xWQWFJNZUEJmfglFZbbTQ4i/Dx1D/IgK8ScqxI8BsWHcPamR00a4oMFEb4zxBp4DJgOpwApjzAIR2VxtsyeBuSLyujHmTODvwHXGmEjgYWAkIMAq576tM6FDW7fuHdugWVoAGz+sf4ZCh8P2TgiJqb0rYG0CI2DIFfaRuQVev8iObLxhwdE+4vnptjQ69BrbaOgOPn426YbF2e6Do25upffxh0mzYN4M21A38FLX9osZVH+i3zzfPg9toCHTLxjudbEHT2NE9rBVdqvm2M9QzUbFzfPtpFpXzLU9kHqcZkeLvn6h7cUCttG7EcorHLy1LIUfdmRx/7n96BNTy1iGE2n0THs1uvQ5WxVTnzP/CNsW2qqduBG1blJQXMav31jFz7tymDI8jqgQP/x8vPD19sLPxwsfL4O38+FlDD5ehoLicn7elc17K/Yx5+c9+HgZBnTpQHZBCel5xVXHDvD1oktYINGh/gyJCycm1J/oUH/KHUL24RKyD5eSXVBCcvaRZrex18WVEv1oYKeI7AYwxrwLXAxUT/QDgHudPy8GPnH+fA7wjYjkOvf9BjgXeKf5obczxXm2pD3sOptENsyrP9FveP9oKay2AUIN6dTfDlZ5/ULbO+f6T2wJ70c3luar8/Zp3ND2php4KexbDr0nuV6C7TwIfnne9typrWfGjm/soJzq7RYnSEFxGYcKy4gfdTO8c6X9TA285OgGIvYqIqoP9HNOBdFlmO359OUDtrdNz7OOr7Krx9JdOfzl001szSjAz8eL73f8yIO/6sf08YmYRtTX5xWWEeDnhb9P7X+HwyXlfL0pg/lr08k9UsqpvaM4o18nhsWH4+N97JVY9uES1lUMZW/PZxhyMIghwQ78fOq4WuvUn9IbFuLbqS+1RZtZUMz02SvYfqCAf16RxGXD41w+p5kTe1BSXsGqvQf5YUc2a1IOMrp7JL1jQukTE0rfmFDiIgLx8nLvgCpXEn1XoPpomlSg5tyl64DLsNU7lwKhxpiOdex7XEWsMeYW4BaAbt1c/wC2K5s+tnXtQ6+2XeQ+v8/2164saVfncMAP/wcxg+1laVNF9YIZn9uS/esX2jlEVs2BpKvtpe/JwBj41eMNb1ddzCA7e2H29uP/PsX5dhracbc3OaTswyX4eBnCg+ofOJVXVMZPO7PZuj+fLRkFbM3IZ1+u7UM/JiGMOUFdCVjxCqZ6ot/xjR3wdcnzlAt4i9hk7BcMF/0Lhk5zOcmnHyrisYVb+Hz9frqGB/LCtOGMSIjk9x+u5y+fbmbxtiyevHwInTrU3oB+qLCUX3bn8svuHJbuymHbgQJ8vQ19O4cyJC6cIV3DGBwXRtrBIuavS2fR5gOUlDvoGh5I1/BAXvx+N/9ZsouwQF8m9ommX+dQNqfns3bfIdIOHTuWINDXm1HdIxnfsyMjEiLILihhS0YB2zLy2ZZRwN7cQhIi13FRUhcuGtqFXp1s4WlP9hGum72MnMOlvHLDSE7v28ml3011/j7ejO8ZxfieUY3e90RpqcbY3wL/NsZMB74H0gCXx72LyEvASwAjR45spYsXN1v7ji1ldRluJ7VaeL/t01tbot/+pU0yrtQpNySyhx2Z+PqF8N61tt+3u0vzbV1Vz5tavoiT/wuOctunvJHyCst49rsdzF26B2MMFyV14YZxiQyOCztmux0HCpjz8x4+Wp1GUVkFXgZ6RIeQFBfOVaO64e1leGPpXp4tOJXfF77LZ98uYdLEU8kvLMXnq7/h5deZG3/syvr3v0SAsEBfwgN9CQuyz+N6FnL16DJCA2rvR55zuITZPyUz+8c9OES4Z1Iffn1aj6qGw1dvGMmby1L462ebOfeZH3j4wgGEBfqyL7eQfQeLSMkpZE/OEbYdKEDEVl2MSozkwqRYjpRWsD71EJ+uS+ftZUd7n0UG+3HlqHguHtqF4d0iMMaQV1TGjzuyWbwtkyXbMvl0XTpxEYEM7RbO9PGJJMWH0y0yiLX7DrF0VzY/78rh8S+2Vh3Ty0Bix2AGdOnABUO6sGbfQf61eCfPfreTAbEdmDwghjd/2YsAb88cy9D48Eb/TdsLVxJ9GlD9GjXOuayKiKRjS/QYY0KAKSJyyBiTBpxeY98lzYi3fcrdDft+OXojieAoO7/Ghg/hzD8fn8x/etqWugZcUvvxGiu8m63GefsK2+2voUbgdkBEKHcIZRUOysqFkooKyiqEkrIKMvKLSTtYRPqhYtIOFZKRX0JSXBjXjUugU6gL3TejettZCzM22PaO6nZ8Y9tZasyTsyk9j682ZhAfGcTo7pF0iwyqqtYoLXfw5i97efa7HeQVlXHFiHh8fQwfrU5j3qpUhjkTV5CfD6//vIcfd2bj5+PFJUO7cOWoeAZ2CTuud8bNp3Tn21XRlC2cR9bi/5C0uIikik2877+Gv1TMwKejHzed2glfLy8OFZVyqLCMvKIy0g8V87eFW/nXtzu5dmwCN05IrCqR788r4uXvk3lneQrF5RWcPziWB37Vj7iIY7uFGmO4bmwC43pEcve7a7n73bVV6/x8vIiPCKRbZBC/GhTL+F4dSYoLP65axeEQ9uYWsj71EGGBvkzoFYVvjeqZsEBfzh8Sy/lDYnE4hIKScsICj/9yOjesM+cOstNTZ+YXsy41j5gO/vTuFEqg37G/t8z8Yj5bv58F69J55tsddA0P5I2bRtMjuo3OOtlCjDRQ+2+M8QG2A2dhE/wK4BoR2VRtmyggV0QcxpjHgAoR+bOzMXYVUNkkvhoYUVlnX5uRI0fKypXNHNrf1iz+m+3edc+mo10I170HH99i7y5UPWmk/AKzz4FfPQFjbmnZOCr/1m1kAqayCgc+XqbBet7P1qfzf19v50hJOSXlDkrLHZSUVzTYIwIgOtSfqBB/tmbk4+vlxSXDunDzqT2Oa0wsLXeQknuEnMOlGGMY9On5lAV0ZNe5bxDo601CxyCCfL3hqYG2h8eVb1JcVsHn6/fz5rK9rEk5dMzxOoX6M6p7JANiOzBvVSrJ2Uc4pVcUD53fn/6xHQDILy5j3spU3vhlL8nZdhK22LAApo1N4OrR3YgMbnhOHPlwJhVbF/L3fh8zPe1hOhduR+5ej19g3XO4bEjN44Xvd/HFhv34eHlx2XD7mfxwdSoOgUuGduW203vSq1PDya+03MH327MIC/KlW2QQ0SH+bq+PdlVGXjFhgb7HfRm0V8aYVSJS65DlBhO98wDnAU9ju1fOFpHHjDGPYEdiLTDGXI7taSPYqpvbRaTEue+NwB+ch3pMRF6r7708LtE7HPBskq1CuX7+0eUlBfBEbzt8/vz/O7r8nattsr9nY9MmXGqDRITtBw6zMS2PXVmH2Zl5mJ1Zh0nJKaR/bAdevWFknfW8i7dmMnPuSnrHhJIUF4a/jxf+vt74+3jh5+wRUdkzovJ1p1B/uoQHEhseUNXwl5x9hNk/JvPBqn0Ulzk4rU80/WJD2Z11hF2Zh9mbW0hFtW+OJ31f4DSvdYwqeb5q2biQA7xTfg8fdv096ztdxIJ16RwsLKNHdDDXjklgyvCuZBaUsDw5lxV7clmRnEt6XjG9OoXw0Hn9Ob1vdK1fag6H8NOubIrLHJzRN/q4hsd6pSyD2WfbQW6rXrNXjafe2/B+wN6cI7zyQzLvr9yHAFeOjOeWiT2Ij2yBgV3qhGt2oj+RPC7R7/kR5pxvZ3WsnPCr0gcz7ERM9221vTuytsFzo+G0B2rvH92OlJY7WJacw6LNB1i0JbOq8czHy5DQMYhenUKIiwjineUpRIX48+ZNY+jW8dgEszw5l+teXUafmFDenjmmzjrlxjh4pJS3lu1lzs97ySsqpXtUMD2jQ+yjUzCdQgMQgdgtr9Jz9d/48ZKlHPIKZ29OIQlbXuaCzBc41+tFdhZ34OyBMUwbk8C4nh3rvCrJKighIsi3ccm7MUTghVNtA6x/GNyzwY5gbYS8Qnsf07Cgk2juFw9UX6LXkbGtbd07djqB/hccv27w5bDpIzsDZO9JdlCHT6Ad6NPGZeQVM/unZOavTcNgCPb3Jtjfh2A/H3y8DWtSDnG4pJwAXy9O6RXNXWf1YkRCJAkdg46piz1/SCwzXlvB5S/8zNybRtOvs63W2JSex01zVtA1IpA5M0a1SJIHiAj2444ze/Ob03vhEKk7AXuPgdVwSmgG9Bxgl+3dBDGD+PK2q6hwCN4uVFFEh7bibfDAVsONusmONB49s9FJHjTBnww00bem0kLYNN82qtZWDdNrkv3H3PABxAyw9fYjZ7g+/WsrOZBfzO6sI3TrGERsh4Bj6lx3Zhbw4n9388naNCocwqT+MYQH+XKkpIIjpeUUllRwuKScC5NimdQ/hvE9o+qtAx3eLYIPbh3Hda8u44oXlvLajNFEBvtxw+zlhAb48OZNY5o0zLwhXl4Gr1p7VTvFOGdTzNhoG86rulXeAeBSkj9hkq62U1dXztyoVA2a6FvT1s/sKNikq2pf7+NvRy5u/MgOipKKZvXPbo7M/GK+2JjB5+v3s2JvblW7rb+PFwkdg0joGExZhYMl27II8PXi6tHdmHlqy9Tn9okJZd6t47nu1WVMe2UZYYG+OATeuHkMXcIDm338JgnuCKGxR+emr+xW2bvx3SpbnW+AnSZDqTpoom9Na9+2dx9KmFD3NoOn2tGKK162Nxg+gQOZ9uYc4butmXy5MYPle2xy7xsTyj2T+pAUH07qwUL2ZB8hObuQ5OwjHCkp566zenPDuIQWL2XHRwbxwa3juWH2clJyC3ln5lh6urvLW8zAo1Mh7PgG/DvY2/op1c5oom8txfm2FDjh7voHPSVMsCXHgv0w/q5WDam03MGKPbl8tzWTxVsz2e3s0terUwh3ndmbC4bE0tuNc5hEh/rz8e3jOVJS4VLXwlYXM8i2n5SX2ETf4/ST62YVymNoom8tqcvtnDLdT6t/Oy9vO5Ng1jboMrTVwtmZWcCMOSvYl1uEn48XY3t05LpxCZzRtxOJUW2nG6e/j3edc6GccJ0Hg6PMThJWkN42q22UcoEm+tayd6mdbiBuVMPbjryxVUNZtjuHW95Yha+34YVpw5nYJ5ogP/3TN6hy+oMfn7bPvSa5LxalmkH/21vL3p/tbJEtOQ95E3y6Lp373l9HXGQgr88YrYNhGqNjb/D2h0zbrbLOm3wo1cZpom8N5SV2iuHRM1vl8CJCfnE5mfnFHMgvIbewlC5hAfTqFFI1I6KI8PIPu/nbwq2MSozg5etHNjhboqrB2wc69bM3S9dqG9WOaaJvDWmroaIEuo1r0cN+siaNZ7/dQdqhIkrKa78vbFSIP707hRDg68XibVmcPySW/5uadMJuWeZxYgbZRN+E2SqVais00beGlJ/tcwsl+sLScv48fxPzVqWSFBfG9eMSiOkQQKcOAcSE+hMR7EfawSJ2ZBaw44CdR2ZHZgG3nd6T353dt91MMtUm9b8QDu6F+NHujkSpJtNE3xr2LoWovi0ywnXL/nzueHs1u7OPcOeZvbj7rN61DtvvExPKGf0af9ME1YC+v7IPpdoxTfQtzVEB+5bBoMuadRgR4e3lKTzy6WY6BPry1k1jGN+r7d7BRinVdmmib2kHNtl5R7qNb9LuWQUlzF+bxoer09iyP5+JfaL55xVJRLXCfC9KqZODJvqWlrLUPie4nuhLyiv4dksmH65KZcn2LCocQlJcGI9fNpgrRsZrHbtSqlk00be0vT9BWDyExze8LbAxLY+7313DrqwjxHTwZ+apPZgyvKtbpyJQSnkWTfQtScQ2xPY4vcFNKxzCC//dxVPfbCcqxJ+XrhvBWf1j2tb0t0opj6CJviXl7oYjmZBQf7fKfbmF3Pv+WlbsOcj5Q2J57JJBOphJKdVqNNG3pL2V/efrrp9fsC6dP3y0AQM8dWUSlwzt2uDNsZVSqjk00beklKUQGAnRfWtd/cmaNO55fy0jEyJ46sqhxEXovDNKqdanib4l7f3ZjoatpYT+1aYM7vtgHWO7d+S1GaN0SgKl1AnTSremPwkVZMDB5Frr53/YkcWdb69hcNcwXr5hpCZ5pdQJpYm+pVTWz9foP79iTy4z566kZ6cQXp8xmhB/vYhSSp1Ymuhbyt6fwTcYOidVLWU62GQAACAASURBVNqQmseNr62gS3ggb9w0mrAgvQ2dUurE00TfUlKWQvwoO4c5UFxWwYw5KwgL8uWtm8foFAZKKbfRRN8SivPsHDfVulV+tn4/2YdL+MflQ4gNC3RjcEqpk50m+paQtR0QiB1SteiNX/bSq1MI43o0f6pipZRqDk30LSF3t32O7AnYuvl1+w4xbUw3HQyllHI7lxK9MeZcY8w2Y8xOY8wDtazvZoxZbIxZY4xZb4w5z7k80RhTZIxZ63y80NIn0Cbk7gYMRCQA8OYvewn09eayEXHujUsppXBhwJQxxht4DpgMpAIrjDELRGRztc3+CLwvIs8bYwYAC4FE57pdIjK0ZcNuY3J32xkrffzJKypj/ro0LhnalQ4B2stGKeV+rpToRwM7RWS3iJQC7wIX19hGgA7On8OA9JYLsR3I3Q2R3QH4aHUqxWUOpo1NcHNQSilluZLouwL7qr1OdS6rbhYwzRiTii3N31ltXXdnlc5/jTGn1vYGxphbjDErjTErs7KyXI++rcjdDZE9EBHe/GUvQ+PDGdQ1zN1RKaUU0HKNsVcDc0QkDjgPeMMY4wXsB7qJyDDgXuBtY0yHmjuLyEsiMlJERkZHR7dQSCdI0UEoyoXIHizdncOurCNamldKtSmuJPo0oPrtkuKcy6q7CXgfQESWAgFAlIiUiEiOc/kqYBfQp7lBtym5yfY5sgdv/rKXsEBfLhgS696YlFKqGlcS/QqgtzGmuzHGD7gKWFBjmxTgLABjTH9sos8yxkQ7G3MxxvQAegO7Wyr4NsHZtTLHvytfbzrAFSPjdNIypVSb0mCvGxEpN8bcAXwFeAOzRWSTMeYRYKWILADuA142xtyDbZidLiJijJkIPGKMKQMcwK0ikttqZ+MOzhL9ezt9KHcI14zRahulVNvi0lSKIrIQ28hafdmfq/28GZhQy34fAh82M8a2LXc3EtqFuSszObV3FN2jgt0dkVJKHUNHxjZX7m4OB8eTkV/M1JHxDW+vlFInmCb65srdTbqXbXwd0z3SzcEopdTxNNE3R0kBHMlkS0k0XcMDiekQ4O6IlFLqOJrom8PZELs8L4zhCRFuDkYppWqnib45nF0r1x6JZHi3cDcHo5RStdNE3xzORL9XYhjeTUv0Sqm2SRN9c+Tu5rBPJBW+wQzoctzMDkop1SZoom+Og3tIoTNDuobj662/SqVU26TZqRkkZxdbS6MYlqD180qptksTfVOVFWEK0tldEcMIrZ9XSrVhmuib6uAewNkQq10rlVJtmCb6pnL2uCkOTSQqxN/NwSilVN000TeR5OwCIKpbXzdHopRS9XNp9kp1vCMZOyiVEAb06ObuUJRSql6a6JuoMGMHadKZYdoQq5Rq47Tqpol88/aQajrTr3Oou0NRSql6aaJvivISOpQeoCQ0ER8dKKWUauM0SzVBUdZuvHEQ2LmXu0NRSqkGaaJvgr07NgLQKWGAmyNRSqmGaaJvguyULQD06jfEzZEopVTDNNE3QUnmLg4TTERUZ3eHopRSDdJE30giQkD+Hg4GxIEx7g5HKaUapIm+kfbmFNLFsR9HRHd3h6KUUi7RRF+f3N3wzZ9h2xdQWgjA2r2ZxJlsgjv3dnNwSinlGh0ZW581b8JPz9iHtz90P5Www93wNRVExvdzd3RKKeUSTfT1ydwKHXvBeU/A9q9hx1eckbsIAK/oPm4OTimlXKNVN/XJ2gqdBkDPM+FXj1N820omlf6TD/s8AXGj3B2dUkq5xKVEb4w51xizzRiz0xjzQC3ruxljFhtj1hhj1htjzqu27kHnftuMMee0ZPCtqqwYDiZD9NEqmk3peex0dCY06SLtcaOUajcarLoxxngDzwGTgVRghTFmgYhsrrbZH4H3ReR5Y8wAYCGQ6Pz5KmAg0AVYZIzpIyIVLX0iLS5nJ4gDoo/ON78m5RAAQ7vpPWKVUu2HKyX60cBOEdktIqXAu8DFNbYRoIPz5zAg3fnzxcC7IlIiIsnATufx2r6srfa5U/+qRWv2HaJreCCdQgPcFJRSSjWeK4m+K7Cv2utU57LqZgHTjDGp2NL8nY3Yt23K2grGyzbGOq1NOaSleaVUu9NSjbFXA3NEJA44D3jDGOPysY0xtxhjVhpjVmZlZbVQSM2UtRUie4CPvR9sZn4xaYeKGBaviV4p1b64kozTgPhqr+Ocy6q7CXgfQESWAgFAlIv7IiIvichIERkZHR3tevStKWvbMQ2xa/bZ+vlhWqJXSrUzriT6FUBvY0x3Y4wftnF1QY1tUoCzAIwx/bGJPsu53VXGGH9jTHegN7C8pYJvNeUlkLPrmES/dt8hfL0NA7uEuTEwpZRqvAZ73YhIuTHmDuArwBuYLSKbjDGPACtFZAFwH/CyMeYebMPsdBERYJMx5n1gM1AO3N4+etzsAqk4tkSfcpD+sR0I8PV2Y2BKKdV4Lo2MFZGF2EbW6sv+XO3nzcCEOvZ9DHisGTGeeFU9bmyir3AI61PzmDoizo1BKaVU0+jI2NpkbTumx832AwUUllZojxulVLukib42WVsgIhF8A4GjA6WGxUe4MSillGoaTfS1qdHjZu2+g0QE+ZLQMciNQSmlVNNooq+posxOf3BMQ+whhsaHY3R+G6VUO6SJvqbc3eAor0r0+cVl7Mw6zFCttlFKtVOa6GvK3GKfnZOZrd+Xh4gOlFJKtV+a6GvK2gYYiLI3Flm77yAASTr1gVKqndJEX1PWVohIAD/b8Lom5RA9o4MJC/R1c2BKKdU0muhrqtbjRkRYu++Q1s8rpdo1TfTVVZRDzo6qRL8vt4icI6VaP6+Uatc00Vd3MBkqSqsS/Rpn/fxQrZ9XSrVjmuirq5zjxtnjZkNqHn4+XvTtHOrGoJRSqnk00VeXeWyi35SeT//Oofh6669JKdV+aQarLmsrhHcDv2BEhE3peQzQ+eeVUu2cJvrqqvW4ST1YRH5xOQO7dGhgJ6WUats00VdyVED29mrVNnkADOqqJXqlVPumib7SwT1QUQLR/QFbP+/tZeinDbFKqXZOE32lqh43tupmY1oePaOD9daBSql2TxN9papEb+e42ZSezyBtiFVKeQBN9JUyNkBYN/APJaughMyCEgZoQ6xSygNoogcQgZRfoNsY4GhD7EAt0SulPIAmerANsQX7ods4wFbbAFqiV0p5BE30AClL7XNVos+jW2SQTk2slPIImugB9v4MAeFVPW42pefrQCmllMfQRA/O+vlx4OVFfnEZe3MKNdErpTyGJvrDWXYO+m5jAdjsrJ8fqCNilVIeQhN9Zf18wnjgaEOsluiVUp5CE33KUvAJgNihgG2IjQ71p1NogJsDU0qpluFSojfGnGuM2WaM2WmMeaCW9U8ZY9Y6H9uNMYeqrauotm5BSwbfIlKWQteR4OMHwKa0fAZpaV4p5UF8GtrAGOMNPAdMBlKBFcaYBSKyuXIbEbmn2vZ3AsOqHaJIRIa2XMgtqOQw7F8Pp94LQHFZBTuzDjN5QIybA1NKqZbjSol+NLBTRHaLSCnwLnBxPdtfDbzTEsG1utTlIBVV/ee3ZRRQ4RCtn1dKeRRXEn1XYF+116nOZccxxiQA3YHvqi0OMMasNMb8Yoy5pI79bnFuszIrK8vF0FvA3qVgvCB+NHC0IVbnoFdKeZKWboy9CpgnIhXVliWIyEjgGuBpY0zPmjuJyEsiMlJERkZHR7dwSPVIWQqdB4O/nXN+Y3oeHQJ8iIsIPHExKKVUK3Ml0acB8dVexzmX1eYqalTbiEia83k3sIRj6+/dp7wUUldCt/FVizal5zOgSweMMW4MTCmlWpYriX4F0NsY090Y44dN5sf1njHG9AMigKXVlkUYY/ydP0cBE4DNNfd1i/3roLwIEmz9fHmFg63783XGSqWUx2mw142IlBtj7gC+AryB2SKyyRjzCLBSRCqT/lXAuyIi1XbvD7xojHFgv1Qer95bx61SfrbPzobY3dlHKCl3MKirNsQqpTxLg4keQEQWAgtrLPtzjdezatnvZ2BwM+JrPSm/QGRPCOkE2FsHgs5Br5TyPCfnyFiHwzbEOqttADak5eHv40WPqGA3BqaUUi3v5Ez02dug6GBVtY3DIXy5MYNxPTvi431y/kqUUp7r5MxqNW408svuHPbnFTNleJwbg1JKqdZxcib6vT9DSAxE9gDgw9VphPr76NQHSimPdPIl+pxdsHk+9DkHjKGwtJwvNu7nvMGxBPh6uzs6pZRqcSdXoheBL+4Hb3844yEAvt50gMLSCi4bXuusDkop1e6dXIl+6+ewcxGc8SCEdgbgw9WpxEUEMiox0s3BKaVU6zh5En1pIXz5IET3h9G3AHAgv5ifdmZz6bCueHnptAdKKc/k0oApj/DjU5CXAtM/B29fAOavTcMhcOkwrbZRSnmuk6NEn7sbfnoGBl0OiacAICJ8uCqNYd3C6REd4uYAlVKq9Zwcif6LB2wp/uy/Vi3avD+fbQcKuEz7ziulPJznJ/ptX8COr+C030OH2KrFH61Ow9fbcOGQ2Hp2Vkqp9s/zE/2iWRDVF8beVrWovMLB/LXpnNmvE+FBfu6LTSmlTgDPTvQVZZC1DQZeWtUAC/DDzmyyD5dotY1S6qTg2Ym+IAOQY6psAN5dnkJ4kC9n9O3knriUUuoE8vBEv98+h3apWvTlxv18tekA08cn4ufj2aevlFLg6Yk+P90+d7CJPrOgmAc/2sDgrmHcfkYvNwamlFInzkmT6EWE++etp7C0gqeuHIqvzjuvlDpJeHa2K0i3E5gFRvDWshSWbMviD+f1p1cnHSCllDp5eHaiz98PHbqwO/sIj32+hVN7R3Hd2AR3R6WUUieUhyf6dCQ0lnveX4efjxdPXJ6kk5cppU46np3oC9LZXhTKun2HeOzSQXQOC3B3REopdcJ5bqIXQfL381OmP+cMjOGCIV0a3kcppTyQ5yb6wlxMRQn7ysP0piJKqZOa5yb6Atu1MkMi6Roe6OZglFLKfTw30efbUbEHJIKuEZrolVInL89N9M4S/X7pqCV6pdRJzXMTfX46gqHAN4LIYJ2KWCl18nIp0RtjzjXGbDPG7DTGPFDL+qeMMWudj+3GmEPV1t1gjNnhfNzQksHXKz+dfO8IYsJDMUb7ziulTl4N3hzcGOMNPAdMBlKBFcaYBSKyuXIbEbmn2vZ3AsOcP0cCDwMjAQFWOfc92KJnUZuC/RygI10jglr9rZRSqi1zpUQ/GtgpIrtFpBR4F7i4nu2vBt5x/nwO8I2I5DqT+zfAuc0J2GX56aRVhGv9vFLqpOdKou8K7Kv2OtW57DjGmASgO/BdY/Y1xtxijFlpjFmZlZXlStwNkvx09pWHE6c9bpRSJ7mWboy9CpgnIhWN2UlEXhKRkSIyMjo6uvlRlBZiig9pH3qllMK1RJ8GxFd7HedcVpurOFpt09h9W47zzlIZ2odeKaVcSvQrgN7GmO7GGD9sMl9QcyNjTD8gAlhabfFXwNnGmAhjTARwtnNZ63LecCQDLdErpVSDvW5EpNwYcwc2QXsDs0VkkzHmEWCliFQm/auAd0VEqu2ba4x5FPtlAfCIiOS27CnUwlmiz6IjMR10xkrVvpSVlZGamkpxcbG7Q1FtUEBAAHFxcfj6+rq8T4OJHkBEFgILayz7c43Xs+rYdzYw2+WIWkK+s3aoQyzeOv+8amdSU1MJDQ0lMTFRx4CoY4gIOTk5pKam0r17d5f388yRsfn7OWKCiIzQWStV+1NcXEzHjh01yavjGGPo2LFjo6/2PDPRF6STKZHaEKvaLU3yqi5N+Wx4ZKJ35KWT6oggTkfFKqWU5yb6DEcEcdrjRimlPDDRV5TjXZhpu1Zq1Y1STbJnzx4GDRp03PKbb76ZzZs317KHastc6nXTrhzJxEiFjopVHuEvn25ic3p+ix5zQJcOPHzhwCbt+8orr7RIDOXl5fj4tM30U1FRgbe3t7vDaFGeV6LPPzoqNjZc+9Ar1VTl5eVce+219O/fn8svv5zCwkJOP/10Vq5cCUBISAgPPfQQSUlJjB07lgMHDgDw6aefMmbMGIYNG8akSZOqls+aNYvrrruOCRMmcN111zFx4kTWrl1b9X6nnHIK69atqzWW5cuXM27cOIYNG8b48ePZtm0bYJPyb3/7WwYNGsSQIUP417/+BcCKFSsYP348SUlJjB49moKCAubMmcMdd9xRdcwLLriAJUuWVJ3LfffdR1JSEkuXLuWRRx5h1KhRDBo0iFtuuYXK4UE7d+5k0qRJJCUlMXz4cHbt2sX111/PJ598UnXca6+9lvnz57fEn6DliEibeowYMUKaZfMCkYc7yLRHX2zecZRyk82bN7s7BElOThZAfvzxRxERmTFjhjzxxBNy2mmnyYoVK0REBJAFCxaIiMjvfvc7efTRR0VEJDc3VxwOh4iIvPzyy3LvvfeKiMjDDz8sw4cPl8LCQhERmTNnjtx9990iIrJt2zap738/Ly9PysrKRETkm2++kcsuu0xERP7zn//IlClTqtbl5ORISUmJdO/eXZYvX37Mvq+99prcfvvtVcc8//zzZfHixVXn8t5771Wty8nJqfp52rRpVec5evRo+eijj0REpKioSI4cOSJLliyRiy++WEREDh06JImJiVXxtJbaPiPYAay15lUPLNHb6Q+8w2udYFMp5aL4+HgmTJgAwLRp0/jxxx+PWe/n58cFF1wAwIgRI9izZw9gB3ydc845DB48mCeeeIJNmzZV7XPRRRcRGGirVKdOncpnn31GWVkZs2fPZvr06XXGkpeXx9SpUxk0aBD33HNP1TEXLVrEr3/966pqoMjISLZt20ZsbCyjRo0CoEOHDg1WE3l7ezNlypSq14sXL2bMmDEMHjyY7777jk2bNlFQUEBaWhqXXnopYEeoBgUFcdppp7Fjxw6ysrJ45513mDJlSpurlvLIRF+GD6ERMe6ORKl2rWZ/7ZqvfX19q5Z5e3tTXl4OwJ133skdd9zBhg0bePHFF48Z3BMcHFz1c1BQEJMnT2b+/Pm8//77XHvttXXG8qc//YkzzjiDjRs38umnnzZpeggfHx8cDkfV6+rHCAgIqKqXLy4u5je/+Q3z5s1jw4YNzJw5s8H3u/7663nzzTd57bXXuPHGGxsdW2vzuEQv+ekckHC6RAY3vLFSqk4pKSksXWrnKHz77bc55ZRTXNovLy+Prl3tFfXrr79e77Y333wzd911F6NGjSIiIsKlY86ZM6dq+eTJk3nxxRervmRyc3Pp27cv+/fvZ8UKO8VWQUEB5eXlJCYmsnbtWhwOB/v27WP58uW1vldlUo+KiuLw4cPMmzcPgNDQUOLi4qrq40tKSigsLARg+vTpPP300wAMGDCg3nN2B49L9GWH0tgvkdqHXqlm6tu3L8899xz9+/fn4MGD3HbbbS7tN2vWLKZOncqIESOIioqqd9sRI0bQoUMHZsyYUe92999/Pw8++CDDhg2rSupgvyi6devGkCFDSEpK4u2338bPz4/33nuPO++8k6SkJCZPnkxxcTETJkyge/fuDBgwgLvuuovhw4fX+l7h4eHMnDmTQYMGcc4551RVAQG88cYbPPvsswwZMoTx48eTkZEBQExMDP3792/wPNzFyNHJJtuEkSNHSmWrflMU/3Moiw7GEHTtXM7sp9U3qv3ZsmUL/fv3d3cYJ0R6ejqnn346W7duxcur/ZY7CwsLGTx4MKtXryYsLKzV36+2z4gxZpWIjKxt+/b7m62NCD5HMuwNR8J1+gOl2rK5c+cyZswYHnvssXad5BctWkT//v258847T0iSb4q21TTcXMV5+FQU2cFSOipWqTbt+uuv5/rrrz9m2WuvvcYzzzxzzLIJEybw3HPPncjQGmXSpEns3bvX3WHUy7MSvbNrZb5vNCH+nnVqSp0MZsyY0Wbruduz9nu9VJsCm+gdobFuDkQppdoOz0r0zukPfHWwlFJKVfGoRC/OWwgGdYxzcyRKKdV2eFRFdunBNAqkA7Ed22bLt1JKuYNHlehLclM5IBE6PbFSJ1BISEid65YsWVI1H05N5513HocOHWqtsFQ1HlWiJz+d/dq1UnmSLx6AjA0te8zOg+FXj7fsMZtg4cKFLXKctjq3fdXMkW1gjID7I2hBvoUHOKA3HFGqWR544IFj+q3PmjWLv/71r5x11lkMHz6cwYMHN2q+9fz8fM4//3z69u3LrbfeWjWxWGJiItnZ2ezZs4f+/fszc+ZMBg4cyNlnn01RUREAL7/8MqNGjSIpKYkpU6YcM7fMrbfeypgxY7j//vvp3bs3WVlZADgcDnr16lX1uqa65ss/fPgwM2bMYPDgwQwZMoQPP/wQgC+//JLhw4eTlJTEWWedVfU7efLJJ6uOOWjQIPbs2cOePXvo27cv119/PYMGDWLfvn3cdtttjBw5koEDB/Lwww9X7VPbnPmNmaO/Ueqav9hdjybPR19aJPJwB3n6TzdVzYWtVHvk7vnoV69eLRMnTqx63b9/f0lJSZG8vDwREcnKypKePXtW/Z8FBwfXeazFixeLv7+/7Nq1S8rLy2XSpEnywQcfiIhIQkKCZGVlSXJysnh7e8uaNWtERGTq1KnyxhtviIhIdnZ21bEeeughefbZZ0VE5IYbbpDzzz9fysvLRURk1qxZ8tRTT4mIyFdffVU1X31t6pov//7776+aH79yu8zMTImLi5Pdu3eLyNF56h9++GF54oknqrYdOHCgJCcnS3JyshhjZOnSpVXrKvcpLy+X0047TdatW1fnnPmuztF/8s5HX5xHum83DgfFHzedqlLKdcOGDSMzM5P09HTWrVtHREQEnTt35g9/+ANDhgxh0qRJpKWlVZWEGzJ69Gh69OiBt7c3V1999XHz2gN0796doUOHAsfObb9x40ZOPfVUBg8ezFtvvXXM3PZTp06tmlr4xhtvZO7cuQDMnj273kFXdc2Xv2jRIm6//faq7SIiIvjll1+YOHEi3bt3B+x89w1JSEhg7NixVa/ff/99hg8fzrBhw9i0aRObN2+uc878xszR3xhtr2KrqUJjuKXDf4gM9nd3JEq1e1OnTmXevHlkZGRw5ZVX8tZbb5GVlcWqVavw9fUlMTHR5TnhG5rXHsDf/+j/rbe3d1XVzfTp0/nkk09ISkpizpw5Vbf+g2Pnto+PjycmJobvvvuO5cuX89Zbb9UZz5133sm9997LRRddxJIlS5g1a5ZL51FdfXPbV48rOTmZJ598khUrVhAREcH06dPr/b3VnKN/1apVjY6tNp5TogfSDhZp/bxSLeDKK6/k3XffZd68eUydOpW8vDw6deqEr68vixcvbtTcLsuXLyc5ORmHw8F7773n8rz2YOeSj42NpaysrN7kDXbK4mnTph1T0q9NXfPlT548+Zi2iYMHDzJ27Fi+//57kpOTATvfPdj2hdWrVwOwevXqqvU15efnExwcTFhYGAcOHOCLL74AqHPO/MrzcGWO/sbwmERfWFrOwcIy4rTHjVLNNnDgQAoKCujatSuxsbFce+21rFy5ksGDBzN37lz69evn8rFGjRrFHXfcQf/+/enevXvVrfhc8eijjzJmzBgmTJjQ4HtedNFFVQ2q9alrvvw//vGPHDx4kEGDBpGUlMTixYuJjo7mpZde4rLLLiMpKYkrr7wSgClTppCbm8vAgQP597//TZ8+fWp9r6SkJIYNG0a/fv245pprqm7NWNec+eD6HP2N4dJ89MaYc4FnAG/gFRE5rm+WMeYKYBYgwDoRuca5vAKo7B+WIiIX1fdeTZ2PPudwCbM+3czUEXFM7BPd6P2VaitOpvnoW9LKlSu55557+OGHH9wdSrO4Mkd/Y+ejb7CO3hjjDTwHTAZSgRXGmAUisrnaNr2BB4EJInLQGNOp2iGKRGRoQ+/TXB1D/PnX1cNa+22UUm3Q448/zvPPP99g9U5bN3fuXB566CH++c9/tmj/+wZL9MaYccAsETnH+fpBABH5e7Vt/gFsF5FXatn/sIjUPXSuhubeYUqp9q49lug3bNjAddddd8wyf39/li1b5qaI4LHHHuODDz44ZtnUqVN56KGH3BRRy2nxEj3QFdhX7XUqMKbGNn2cb/QTtnpnloh86VwXYIxZCZQDj4vIJzXfwBhzC3ALQLdu3VwISSnPJiLtqpvw4MGDjxno0xY89NBDHpHUa3Klur2mlro28AF6A6cDVwMvG2PCnesSnN8y1wBPG2N61txZRF4SkZEiMjI6WuvX1cktICCAnJycJv1DK88mIuTk5BAQENCo/Vwp0acB8dVexzmXVZcKLBORMiDZGLMdm/hXiEiaM8DdxpglwDBgV6OiVOokEhcXR2pqap1D+NXJLSAggLi4xk3F7kqiXwH0NsZ0xyb4q7Cl8+o+wZbkXzPGRGGrcnYbYyKAQhEpcS6fAPyjUREqdZLx9fWtGompVEtoMNGLSLkx5g7gK2z9+2wR2WSMeQQ7t8IC57qzjTGbgQrgdyKSY4wZD7xojHFgq4ker95bRymlVOtzqR/9iaS9bpRSqvHq63XjMSNjlVJK1a7NleiNMVmA6xNpHC8KyG6hcNzNk84FPOt8POlcQM+nLXP1XBJEpNZui20u0TeXMWZlXZcv7Y0nnQt41vl40rmAnk9b1hLnolU3Sinl4TTRK6WUh/PERP+SuwNoQZ50LuBZ5+NJ5wJ6Pm1Zs8/F4+rolVJKHcsTS/RKKaWq0USvlFIezmMSvTHmXGPMNmPMTmPMA+6Op7GMMbONMZnGmI3VlkUaY74xxuxwPrfMDSRbmTEm3hiz2Biz2RizyRhzt3N5ez2fAGPMcmPMOuf5/MW5vLsxZpnzM/eeMcbP3bG6yhjjbYxZY4z5zPm6PZ/LHmPMBmPMWueU6O32swZgjAk3xswzxmw1xmwxxoxr7vl4RKKvdhesXwEDgKuNMQPcG1WjzQHOrbHszdq9TAAAAu9JREFUAeBbEekNfOt83R6UA/eJyABgLHC78+/RXs+nBDhTRJKAocC5xpixwP8CT4lIL+AgcJMbY2ysu4Et1V6353MBOENEhlbrb95eP2tgb9v6pYj0A5Kwf6fmnY+ItPsHMA74qtrrB4EH3R1XE84jEdhY7fU2INb5cyywzd0xNvG85mNvRdnuzwcIAlZjb76TDfg4lx/zGWzLD+xU498CZwKfAaa9nosz3j1AVI1l7fKzBoQByTg7yrTU+XhEiZ7a74LV1U2xtKQYEdnv/DkDiHFnME1hjEnE3oNgGe34fJxVHWuBTOAb7D0VDolIuXOT9vSZexq4H3A4X3ek/Z4LgABfG2NWOe9WB+33s9YdyMJO+b7GGPOKMSaYZp6PpyR6jyf2q7xd9YU1xoQAHwL/IyL51de1t/MRkQqxN7mPA0YD/dwcUpMYYy4AMkVklbtjaUGniMhwbNXt7caYidVXtrPPmg8wHHheRIYBR6hRTdOU8/GURO/KXbDaowPGmFgA53Omm+NxmTHGF5vk3xKRj5yL2+35VBKRQ8BibPVGuDGm8p4O7eUzNwG4yBizB3gXW33zDO3zXACQo3exywQ+xn4Rt9fPWiqQKiKVd1Wfh038zTofT0n0VXfBcvYWuApY4OaYWsIC4Abnzzdg67rbPGPvav0qsEVE/lltVXs9n+jKeyAbYwKx7Q1bsAn/cudm7eJ8RORBEYkTkUTs/8l3InIt7fBcAIwxwcaY0MqfgbOBjbTTz5qIZAD7jDF9nYvOAjbT3PNxd+NDCzZinAdsx9adPuTueJoQ/zvAfqAM+61+E7bu9FtgB7AIiHR3nC6eyynYS8v1wFrn47x2fD5DgDXO89kI/Nm5vAewHNgJfAD4uzvWRp7X6cBn7flcnHGvcz42Vf7vt9fPmjP2ocBK5+ftEyCiueejUyAopZSH85SqG6WUUnXQRK+UUh5OE71SSnk4TfRKKeXhNNErpZSH00SvlFIeThO9Ukp5uP8HJYIddjhulLUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1HNIr9wj5xY",
        "outputId": "78e74e83-2f0c-49ea-cda6-f29de5037dfe"
      },
      "source": [
        "history_df1.binary_accuracy.max() - history_df1.val_binary_accuracy.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.01956343650817871"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxyK2GDAoQPC"
      },
      "source": [
        "## making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imHHWk12-FYT",
        "outputId": "84c8eb19-a8fa-4bcd-f58e-fee4e2f70a27"
      },
      "source": [
        "preds1 = nn_mod_1.predict(val_X) # this is tf.keras function & could be called on the entire dataset but it's not fair to ask the model to predict on data already seen\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8906534 ],\n",
              "       [0.03351459],\n",
              "       [0.4092056 ],\n",
              "       ...,\n",
              "       [0.24772266],\n",
              "       [0.07768053],\n",
              "       [0.8906534 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaqa0E8Q-FTq",
        "outputId": "24c17d86-2dbe-4ab2-a8b3-85c61db06ad9"
      },
      "source": [
        "# amount of predictions below the threshold of 0.5 - indicating a prediction of not phishing\n",
        "\n",
        "len(preds1[preds1 <= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13920"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zg-XeIt--FPm",
        "outputId": "54f31d62-20b1-471c-8fe9-6ce6b2d327a1"
      },
      "source": [
        "# amount of predictions above the threshold of 0.5 - indicating a prediction of phishing\n",
        "\n",
        "len(preds1[preds1 > 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8242"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2NKMevmOklU",
        "outputId": "2ca81119-5918-45ec-d470-04b1ece27956"
      },
      "source": [
        "# verify length of val_y - double check all preds within (0,1) limit\n",
        "\n",
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "L4vca5K1lwb3",
        "outputId": "3726f309-cf21-4aa3-b2bd-2e56bc0b4adc"
      },
      "source": [
        "# turn predictions into pandas dataframe\n",
        "\n",
        "preds_df = pd.DataFrame(preds1, columns = ['preds'])\n",
        "\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.890653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.033515</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.409206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.109818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.680983</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      preds\n",
              "0  0.890653\n",
              "1  0.033515\n",
              "2  0.409206\n",
              "3  0.109818\n",
              "4  0.680983"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "djCSxiA2lwZW",
        "outputId": "63ba2bdc-6601-4fad-a6cc-153cac1dc519"
      },
      "source": [
        "# attach predictions to original validation data (val_X & val_y) for easier locating, plotting, etc.\n",
        "\n",
        "preds_df = pd.concat([preds_df, val_y.reset_index(drop=True), val_X.reset_index()], axis=1)\n",
        "\n",
        "preds_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>phishing</th>\n",
              "      <th>index</th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_underline_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>qty_questionmark_url</th>\n",
              "      <th>qty_equal_url</th>\n",
              "      <th>qty_at_url</th>\n",
              "      <th>qty_and_url</th>\n",
              "      <th>qty_exclamation_url</th>\n",
              "      <th>qty_space_url</th>\n",
              "      <th>qty_tilde_url</th>\n",
              "      <th>qty_comma_url</th>\n",
              "      <th>qty_plus_url</th>\n",
              "      <th>qty_asterisk_url</th>\n",
              "      <th>qty_hashtag_url</th>\n",
              "      <th>qty_dollar_url</th>\n",
              "      <th>qty_percent_url</th>\n",
              "      <th>qty_tld_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_hyphen_domain</th>\n",
              "      <th>qty_underline_domain</th>\n",
              "      <th>qty_slash_domain</th>\n",
              "      <th>qty_questionmark_domain</th>\n",
              "      <th>qty_equal_domain</th>\n",
              "      <th>qty_at_domain</th>\n",
              "      <th>qty_and_domain</th>\n",
              "      <th>qty_exclamation_domain</th>\n",
              "      <th>qty_space_domain</th>\n",
              "      <th>qty_tilde_domain</th>\n",
              "      <th>qty_comma_domain</th>\n",
              "      <th>qty_plus_domain</th>\n",
              "      <th>qty_asterisk_domain</th>\n",
              "      <th>qty_hashtag_domain</th>\n",
              "      <th>qty_dollar_domain</th>\n",
              "      <th>qty_percent_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>...</th>\n",
              "      <th>qty_asterisk_file</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>qty_dot_params</th>\n",
              "      <th>qty_hyphen_params</th>\n",
              "      <th>qty_underline_params</th>\n",
              "      <th>qty_slash_params</th>\n",
              "      <th>qty_questionmark_params</th>\n",
              "      <th>qty_equal_params</th>\n",
              "      <th>qty_at_params</th>\n",
              "      <th>qty_and_params</th>\n",
              "      <th>qty_exclamation_params</th>\n",
              "      <th>qty_space_params</th>\n",
              "      <th>qty_tilde_params</th>\n",
              "      <th>qty_comma_params</th>\n",
              "      <th>qty_plus_params</th>\n",
              "      <th>qty_asterisk_params</th>\n",
              "      <th>qty_hashtag_params</th>\n",
              "      <th>qty_dollar_params</th>\n",
              "      <th>qty_percent_params</th>\n",
              "      <th>params_length</th>\n",
              "      <th>tld_present_params</th>\n",
              "      <th>qty_params</th>\n",
              "      <th>email_in_url</th>\n",
              "      <th>time_response</th>\n",
              "      <th>domain_spf</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "      <th>url_google_index</th>\n",
              "      <th>domain_google_index</th>\n",
              "      <th>url_shortened</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.890653</td>\n",
              "      <td>1</td>\n",
              "      <td>62575</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.003517</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.138820</td>\n",
              "      <td>0.037377</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.989602</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.033515</td>\n",
              "      <td>0</td>\n",
              "      <td>38126</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.786588</td>\n",
              "      <td>0.260471</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.559770</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.409206</td>\n",
              "      <td>0</td>\n",
              "      <td>1617</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.921059</td>\n",
              "      <td>0.388661</td>\n",
              "      <td>0.014919</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.018994</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.109818</td>\n",
              "      <td>0</td>\n",
              "      <td>8228</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.948421</td>\n",
              "      <td>0.119534</td>\n",
              "      <td>0.021672</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.292813</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.680983</td>\n",
              "      <td>1</td>\n",
              "      <td>55594</td>\n",
              "      <td>0.054903</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027451</td>\n",
              "      <td>0.054903</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.027451</td>\n",
              "      <td>0.768639</td>\n",
              "      <td>0.027451</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.109806</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.274514</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109806</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22157</th>\n",
              "      <td>0.003822</td>\n",
              "      <td>0</td>\n",
              "      <td>65294</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.001480</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.684142</td>\n",
              "      <td>0.658983</td>\n",
              "      <td>0.008618</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.312430</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22158</th>\n",
              "      <td>0.027320</td>\n",
              "      <td>0</td>\n",
              "      <td>10038</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.891957</td>\n",
              "      <td>0.419925</td>\n",
              "      <td>0.166351</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22159</th>\n",
              "      <td>0.247723</td>\n",
              "      <td>0</td>\n",
              "      <td>43642</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.001710</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999761</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22160</th>\n",
              "      <td>0.077681</td>\n",
              "      <td>0</td>\n",
              "      <td>73632</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.838648</td>\n",
              "      <td>0.166678</td>\n",
              "      <td>0.008666</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.518471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22161</th>\n",
              "      <td>0.890653</td>\n",
              "      <td>1</td>\n",
              "      <td>25895</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.954190</td>\n",
              "      <td>0.051122</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.294798</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22162 rows × 114 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          preds  phishing  ...  domain_google_index  url_shortened\n",
              "0      0.890653         1  ...                  0.0            0.0\n",
              "1      0.033515         0  ...                  0.0            0.0\n",
              "2      0.409206         0  ...                  0.0            0.0\n",
              "3      0.109818         0  ...                  0.0            0.0\n",
              "4      0.680983         1  ...                  0.0            0.0\n",
              "...         ...       ...  ...                  ...            ...\n",
              "22157  0.003822         0  ...                  0.0            0.0\n",
              "22158  0.027320         0  ...                  0.0            0.0\n",
              "22159  0.247723         0  ...                  0.0            0.0\n",
              "22160  0.077681         0  ...                  0.0            0.0\n",
              "22161  0.890653         1  ...                  0.0            0.0\n",
              "\n",
              "[22162 rows x 114 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqcjGQPD4kL7"
      },
      "source": [
        "# this is a fun visualization of the model\n",
        "\n",
        "#img_file = '/tmp/model_1.png'\n",
        "#tf.keras.utils.plot_model(nn_mod_1, to_file=img_file, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhxinDvloQPE"
      },
      "source": [
        "# To-Do:\n",
        "\n",
        "    - Work on getting SHAP to work with the format of the model\n",
        "        - Tried to get this to work and the sklearn/keras Classifier wrapper isn't supported\n",
        "    - Feature selection with sklearn wrapper ?\n",
        "    - Predictions using the NN as a callable function compared to the build style above ---- Done\n",
        "    - Clean up notebook w/ better comments lol\n",
        "    \n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi_xm1LgUlOg"
      },
      "source": [
        "---\n",
        "# NN in a more deployable format\n",
        "\n",
        "### (again, this takes a long time to run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJtoHaTV8Qcb"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max', #this needs to be changed to 'val_acc' if running in a Jupyter notebook (for some reason colab is different)\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "\n",
        "# building the model as a callable function for easier use with the sklearn wrapper\n",
        "\n",
        "def phish_nn():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[111]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) #had to slightly alter these parameters per the limitations on metric measuring in the sklearn wrapper\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8) #lowered epochs b/c w/cv there will be more epochs than in the original model fitting\n",
        "  model.predict(val_X) #give the model the ability to predict as above\n",
        "  model.evaluate(val_X, val_y) #give the model the ability to evaluate like was done above\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEf-TNa9UueX"
      },
      "source": [
        "---\n",
        "\n",
        "# Model as Keras Classifier with Cross-Validation\n",
        "\n",
        "## This is still a Keras Sequential model but it is also using a sklearn wrapper as a classifier model\n",
        "\n",
        "# The NN as a function with the cross validation scoring and predictions takes a while to run  -- top 10 is the quickest\n",
        "\n",
        "---\n",
        "\n",
        "### Each model grouping has the same code as the others, just with the changes below\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BCV3qbboQPG"
      },
      "source": [
        "### the parameters that change across the models:\n",
        "\n",
        "    - the input shape (because the inputs are different)\n",
        "    - the early stopping callback is monitoring 'val_accuracy'\n",
        "        - 'val_accuracy' in GoogleColab and 'val_acc' in Jupyter\n",
        "    - the metric measured is now only accuracy b/c of issues with sklearn wrapper compatibility\n",
        "    - decrease in epochs because of the way the cross validation is running ~ trying not avoid 12 hours per CV\n",
        "    - increase in batch size for faster/easier computing\n",
        "    - decrease in patience of early stopping callback\n",
        "\n",
        "### cross-validation scores and predictions\n",
        "\n",
        "    - 5 fold cross-validation on cv_scoring & cv_predictions\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh4qppW9FZRj"
      },
      "source": [
        "mod = KerasClassifier(build_fn=phish_nn,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LONFSzATMKWZ"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jyuAHDIMTJh",
        "scrolled": true,
        "outputId": "a9e35e5e-3a1c-4462-8c9f-f2d53828fd38"
      },
      "source": [
        "cv_results=cross_val_score(mod,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6380 - accuracy: 0.6405 - val_loss: 0.5383 - val_accuracy: 0.7182\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5580 - accuracy: 0.7070 - val_loss: 0.5393 - val_accuracy: 0.7279\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5438 - accuracy: 0.7180 - val_loss: 0.5230 - val_accuracy: 0.7311\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5308 - accuracy: 0.7284 - val_loss: 0.4928 - val_accuracy: 0.7853\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4862 - accuracy: 0.7548 - val_loss: 0.4303 - val_accuracy: 0.7999\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4259 - accuracy: 0.7988 - val_loss: 0.3972 - val_accuracy: 0.8398\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3740 - accuracy: 0.8279 - val_loss: 0.3265 - val_accuracy: 0.8774\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8546 - val_loss: 0.2978 - val_accuracy: 0.8941\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8656 - val_loss: 0.3799 - val_accuracy: 0.8451\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3137 - accuracy: 0.8704 - val_loss: 0.3022 - val_accuracy: 0.8815\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3106 - accuracy: 0.8731 - val_loss: 0.2910 - val_accuracy: 0.8723\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2957 - accuracy: 0.8777 - val_loss: 0.3183 - val_accuracy: 0.9064\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2965 - accuracy: 0.8775 - val_loss: 0.2961 - val_accuracy: 0.8735\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2821 - accuracy: 0.8875 - val_loss: 0.3229 - val_accuracy: 0.8751\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.8831 - val_loss: 0.3000 - val_accuracy: 0.9140\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2696 - accuracy: 0.8907 - val_loss: 0.2418 - val_accuracy: 0.9042\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8897 - val_loss: 0.2926 - val_accuracy: 0.9054\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2676 - accuracy: 0.8891 - val_loss: 0.2576 - val_accuracy: 0.8904\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2697 - accuracy: 0.8905 - val_loss: 0.2780 - val_accuracy: 0.8990\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8936 - val_loss: 0.3033 - val_accuracy: 0.9072\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2733 - accuracy: 0.8868 - val_loss: 0.2897 - val_accuracy: 0.9150\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8979 - val_loss: 0.2566 - val_accuracy: 0.9062\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.8921 - val_loss: 0.2903 - val_accuracy: 0.8924\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8917 - val_loss: 0.2825 - val_accuracy: 0.8994\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8956 - val_loss: 0.2934 - val_accuracy: 0.9133\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2577 - accuracy: 0.8946 - val_loss: 0.2689 - val_accuracy: 0.9108\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2560 - accuracy: 0.8957 - val_loss: 0.2648 - val_accuracy: 0.9170\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2433 - accuracy: 0.9014 - val_loss: 0.2833 - val_accuracy: 0.8878\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8952 - val_loss: 0.2357 - val_accuracy: 0.9215\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2467 - accuracy: 0.9011 - val_loss: 0.2877 - val_accuracy: 0.9077\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2505 - accuracy: 0.8994 - val_loss: 0.2565 - val_accuracy: 0.9097\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2459 - accuracy: 0.9016 - val_loss: 0.2463 - val_accuracy: 0.9205\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2433 - accuracy: 0.9019 - val_loss: 0.2752 - val_accuracy: 0.9121\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2479 - accuracy: 0.8992 - val_loss: 0.2674 - val_accuracy: 0.9219\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2416 - accuracy: 0.9024 - val_loss: 0.2402 - val_accuracy: 0.9162\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2407 - accuracy: 0.9028 - val_loss: 0.2685 - val_accuracy: 0.9199\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2438 - accuracy: 0.9015 - val_loss: 0.2343 - val_accuracy: 0.9208\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2380 - accuracy: 0.9067 - val_loss: 0.2331 - val_accuracy: 0.9202\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2414 - accuracy: 0.9019 - val_loss: 0.2525 - val_accuracy: 0.9149\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2407 - accuracy: 0.9042 - val_loss: 0.3032 - val_accuracy: 0.8884\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2393 - accuracy: 0.9048 - val_loss: 0.2136 - val_accuracy: 0.9187\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2434 - accuracy: 0.9014 - val_loss: 0.2359 - val_accuracy: 0.9123\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2478 - accuracy: 0.8984 - val_loss: 0.2396 - val_accuracy: 0.9083\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2370 - accuracy: 0.9045 - val_loss: 0.2202 - val_accuracy: 0.9234\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2424 - accuracy: 0.9048 - val_loss: 0.2623 - val_accuracy: 0.9133\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2475 - accuracy: 0.9012 - val_loss: 0.2691 - val_accuracy: 0.9146\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2346 - accuracy: 0.9062 - val_loss: 0.2156 - val_accuracy: 0.9165\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2341 - accuracy: 0.9060 - val_loss: 0.2510 - val_accuracy: 0.9187\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2312 - accuracy: 0.9074 - val_loss: 0.2901 - val_accuracy: 0.9193\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2340 - accuracy: 0.9065 - val_loss: 0.2143 - val_accuracy: 0.9138\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2167 - accuracy: 0.9113\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2388 - accuracy: 0.9035\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2377 - accuracy: 0.9045\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2352 - accuracy: 0.9072\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2442 - accuracy: 0.9032\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2407 - accuracy: 0.9027\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2410 - accuracy: 0.9042\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2328 - accuracy: 0.9067\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2300 - accuracy: 0.9092\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2258 - accuracy: 0.9094\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2289 - accuracy: 0.9089\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2556 - accuracy: 0.8995\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6236 - accuracy: 0.6537 - val_loss: 0.5723 - val_accuracy: 0.7236\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5567 - accuracy: 0.7133 - val_loss: 0.5914 - val_accuracy: 0.7180\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5434 - accuracy: 0.7184 - val_loss: 0.5785 - val_accuracy: 0.7224\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5294 - accuracy: 0.7259 - val_loss: 0.5221 - val_accuracy: 0.7572\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4860 - accuracy: 0.7582 - val_loss: 0.4776 - val_accuracy: 0.7946\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4342 - accuracy: 0.7994 - val_loss: 0.4381 - val_accuracy: 0.8309\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3901 - accuracy: 0.8349 - val_loss: 0.3700 - val_accuracy: 0.8691\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3648 - accuracy: 0.8562 - val_loss: 0.4581 - val_accuracy: 0.8467\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3404 - accuracy: 0.8688 - val_loss: 0.3497 - val_accuracy: 0.8920\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3204 - accuracy: 0.8752 - val_loss: 0.3057 - val_accuracy: 0.9104\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3104 - accuracy: 0.8807 - val_loss: 0.3061 - val_accuracy: 0.8931\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3021 - accuracy: 0.8826 - val_loss: 0.3848 - val_accuracy: 0.9102\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2936 - accuracy: 0.8879 - val_loss: 0.2786 - val_accuracy: 0.9116\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8912 - val_loss: 0.3973 - val_accuracy: 0.8298\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3104 - accuracy: 0.8766 - val_loss: 0.2852 - val_accuracy: 0.9173\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2893 - accuracy: 0.8866 - val_loss: 0.3051 - val_accuracy: 0.9092\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2759 - accuracy: 0.8925 - val_loss: 0.4036 - val_accuracy: 0.8825\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2711 - accuracy: 0.8955 - val_loss: 0.6159 - val_accuracy: 0.3495\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2749 - accuracy: 0.8923 - val_loss: 0.2771 - val_accuracy: 0.9156\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2697 - accuracy: 0.8957 - val_loss: 0.3268 - val_accuracy: 0.9175\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2632 - accuracy: 0.8977 - val_loss: 0.2711 - val_accuracy: 0.9084\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2648 - accuracy: 0.8951 - val_loss: 0.3609 - val_accuracy: 0.8521\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2617 - accuracy: 0.8979 - val_loss: 0.3351 - val_accuracy: 0.9031\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2719 - accuracy: 0.8929 - val_loss: 0.2768 - val_accuracy: 0.9105\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.9009 - val_loss: 0.2940 - val_accuracy: 0.9111\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2600 - accuracy: 0.8963 - val_loss: 0.3871 - val_accuracy: 0.8959\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.8968 - val_loss: 0.3032 - val_accuracy: 0.9017\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.9006 - val_loss: 0.3013 - val_accuracy: 0.9131\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2573 - accuracy: 0.9005 - val_loss: 0.2327 - val_accuracy: 0.9174\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8976 - val_loss: 0.2553 - val_accuracy: 0.9116\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8993 - val_loss: 0.2381 - val_accuracy: 0.9068\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2503 - accuracy: 0.9021 - val_loss: 0.2858 - val_accuracy: 0.9169\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8996 - val_loss: 0.3994 - val_accuracy: 0.8967\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2493 - accuracy: 0.9030 - val_loss: 0.2570 - val_accuracy: 0.9143\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2564 - accuracy: 0.8998 - val_loss: 0.2688 - val_accuracy: 0.9137\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3285 - accuracy: 0.9163\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2732 - accuracy: 0.8945\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2605 - accuracy: 0.8989\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2608 - accuracy: 0.8992\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2610 - accuracy: 0.8987\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2609 - accuracy: 0.8981\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2637 - accuracy: 0.8982\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2560 - accuracy: 0.9015\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2557 - accuracy: 0.9019\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2486 - accuracy: 0.9045\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2489 - accuracy: 0.9031\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2570 - accuracy: 0.9101\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6251 - accuracy: 0.6505 - val_loss: 0.5845 - val_accuracy: 0.7193\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5590 - accuracy: 0.7069 - val_loss: 0.5655 - val_accuracy: 0.7218\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5477 - accuracy: 0.7163 - val_loss: 0.5590 - val_accuracy: 0.7281\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5378 - accuracy: 0.7216 - val_loss: 0.5585 - val_accuracy: 0.7241\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5138 - accuracy: 0.7336 - val_loss: 0.4909 - val_accuracy: 0.7684\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4626 - accuracy: 0.7724 - val_loss: 0.4620 - val_accuracy: 0.8054\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4282 - accuracy: 0.8038 - val_loss: 0.4330 - val_accuracy: 0.8065\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3906 - accuracy: 0.8286 - val_loss: 0.3794 - val_accuracy: 0.8711\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3546 - accuracy: 0.8567 - val_loss: 0.3863 - val_accuracy: 0.8674\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3389 - accuracy: 0.8645 - val_loss: 0.3645 - val_accuracy: 0.8812\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3300 - accuracy: 0.8695 - val_loss: 0.3462 - val_accuracy: 0.8953\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3190 - accuracy: 0.8760 - val_loss: 0.3113 - val_accuracy: 0.9070\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3052 - accuracy: 0.8773 - val_loss: 0.3436 - val_accuracy: 0.8993\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3080 - accuracy: 0.8757 - val_loss: 0.4198 - val_accuracy: 0.8586\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.8882 - val_loss: 0.2955 - val_accuracy: 0.9014\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.8869 - val_loss: 0.2780 - val_accuracy: 0.8956\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2821 - accuracy: 0.8890 - val_loss: 0.3081 - val_accuracy: 0.9072\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2842 - accuracy: 0.8886 - val_loss: 0.3275 - val_accuracy: 0.8943\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2835 - accuracy: 0.8878 - val_loss: 0.2802 - val_accuracy: 0.9147\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8968 - val_loss: 0.3463 - val_accuracy: 0.9110\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2693 - accuracy: 0.8925 - val_loss: 0.3019 - val_accuracy: 0.9096\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8931 - val_loss: 0.3089 - val_accuracy: 0.9194\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.8989 - val_loss: 0.3328 - val_accuracy: 0.9056\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8963 - val_loss: 0.2890 - val_accuracy: 0.9177\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8972 - val_loss: 0.2993 - val_accuracy: 0.9207\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.9021 - val_loss: 0.2397 - val_accuracy: 0.9230\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.8957 - val_loss: 0.2977 - val_accuracy: 0.9167\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8945 - val_loss: 0.2833 - val_accuracy: 0.9179\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8990 - val_loss: 0.3510 - val_accuracy: 0.9083\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2590 - accuracy: 0.8987 - val_loss: 0.2474 - val_accuracy: 0.9147\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.9008 - val_loss: 0.2832 - val_accuracy: 0.9047\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2508 - accuracy: 0.9018 - val_loss: 0.2951 - val_accuracy: 0.9135\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2543 - accuracy: 0.9003 - val_loss: 0.2793 - val_accuracy: 0.9190\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2552 - accuracy: 0.9001 - val_loss: 0.3511 - val_accuracy: 0.9114\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2528 - accuracy: 0.9021 - val_loss: 0.2212 - val_accuracy: 0.9194\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.9007 - val_loss: 0.2587 - val_accuracy: 0.9129\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2492 - accuracy: 0.9055 - val_loss: 0.2612 - val_accuracy: 0.9156\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2491 - accuracy: 0.9013 - val_loss: 0.2627 - val_accuracy: 0.8969\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2520 - accuracy: 0.9001 - val_loss: 0.2238 - val_accuracy: 0.9187\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2375 - accuracy: 0.9077 - val_loss: 0.3182 - val_accuracy: 0.9210\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2417 - accuracy: 0.9049 - val_loss: 0.2366 - val_accuracy: 0.9191\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2434 - accuracy: 0.9201\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2645 - accuracy: 0.8964\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2552 - accuracy: 0.8996\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2571 - accuracy: 0.8985\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2520 - accuracy: 0.9012\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2470 - accuracy: 0.9040\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2527 - accuracy: 0.9007\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2455 - accuracy: 0.9038\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2498 - accuracy: 0.9022\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2451 - accuracy: 0.9042\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2418 - accuracy: 0.9065\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2442 - accuracy: 0.9157\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6324 - accuracy: 0.6388 - val_loss: 0.5458 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5551 - accuracy: 0.6992 - val_loss: 0.5529 - val_accuracy: 0.7250\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5441 - accuracy: 0.7180 - val_loss: 0.5337 - val_accuracy: 0.7284\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5289 - accuracy: 0.7251 - val_loss: 0.5095 - val_accuracy: 0.7506\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4950 - accuracy: 0.7444 - val_loss: 0.4903 - val_accuracy: 0.7601\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4568 - accuracy: 0.7820 - val_loss: 0.4190 - val_accuracy: 0.8059\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4166 - accuracy: 0.8098 - val_loss: 0.3734 - val_accuracy: 0.8425\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3807 - accuracy: 0.8380 - val_loss: 0.3936 - val_accuracy: 0.8691\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3736 - accuracy: 0.8416 - val_loss: 0.3323 - val_accuracy: 0.8863\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3444 - accuracy: 0.8573 - val_loss: 0.3713 - val_accuracy: 0.8787\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3333 - accuracy: 0.8677 - val_loss: 0.3182 - val_accuracy: 0.8866\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3293 - accuracy: 0.8703 - val_loss: 0.3285 - val_accuracy: 0.8850\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3179 - accuracy: 0.8699 - val_loss: 0.3032 - val_accuracy: 0.8975\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3134 - accuracy: 0.8721 - val_loss: 0.2932 - val_accuracy: 0.9016\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3012 - accuracy: 0.8799 - val_loss: 0.2879 - val_accuracy: 0.8999\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2896 - accuracy: 0.8848 - val_loss: 0.2848 - val_accuracy: 0.8828\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3007 - accuracy: 0.8800 - val_loss: 0.3228 - val_accuracy: 0.8869\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8861 - val_loss: 0.2876 - val_accuracy: 0.9073\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2905 - accuracy: 0.8812 - val_loss: 0.2613 - val_accuracy: 0.9123\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2730 - accuracy: 0.8911 - val_loss: 0.2741 - val_accuracy: 0.9082\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2858 - accuracy: 0.8856 - val_loss: 0.2439 - val_accuracy: 0.9153\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2738 - accuracy: 0.8899 - val_loss: 0.2844 - val_accuracy: 0.9039\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8876 - val_loss: 0.2523 - val_accuracy: 0.9044\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8913 - val_loss: 0.3908 - val_accuracy: 0.9056\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.8964 - val_loss: 0.2650 - val_accuracy: 0.9143\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8957 - val_loss: 0.2796 - val_accuracy: 0.9068\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2634 - accuracy: 0.8963 - val_loss: 0.2456 - val_accuracy: 0.9159\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8952 - val_loss: 0.2422 - val_accuracy: 0.9150\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2660 - accuracy: 0.8931 - val_loss: 0.2465 - val_accuracy: 0.9179\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2534 - accuracy: 0.9002 - val_loss: 0.2464 - val_accuracy: 0.9214\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8953 - val_loss: 0.2519 - val_accuracy: 0.9163\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.9005 - val_loss: 0.2512 - val_accuracy: 0.9103\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.9016 - val_loss: 0.3118 - val_accuracy: 0.9000\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2602 - accuracy: 0.8969 - val_loss: 0.3062 - val_accuracy: 0.9038\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2524 - accuracy: 0.9002 - val_loss: 0.2445 - val_accuracy: 0.9178\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.9008 - val_loss: 0.2491 - val_accuracy: 0.9158\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2450 - accuracy: 0.9021 - val_loss: 0.2987 - val_accuracy: 0.8950\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2590 - accuracy: 0.8965 - val_loss: 0.2284 - val_accuracy: 0.9161\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.9008 - val_loss: 0.2369 - val_accuracy: 0.9152\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2472 - accuracy: 0.9046 - val_loss: 0.2590 - val_accuracy: 0.9097\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2505 - accuracy: 0.9002 - val_loss: 0.2633 - val_accuracy: 0.9072\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2521 - accuracy: 0.8979 - val_loss: 0.2385 - val_accuracy: 0.9178\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2445 - accuracy: 0.9037 - val_loss: 0.3097 - val_accuracy: 0.9076\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2495 - accuracy: 0.9007 - val_loss: 0.2754 - val_accuracy: 0.9059\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2419 - accuracy: 0.9050 - val_loss: 0.2428 - val_accuracy: 0.9225\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2467 - accuracy: 0.9034 - val_loss: 0.2471 - val_accuracy: 0.9202\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2399 - accuracy: 0.9054 - val_loss: 0.2983 - val_accuracy: 0.8818\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2545 - accuracy: 0.8989 - val_loss: 0.2323 - val_accuracy: 0.9182\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2390 - accuracy: 0.9063 - val_loss: 0.2551 - val_accuracy: 0.9198\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2384 - accuracy: 0.9059 - val_loss: 0.2622 - val_accuracy: 0.9088\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2654 - accuracy: 0.9061\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2391 - accuracy: 0.9055\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2442 - accuracy: 0.9035\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2398 - accuracy: 0.9056\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2467 - accuracy: 0.9024\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2415 - accuracy: 0.9046\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2374 - accuracy: 0.9058\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2394 - accuracy: 0.9046\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2381 - accuracy: 0.9062\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2380 - accuracy: 0.9061\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2336 - accuracy: 0.9081\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.3179 - accuracy: 0.9020\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6281 - accuracy: 0.6478 - val_loss: 0.5656 - val_accuracy: 0.7209\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5583 - accuracy: 0.7083 - val_loss: 0.5604 - val_accuracy: 0.7291\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5393 - accuracy: 0.7231 - val_loss: 0.5490 - val_accuracy: 0.7382\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5141 - accuracy: 0.7346 - val_loss: 0.4853 - val_accuracy: 0.7693\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4655 - accuracy: 0.7728 - val_loss: 0.4604 - val_accuracy: 0.8014\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4225 - accuracy: 0.8106 - val_loss: 0.4518 - val_accuracy: 0.8151\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3774 - accuracy: 0.8487 - val_loss: 0.3761 - val_accuracy: 0.8826\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3520 - accuracy: 0.8604 - val_loss: 0.3809 - val_accuracy: 0.8791\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3368 - accuracy: 0.8674 - val_loss: 0.3499 - val_accuracy: 0.8860\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3265 - accuracy: 0.8706 - val_loss: 0.3124 - val_accuracy: 0.8918\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3137 - accuracy: 0.8792 - val_loss: 0.2963 - val_accuracy: 0.8906\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2992 - accuracy: 0.8851 - val_loss: 0.5390 - val_accuracy: 0.7585\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3035 - accuracy: 0.8834 - val_loss: 0.3188 - val_accuracy: 0.9041\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2923 - accuracy: 0.8899 - val_loss: 0.3452 - val_accuracy: 0.8874\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8927 - val_loss: 0.3502 - val_accuracy: 0.8534\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2677 - accuracy: 0.8957 - val_loss: 0.2726 - val_accuracy: 0.9160\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2755 - accuracy: 0.8926 - val_loss: 0.2915 - val_accuracy: 0.9123\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2816 - accuracy: 0.8883 - val_loss: 0.3046 - val_accuracy: 0.9082\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8965 - val_loss: 0.2584 - val_accuracy: 0.9163\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2702 - accuracy: 0.8975 - val_loss: 0.2505 - val_accuracy: 0.8922\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2714 - accuracy: 0.8902 - val_loss: 0.2750 - val_accuracy: 0.9160\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8978 - val_loss: 0.2596 - val_accuracy: 0.9152\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8948 - val_loss: 0.2578 - val_accuracy: 0.9183\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2559 - accuracy: 0.9003 - val_loss: 0.2782 - val_accuracy: 0.9148\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2630 - accuracy: 0.8965 - val_loss: 0.2896 - val_accuracy: 0.9097\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2632 - accuracy: 0.8954 - val_loss: 0.3236 - val_accuracy: 0.9092\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2565 - accuracy: 0.8976 - val_loss: 0.2635 - val_accuracy: 0.9087\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2565 - accuracy: 0.8972 - val_loss: 0.4663 - val_accuracy: 0.8419\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8978 - val_loss: 0.2719 - val_accuracy: 0.8912\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8976 - val_loss: 0.2671 - val_accuracy: 0.9096\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2561 - accuracy: 0.8976 - val_loss: 0.2361 - val_accuracy: 0.9156\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2498 - accuracy: 0.9037 - val_loss: 0.2596 - val_accuracy: 0.9161\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2441 - accuracy: 0.9038 - val_loss: 0.2811 - val_accuracy: 0.9078\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2560 - accuracy: 0.9002 - val_loss: 0.3679 - val_accuracy: 0.8118\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2592 - accuracy: 0.8968 - val_loss: 0.2276 - val_accuracy: 0.9158\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2469 - accuracy: 0.9018 - val_loss: 0.2444 - val_accuracy: 0.9006\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8972 - val_loss: 0.2431 - val_accuracy: 0.9132\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.9026 - val_loss: 0.2305 - val_accuracy: 0.9160\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2602 - accuracy: 0.9152\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2580 - accuracy: 0.8997\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2641 - accuracy: 0.8960\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2560 - accuracy: 0.9003\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2550 - accuracy: 0.9008\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2542 - accuracy: 0.9017\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2501 - accuracy: 0.9031\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2513 - accuracy: 0.9019\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2435 - accuracy: 0.9055\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2483 - accuracy: 0.9032\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2400 - accuracy: 0.9073\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2490 - accuracy: 0.9152\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nph21h-MX-QH",
        "outputId": "d6712a67-00e5-4793-ac89-12d40cf18a16"
      },
      "source": [
        "print(round(cv_results.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L6liyFVX-OA",
        "outputId": "5afb27d7-f454-475e-c667-e86d2d4198df"
      },
      "source": [
        "print(round(cv_results.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0067\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNFGaP_0X-LI",
        "outputId": "bead75dd-a2e0-420f-fc0a-c56df244c6c0"
      },
      "source": [
        "cv_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.89954877, 0.91009587, 0.91567487, 0.90196854, 0.91522366])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwEJmJRIoQPH"
      },
      "source": [
        "#### The cross validation results are very good news. With the lowest cv_score being 89.96% accuracy and the maximum being 91.57% accuracy, this is a positive note that the model is training and predicting correctly, with the cv_scores having a *slightly* lower average accuracy (90.85%) compared to the initial build and run of the NN (92.05%). It will be interesting to see the cross validation predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixiNIoMRX-Ir",
        "scrolled": true,
        "outputId": "b7ae55ae-938b-4b2c-e479-6476c0c28e89"
      },
      "source": [
        "cv_preds = cross_val_predict(mod, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6294 - accuracy: 0.6447 - val_loss: 0.6065 - val_accuracy: 0.7193\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5612 - accuracy: 0.7074 - val_loss: 0.5678 - val_accuracy: 0.7263\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5425 - accuracy: 0.7217 - val_loss: 0.5877 - val_accuracy: 0.7199\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5358 - accuracy: 0.7255 - val_loss: 0.5362 - val_accuracy: 0.7426\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5082 - accuracy: 0.7381 - val_loss: 0.4934 - val_accuracy: 0.7703\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4569 - accuracy: 0.7778 - val_loss: 0.4523 - val_accuracy: 0.7942\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4143 - accuracy: 0.8111 - val_loss: 0.3967 - val_accuracy: 0.8520\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3760 - accuracy: 0.8437 - val_loss: 0.4230 - val_accuracy: 0.8820\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3663 - accuracy: 0.8522 - val_loss: 0.3404 - val_accuracy: 0.8882\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3310 - accuracy: 0.8699 - val_loss: 0.3399 - val_accuracy: 0.9007\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3172 - accuracy: 0.8753 - val_loss: 0.3317 - val_accuracy: 0.8753\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8601 - val_loss: 0.3132 - val_accuracy: 0.8960\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3045 - accuracy: 0.8776 - val_loss: 0.3240 - val_accuracy: 0.9043\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8767 - val_loss: 0.3362 - val_accuracy: 0.8856\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8799 - val_loss: 0.2976 - val_accuracy: 0.8829\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2935 - accuracy: 0.8846 - val_loss: 0.3196 - val_accuracy: 0.9139\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2864 - accuracy: 0.8846 - val_loss: 0.3359 - val_accuracy: 0.9105\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8847 - val_loss: 0.2541 - val_accuracy: 0.9048\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2865 - accuracy: 0.8840 - val_loss: 0.2864 - val_accuracy: 0.9163\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2795 - accuracy: 0.8900 - val_loss: 0.2905 - val_accuracy: 0.9180\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8954 - val_loss: 0.2361 - val_accuracy: 0.9153\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2783 - accuracy: 0.8862 - val_loss: 0.2868 - val_accuracy: 0.8862\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2629 - accuracy: 0.8969 - val_loss: 0.3187 - val_accuracy: 0.9033\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8941 - val_loss: 0.2909 - val_accuracy: 0.8984\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2743 - accuracy: 0.8919 - val_loss: 0.2837 - val_accuracy: 0.9145\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8941 - val_loss: 0.2811 - val_accuracy: 0.9155\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8999 - val_loss: 0.2577 - val_accuracy: 0.9141\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8989 - val_loss: 0.2689 - val_accuracy: 0.9138\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8981 - val_loss: 0.3439 - val_accuracy: 0.9078\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8960 - val_loss: 0.2854 - val_accuracy: 0.8943\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2590 - accuracy: 0.8981 - val_loss: 0.2276 - val_accuracy: 0.9183\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2524 - accuracy: 0.9011 - val_loss: 0.4414 - val_accuracy: 0.8819\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8991 - val_loss: 0.2440 - val_accuracy: 0.9202\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2502 - accuracy: 0.9035 - val_loss: 0.2882 - val_accuracy: 0.9123\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.9000 - val_loss: 0.2692 - val_accuracy: 0.9170\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.9027 - val_loss: 0.2342 - val_accuracy: 0.9119\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2475 - accuracy: 0.9018 - val_loss: 0.3468 - val_accuracy: 0.9111\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2480 - accuracy: 0.9029 - val_loss: 0.2874 - val_accuracy: 0.9140\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2468 - accuracy: 0.9019 - val_loss: 0.2453 - val_accuracy: 0.9142\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.9047 - val_loss: 0.3023 - val_accuracy: 0.8907\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2441 - accuracy: 0.9043 - val_loss: 0.3055 - val_accuracy: 0.8988\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2479 - accuracy: 0.9047 - val_loss: 0.2789 - val_accuracy: 0.9143\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2394 - accuracy: 0.9057 - val_loss: 0.2659 - val_accuracy: 0.9149\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2400 - accuracy: 0.9059 - val_loss: 0.3021 - val_accuracy: 0.9173\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2441 - accuracy: 0.9044 - val_loss: 0.3035 - val_accuracy: 0.8927\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2383 - accuracy: 0.9085 - val_loss: 0.2938 - val_accuracy: 0.8869\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2446 - accuracy: 0.9041 - val_loss: 0.2226 - val_accuracy: 0.9176\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2359 - accuracy: 0.9073 - val_loss: 0.2968 - val_accuracy: 0.8775\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2475 - accuracy: 0.9179\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2543 - accuracy: 0.9017\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2549 - accuracy: 0.8993\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2524 - accuracy: 0.9012\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2476 - accuracy: 0.9029\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2444 - accuracy: 0.9051\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2476 - accuracy: 0.9023\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2411 - accuracy: 0.9065\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2437 - accuracy: 0.9049\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2421 - accuracy: 0.9050\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2422 - accuracy: 0.9050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6257 - accuracy: 0.6514 - val_loss: 0.5903 - val_accuracy: 0.7200\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5581 - accuracy: 0.7043 - val_loss: 0.5518 - val_accuracy: 0.7244\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5447 - accuracy: 0.7159 - val_loss: 0.5533 - val_accuracy: 0.7302\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5313 - accuracy: 0.7244 - val_loss: 0.5409 - val_accuracy: 0.7427\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4973 - accuracy: 0.7463 - val_loss: 0.4746 - val_accuracy: 0.7691\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4504 - accuracy: 0.7909 - val_loss: 0.4672 - val_accuracy: 0.8111\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4102 - accuracy: 0.8191 - val_loss: 0.4269 - val_accuracy: 0.8463\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3651 - accuracy: 0.8541 - val_loss: 0.3691 - val_accuracy: 0.8746\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3379 - accuracy: 0.8704 - val_loss: 0.4574 - val_accuracy: 0.8260\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8759 - val_loss: 0.2940 - val_accuracy: 0.8851\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8710 - val_loss: 0.3065 - val_accuracy: 0.9000\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3143 - accuracy: 0.8771 - val_loss: 0.4373 - val_accuracy: 0.8111\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2937 - accuracy: 0.8866 - val_loss: 0.3365 - val_accuracy: 0.9068\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3037 - accuracy: 0.8774 - val_loss: 0.3246 - val_accuracy: 0.8974\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2867 - accuracy: 0.8886 - val_loss: 0.3196 - val_accuracy: 0.8965\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.8916 - val_loss: 0.3462 - val_accuracy: 0.9019\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2990 - accuracy: 0.8807 - val_loss: 0.2851 - val_accuracy: 0.8969\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8920 - val_loss: 0.2752 - val_accuracy: 0.9094\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2693 - accuracy: 0.8922 - val_loss: 0.2901 - val_accuracy: 0.9103\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.8892 - val_loss: 0.3601 - val_accuracy: 0.9010\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2814 - accuracy: 0.8882 - val_loss: 0.3217 - val_accuracy: 0.9131\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2749 - accuracy: 0.8874 - val_loss: 0.4112 - val_accuracy: 0.8654\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8892 - val_loss: 0.3218 - val_accuracy: 0.8713\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2733 - accuracy: 0.8897 - val_loss: 0.2306 - val_accuracy: 0.9102\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8940 - val_loss: 0.2479 - val_accuracy: 0.9182\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2564 - accuracy: 0.8997 - val_loss: 0.2912 - val_accuracy: 0.9095\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8960 - val_loss: 0.2568 - val_accuracy: 0.9174\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8996 - val_loss: 0.2789 - val_accuracy: 0.9070\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2536 - accuracy: 0.9013 - val_loss: 0.2660 - val_accuracy: 0.9171\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2509 - accuracy: 0.9011 - val_loss: 0.2435 - val_accuracy: 0.9200\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8949 - val_loss: 0.2981 - val_accuracy: 0.9074\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2540 - accuracy: 0.9005 - val_loss: 0.2543 - val_accuracy: 0.9170\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2521 - accuracy: 0.8990 - val_loss: 0.2323 - val_accuracy: 0.9138\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2479 - accuracy: 0.9018 - val_loss: 0.2813 - val_accuracy: 0.9158\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2549 - accuracy: 0.8992 - val_loss: 0.2613 - val_accuracy: 0.9075\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8980 - val_loss: 0.2590 - val_accuracy: 0.9162\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2560 - accuracy: 0.8993 - val_loss: 0.2512 - val_accuracy: 0.9143\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2473 - accuracy: 0.9022 - val_loss: 0.2650 - val_accuracy: 0.9109\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8969 - val_loss: 0.2612 - val_accuracy: 0.9065\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2563 - accuracy: 0.8980 - val_loss: 0.2283 - val_accuracy: 0.9161\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2435 - accuracy: 0.9044 - val_loss: 0.2713 - val_accuracy: 0.9011\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2547 - accuracy: 0.9000 - val_loss: 0.2181 - val_accuracy: 0.9179\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2479 - accuracy: 0.9019 - val_loss: 0.2818 - val_accuracy: 0.9001\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2536 - accuracy: 0.9010 - val_loss: 0.2390 - val_accuracy: 0.9213\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2460 - accuracy: 0.9014 - val_loss: 0.3351 - val_accuracy: 0.8771\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2441 - accuracy: 0.9037 - val_loss: 0.3795 - val_accuracy: 0.8465\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2546 - accuracy: 0.8981 - val_loss: 0.3002 - val_accuracy: 0.8848\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2559 - accuracy: 0.8973 - val_loss: 0.3241 - val_accuracy: 0.9091\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2419 - accuracy: 0.9036 - val_loss: 0.2346 - val_accuracy: 0.9052\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2461 - accuracy: 0.9030 - val_loss: 0.2399 - val_accuracy: 0.9037\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2415 - accuracy: 0.9043\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2404 - accuracy: 0.9053\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2393 - accuracy: 0.9040\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2440 - accuracy: 0.9031\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2384 - accuracy: 0.9069\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2353 - accuracy: 0.9085\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2413 - accuracy: 0.9046\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2399 - accuracy: 0.9044\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2406 - accuracy: 0.9048\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2370 - accuracy: 0.9068\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2358 - accuracy: 0.9074\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6206 - accuracy: 0.6513 - val_loss: 0.5694 - val_accuracy: 0.7175\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5572 - accuracy: 0.7077 - val_loss: 0.5873 - val_accuracy: 0.7264\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5422 - accuracy: 0.7207 - val_loss: 0.5477 - val_accuracy: 0.7338\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5232 - accuracy: 0.7309 - val_loss: 0.4844 - val_accuracy: 0.7824\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4573 - accuracy: 0.7806 - val_loss: 0.4421 - val_accuracy: 0.8043\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4220 - accuracy: 0.8060 - val_loss: 0.4637 - val_accuracy: 0.8144\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3868 - accuracy: 0.8321 - val_loss: 0.4015 - val_accuracy: 0.8621\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3572 - accuracy: 0.8557 - val_loss: 0.3718 - val_accuracy: 0.8767\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8668 - val_loss: 0.3151 - val_accuracy: 0.8846\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8720 - val_loss: 0.3438 - val_accuracy: 0.8771\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8757 - val_loss: 0.3137 - val_accuracy: 0.9091\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3059 - accuracy: 0.8776 - val_loss: 0.3231 - val_accuracy: 0.8982\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3003 - accuracy: 0.8768 - val_loss: 0.3846 - val_accuracy: 0.9029\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8844 - val_loss: 0.2786 - val_accuracy: 0.8910\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2965 - accuracy: 0.8823 - val_loss: 0.3432 - val_accuracy: 0.8811\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2853 - accuracy: 0.8858 - val_loss: 0.3231 - val_accuracy: 0.9125\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2797 - accuracy: 0.8888 - val_loss: 0.2843 - val_accuracy: 0.9128\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8913 - val_loss: 0.2598 - val_accuracy: 0.9183\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8956 - val_loss: 0.3175 - val_accuracy: 0.9125\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8923 - val_loss: 0.3059 - val_accuracy: 0.9027\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2840 - accuracy: 0.8883 - val_loss: 0.2699 - val_accuracy: 0.9096\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2588 - accuracy: 0.8981 - val_loss: 0.2678 - val_accuracy: 0.9117\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2649 - accuracy: 0.8961 - val_loss: 0.5108 - val_accuracy: 0.8590\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8988 - val_loss: 0.2388 - val_accuracy: 0.9153\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2620 - accuracy: 0.8985 - val_loss: 0.3713 - val_accuracy: 0.9037\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8970 - val_loss: 0.3116 - val_accuracy: 0.9160\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2591 - accuracy: 0.8984 - val_loss: 0.3051 - val_accuracy: 0.9065\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2577 - accuracy: 0.8998 - val_loss: 0.2514 - val_accuracy: 0.9127\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8983 - val_loss: 0.2768 - val_accuracy: 0.9123\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2501 - accuracy: 0.9024 - val_loss: 0.2545 - val_accuracy: 0.9238\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2435 - accuracy: 0.9040 - val_loss: 0.2545 - val_accuracy: 0.9194\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2453 - accuracy: 0.9044 - val_loss: 0.2474 - val_accuracy: 0.9045\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8982 - val_loss: 0.2670 - val_accuracy: 0.9160\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2531 - accuracy: 0.9009 - val_loss: 0.2543 - val_accuracy: 0.9182\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2464 - accuracy: 0.9057 - val_loss: 0.2501 - val_accuracy: 0.8968\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2515 - accuracy: 0.9037 - val_loss: 0.3080 - val_accuracy: 0.8963\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8984 - val_loss: 0.2351 - val_accuracy: 0.9108\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2419 - accuracy: 0.9054 - val_loss: 0.2458 - val_accuracy: 0.9204\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2455 - accuracy: 0.9020 - val_loss: 0.2758 - val_accuracy: 0.8854\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2447 - accuracy: 0.9048 - val_loss: 0.4043 - val_accuracy: 0.8670\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2490 - accuracy: 0.9014 - val_loss: 0.2520 - val_accuracy: 0.9199\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2449 - accuracy: 0.9055 - val_loss: 0.2468 - val_accuracy: 0.9145\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2430 - accuracy: 0.9066 - val_loss: 0.2737 - val_accuracy: 0.9174\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2430 - accuracy: 0.9047 - val_loss: 0.2406 - val_accuracy: 0.9044\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.9053 - val_loss: 0.2260 - val_accuracy: 0.9185\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2583 - accuracy: 0.9223\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2508 - accuracy: 0.9025\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.9028\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2525 - accuracy: 0.9020\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2488 - accuracy: 0.9034\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2443 - accuracy: 0.9047\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2419 - accuracy: 0.9054\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2494 - accuracy: 0.9030\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2459 - accuracy: 0.9036\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2485 - accuracy: 0.9030\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2423 - accuracy: 0.9045\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6263 - accuracy: 0.6452 - val_loss: 0.5630 - val_accuracy: 0.7228\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5525 - accuracy: 0.7145 - val_loss: 0.5565 - val_accuracy: 0.7259\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5411 - accuracy: 0.7191 - val_loss: 0.5471 - val_accuracy: 0.7296\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5295 - accuracy: 0.7280 - val_loss: 0.5063 - val_accuracy: 0.7574\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4913 - accuracy: 0.7497 - val_loss: 0.4393 - val_accuracy: 0.7865\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4505 - accuracy: 0.7815 - val_loss: 0.4286 - val_accuracy: 0.8030\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4230 - accuracy: 0.8050 - val_loss: 0.4109 - val_accuracy: 0.8368\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3967 - accuracy: 0.8303 - val_loss: 0.3821 - val_accuracy: 0.8620\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3581 - accuracy: 0.8560 - val_loss: 0.3575 - val_accuracy: 0.8867\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3458 - accuracy: 0.8622 - val_loss: 0.3165 - val_accuracy: 0.8822\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8679 - val_loss: 0.3306 - val_accuracy: 0.8999\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3198 - accuracy: 0.8729 - val_loss: 0.4603 - val_accuracy: 0.8099\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3262 - accuracy: 0.8662 - val_loss: 0.4618 - val_accuracy: 0.8980\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3095 - accuracy: 0.8712 - val_loss: 0.2992 - val_accuracy: 0.8803\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3139 - accuracy: 0.8747 - val_loss: 0.3827 - val_accuracy: 0.8951\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2979 - accuracy: 0.8799 - val_loss: 0.3227 - val_accuracy: 0.9057\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2914 - accuracy: 0.8837 - val_loss: 0.2950 - val_accuracy: 0.9073\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2890 - accuracy: 0.8853 - val_loss: 0.3823 - val_accuracy: 0.9008\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2878 - accuracy: 0.8850 - val_loss: 0.2569 - val_accuracy: 0.8966\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2849 - accuracy: 0.8841 - val_loss: 0.3354 - val_accuracy: 0.8993\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8880 - val_loss: 0.2846 - val_accuracy: 0.9180\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2722 - accuracy: 0.8918 - val_loss: 0.3030 - val_accuracy: 0.8974\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2756 - accuracy: 0.8891 - val_loss: 0.2728 - val_accuracy: 0.9034\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2662 - accuracy: 0.8943 - val_loss: 0.3142 - val_accuracy: 0.9078\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.8891 - val_loss: 0.2742 - val_accuracy: 0.9146\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2684 - accuracy: 0.8907 - val_loss: 0.2447 - val_accuracy: 0.9038\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2807 - accuracy: 0.8878 - val_loss: 0.2784 - val_accuracy: 0.9157\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8965 - val_loss: 0.2678 - val_accuracy: 0.9150\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8957 - val_loss: 0.3689 - val_accuracy: 0.9163\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2557 - accuracy: 0.9003 - val_loss: 0.2935 - val_accuracy: 0.9090\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8958 - val_loss: 0.2638 - val_accuracy: 0.9150\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8947 - val_loss: 0.3516 - val_accuracy: 0.9054\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2559 - accuracy: 0.8991 - val_loss: 0.3201 - val_accuracy: 0.9059\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2522 - accuracy: 0.8963 - val_loss: 0.2760 - val_accuracy: 0.8953\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2471 - accuracy: 0.9006 - val_loss: 0.2891 - val_accuracy: 0.9089\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8973 - val_loss: 0.3090 - val_accuracy: 0.8869\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2874 - accuracy: 0.9173\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2757 - accuracy: 0.8901\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2797 - accuracy: 0.8911\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2765 - accuracy: 0.8900\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2657 - accuracy: 0.8945\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2662 - accuracy: 0.8949\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2705 - accuracy: 0.8924\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2591 - accuracy: 0.8967\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2592 - accuracy: 0.8972\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2542 - accuracy: 0.8986\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2607 - accuracy: 0.8973\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6228 - accuracy: 0.6503 - val_loss: 0.5673 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5564 - accuracy: 0.7042 - val_loss: 0.5803 - val_accuracy: 0.7253\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5455 - accuracy: 0.7201 - val_loss: 0.5858 - val_accuracy: 0.7158\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5352 - accuracy: 0.7228 - val_loss: 0.5413 - val_accuracy: 0.7339\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5254 - accuracy: 0.7282 - val_loss: 0.4876 - val_accuracy: 0.7660\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4798 - accuracy: 0.7635 - val_loss: 0.4575 - val_accuracy: 0.7751\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4408 - accuracy: 0.7958 - val_loss: 0.4012 - val_accuracy: 0.8510\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3967 - accuracy: 0.8285 - val_loss: 0.3782 - val_accuracy: 0.8560\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3672 - accuracy: 0.8516 - val_loss: 0.4008 - val_accuracy: 0.8330\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3477 - accuracy: 0.8594 - val_loss: 0.4008 - val_accuracy: 0.8656\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3308 - accuracy: 0.8683 - val_loss: 0.3554 - val_accuracy: 0.9019\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3189 - accuracy: 0.8719 - val_loss: 0.2969 - val_accuracy: 0.8989\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3011 - accuracy: 0.8799 - val_loss: 0.3162 - val_accuracy: 0.9062\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2940 - accuracy: 0.8829 - val_loss: 0.2942 - val_accuracy: 0.9025\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2931 - accuracy: 0.8834 - val_loss: 0.3290 - val_accuracy: 0.8709\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2950 - accuracy: 0.8805 - val_loss: 0.2774 - val_accuracy: 0.8957\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8846 - val_loss: 0.2948 - val_accuracy: 0.8985\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2819 - accuracy: 0.8880 - val_loss: 0.3275 - val_accuracy: 0.9041\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8863 - val_loss: 0.3225 - val_accuracy: 0.8860\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2759 - accuracy: 0.8898 - val_loss: 0.2842 - val_accuracy: 0.9149\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8919 - val_loss: 0.2791 - val_accuracy: 0.9006\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2696 - accuracy: 0.8933 - val_loss: 0.2556 - val_accuracy: 0.9005\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2654 - accuracy: 0.8960 - val_loss: 0.2783 - val_accuracy: 0.9014\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2669 - accuracy: 0.8942 - val_loss: 0.2863 - val_accuracy: 0.9042\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2656 - accuracy: 0.8918 - val_loss: 0.2825 - val_accuracy: 0.8968\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2619 - accuracy: 0.8956 - val_loss: 0.2750 - val_accuracy: 0.9199\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2560 - accuracy: 0.8975 - val_loss: 0.2719 - val_accuracy: 0.9146\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8975 - val_loss: 0.2486 - val_accuracy: 0.9136\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2506 - accuracy: 0.9016 - val_loss: 0.2696 - val_accuracy: 0.9155\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2494 - accuracy: 0.9001 - val_loss: 0.2886 - val_accuracy: 0.9009\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2673 - accuracy: 0.8931 - val_loss: 0.2416 - val_accuracy: 0.8917\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8890 - val_loss: 0.2849 - val_accuracy: 0.9185\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8976 - val_loss: 0.2979 - val_accuracy: 0.9136\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.9005 - val_loss: 0.3861 - val_accuracy: 0.8868\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2669 - accuracy: 0.8923 - val_loss: 0.2446 - val_accuracy: 0.9131\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8957 - val_loss: 0.2360 - val_accuracy: 0.9176\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8987 - val_loss: 0.2468 - val_accuracy: 0.9155\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2400 - accuracy: 0.9068 - val_loss: 0.2720 - val_accuracy: 0.8964\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2469 - accuracy: 0.9037 - val_loss: 0.2691 - val_accuracy: 0.9053\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2432 - accuracy: 0.9044 - val_loss: 0.2452 - val_accuracy: 0.9199\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2454 - accuracy: 0.9037 - val_loss: 0.2564 - val_accuracy: 0.9219\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2394 - accuracy: 0.9059 - val_loss: 0.2319 - val_accuracy: 0.9162\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2371 - accuracy: 0.9076 - val_loss: 0.2786 - val_accuracy: 0.9025\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2450 - accuracy: 0.9039 - val_loss: 0.2765 - val_accuracy: 0.9139\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2491 - accuracy: 0.9024 - val_loss: 0.2130 - val_accuracy: 0.9225\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2367 - accuracy: 0.9079 - val_loss: 0.2492 - val_accuracy: 0.9162\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2473 - accuracy: 0.9018 - val_loss: 0.2528 - val_accuracy: 0.9194\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2368 - accuracy: 0.9070 - val_loss: 0.2339 - val_accuracy: 0.9072\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.9026 - val_loss: 0.2425 - val_accuracy: 0.9163\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.9018 - val_loss: 0.2494 - val_accuracy: 0.9226\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2547 - accuracy: 0.9181\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2411 - accuracy: 0.9051\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2415 - accuracy: 0.9047\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2417 - accuracy: 0.9062\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2404 - accuracy: 0.9047\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2388 - accuracy: 0.9064\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2375 - accuracy: 0.9059\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2349 - accuracy: 0.9090\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2381 - accuracy: 0.9057\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2344 - accuracy: 0.9078\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2339 - accuracy: 0.9080\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgZcqFsm5AFT",
        "outputId": "253cd48e-ad5e-4b02-b9a3-b370f9dc6dc0"
      },
      "source": [
        "cv_preds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [1],\n",
              "       ...,\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6OvtjylHYOL",
        "outputId": "35b196f2-3c7a-429a-9ca7-61ffe95393a3"
      },
      "source": [
        "len(cv_preds[cv_preds == 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "35199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhK6utE3IW9Z",
        "outputId": "10e6682e-26cb-461f-8cb1-a2c1a7aba096"
      },
      "source": [
        "cm = confusion_matrix(y, cv_preds)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[51518  6482]\n",
            " [ 1930 28717]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AD-tbKBauWhl"
      },
      "source": [
        "# neural network on URL attributes (table 1)\n",
        "\n",
        "### (20 features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "1PQ4dcaouS2L",
        "outputId": "c75bbe5e-72ee-4551-db69-91b6cac039cb"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table1 = ['qty_dot_url', 'qty_hyphen_url', 'qty_underline_url', 'qty_slash_url', 'qty_questionmark_url', 'qty_equal_url',\n",
        "                   'qty_at_url', 'qty_and_url', 'qty_exclamation_url', 'qty_space_url', 'qty_tilde_url', 'qty_comma_url',\n",
        "                   'qty_plus_url', 'qty_asterisk_url', 'qty_hashtag_url', 'qty_dollar_url', 'qty_percent_url', 'qty_tld_url',\n",
        "                   'length_url', 'email_in_url'] \n",
        "\n",
        "X = full_df[features_table1]\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_underline_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>qty_questionmark_url</th>\n",
              "      <th>qty_equal_url</th>\n",
              "      <th>qty_at_url</th>\n",
              "      <th>qty_and_url</th>\n",
              "      <th>qty_exclamation_url</th>\n",
              "      <th>qty_space_url</th>\n",
              "      <th>qty_tilde_url</th>\n",
              "      <th>qty_comma_url</th>\n",
              "      <th>qty_plus_url</th>\n",
              "      <th>qty_asterisk_url</th>\n",
              "      <th>qty_hashtag_url</th>\n",
              "      <th>qty_dollar_url</th>\n",
              "      <th>qty_percent_url</th>\n",
              "      <th>qty_tld_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>email_in_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_url  qty_hyphen_url  ...  length_url  email_in_url\n",
              "5676             1               0  ...          11             0\n",
              "39002            3               0  ...          13             0\n",
              "1732             2               0  ...          16             0\n",
              "39668            2               0  ...          26             0\n",
              "82035            2               0  ...          28             0\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxnR5BBuFPbd"
      },
      "source": [
        "### neural net - build/fit/predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JybKWkn1U8i",
        "outputId": "c7c6f18d-599a-41f3-e072-1c3fb7fc0c26"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "table1_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[20]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "table1_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "table1_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                1344      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 14,241\n",
            "Trainable params: 14,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evPK4SLp1U6a",
        "outputId": "a250dd07-5c3c-4908-da13-b1d1b0a587d7"
      },
      "source": [
        "history = table1_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.5745 - binary_accuracy: 0.7209 - auc: 0.7738 - val_loss: 0.3422 - val_binary_accuracy: 0.8353 - val_auc: 0.9512\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3169 - binary_accuracy: 0.8684 - auc: 0.9374 - val_loss: 0.3037 - val_binary_accuracy: 0.8661 - val_auc: 0.9513\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2787 - binary_accuracy: 0.8820 - auc: 0.9453 - val_loss: 0.2908 - val_binary_accuracy: 0.8804 - val_auc: 0.9531\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2733 - binary_accuracy: 0.8840 - auc: 0.9465 - val_loss: 0.2777 - val_binary_accuracy: 0.8887 - val_auc: 0.9540\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2692 - binary_accuracy: 0.8844 - auc: 0.9467 - val_loss: 0.2586 - val_binary_accuracy: 0.8910 - val_auc: 0.9546\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2644 - binary_accuracy: 0.8834 - auc: 0.9473 - val_loss: 0.2649 - val_binary_accuracy: 0.8935 - val_auc: 0.9555\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2546 - binary_accuracy: 0.8892 - auc: 0.9508 - val_loss: 0.2657 - val_binary_accuracy: 0.8963 - val_auc: 0.9566\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2600 - binary_accuracy: 0.8887 - auc: 0.9502 - val_loss: 0.3026 - val_binary_accuracy: 0.8930 - val_auc: 0.9567\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2518 - binary_accuracy: 0.8907 - auc: 0.9525 - val_loss: 0.2797 - val_binary_accuracy: 0.8930 - val_auc: 0.9576\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2536 - binary_accuracy: 0.8883 - auc: 0.9517 - val_loss: 0.2779 - val_binary_accuracy: 0.8969 - val_auc: 0.9591\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2518 - binary_accuracy: 0.8896 - auc: 0.9528 - val_loss: 0.2792 - val_binary_accuracy: 0.8939 - val_auc: 0.9591\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2444 - binary_accuracy: 0.8930 - auc: 0.9552 - val_loss: 0.2905 - val_binary_accuracy: 0.8947 - val_auc: 0.9588\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2466 - binary_accuracy: 0.8911 - auc: 0.9545 - val_loss: 0.2866 - val_binary_accuracy: 0.8893 - val_auc: 0.9584\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2499 - binary_accuracy: 0.8900 - auc: 0.9538 - val_loss: 0.2578 - val_binary_accuracy: 0.8980 - val_auc: 0.9606\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2456 - binary_accuracy: 0.8942 - auc: 0.9553 - val_loss: 0.2556 - val_binary_accuracy: 0.8976 - val_auc: 0.9603\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2392 - binary_accuracy: 0.8941 - auc: 0.9569 - val_loss: 0.2392 - val_binary_accuracy: 0.9016 - val_auc: 0.9604\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2443 - binary_accuracy: 0.8939 - auc: 0.9550 - val_loss: 0.2670 - val_binary_accuracy: 0.8979 - val_auc: 0.9603\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2441 - binary_accuracy: 0.8934 - auc: 0.9555 - val_loss: 0.2601 - val_binary_accuracy: 0.8956 - val_auc: 0.9604\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2409 - binary_accuracy: 0.8944 - auc: 0.9564 - val_loss: 0.2527 - val_binary_accuracy: 0.8951 - val_auc: 0.9604\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2392 - binary_accuracy: 0.8956 - auc: 0.9573 - val_loss: 0.2677 - val_binary_accuracy: 0.8919 - val_auc: 0.9611\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2387 - binary_accuracy: 0.8970 - auc: 0.9568 - val_loss: 0.2481 - val_binary_accuracy: 0.8963 - val_auc: 0.9600\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2420 - binary_accuracy: 0.8956 - auc: 0.9564 - val_loss: 0.2619 - val_binary_accuracy: 0.8974 - val_auc: 0.9607\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2411 - binary_accuracy: 0.8962 - auc: 0.9560 - val_loss: 0.2516 - val_binary_accuracy: 0.8978 - val_auc: 0.9609\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2398 - binary_accuracy: 0.8944 - auc: 0.9581 - val_loss: 0.2634 - val_binary_accuracy: 0.8900 - val_auc: 0.9617\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2424 - binary_accuracy: 0.8946 - auc: 0.9566 - val_loss: 0.2652 - val_binary_accuracy: 0.8906 - val_auc: 0.9608\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2333 - binary_accuracy: 0.9005 - auc: 0.9598 - val_loss: 0.2556 - val_binary_accuracy: 0.8977 - val_auc: 0.9611\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2360 - binary_accuracy: 0.8957 - auc: 0.9583 - val_loss: 0.2473 - val_binary_accuracy: 0.8980 - val_auc: 0.9618\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2386 - binary_accuracy: 0.8982 - auc: 0.9580 - val_loss: 0.2559 - val_binary_accuracy: 0.8943 - val_auc: 0.9624\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2383 - binary_accuracy: 0.8962 - auc: 0.9578 - val_loss: 0.2537 - val_binary_accuracy: 0.8921 - val_auc: 0.9636\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2405 - binary_accuracy: 0.8942 - auc: 0.9578 - val_loss: 0.2551 - val_binary_accuracy: 0.8966 - val_auc: 0.9628\n",
            "Epoch 31/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2349 - binary_accuracy: 0.8987 - auc: 0.9594 - val_loss: 0.2681 - val_binary_accuracy: 0.8912 - val_auc: 0.9614\n",
            "Epoch 32/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2377 - binary_accuracy: 0.8970 - auc: 0.9586 - val_loss: 0.2451 - val_binary_accuracy: 0.8987 - val_auc: 0.9618\n",
            "Epoch 33/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2353 - binary_accuracy: 0.8994 - auc: 0.9591 - val_loss: 0.2509 - val_binary_accuracy: 0.8949 - val_auc: 0.9619\n",
            "Epoch 34/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2373 - binary_accuracy: 0.8966 - auc: 0.9585 - val_loss: 0.2504 - val_binary_accuracy: 0.8991 - val_auc: 0.9619\n",
            "Epoch 35/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2309 - binary_accuracy: 0.8989 - auc: 0.9603 - val_loss: 0.2578 - val_binary_accuracy: 0.8936 - val_auc: 0.9615\n",
            "Epoch 36/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2348 - binary_accuracy: 0.8990 - auc: 0.9590 - val_loss: 0.2541 - val_binary_accuracy: 0.8957 - val_auc: 0.9618\n",
            "Epoch 37/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2351 - binary_accuracy: 0.8981 - auc: 0.9588 - val_loss: 0.2476 - val_binary_accuracy: 0.8981 - val_auc: 0.9623\n",
            "Epoch 38/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2339 - binary_accuracy: 0.8975 - auc: 0.9593 - val_loss: 0.2519 - val_binary_accuracy: 0.8941 - val_auc: 0.9616\n",
            "Epoch 39/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2342 - binary_accuracy: 0.8992 - auc: 0.9596 - val_loss: 0.2532 - val_binary_accuracy: 0.8941 - val_auc: 0.9644\n",
            "Epoch 40/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2335 - binary_accuracy: 0.8988 - auc: 0.9598 - val_loss: 0.2412 - val_binary_accuracy: 0.8989 - val_auc: 0.9637\n",
            "Epoch 41/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2381 - binary_accuracy: 0.8976 - auc: 0.9587 - val_loss: 0.2365 - val_binary_accuracy: 0.9010 - val_auc: 0.9620\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "QEylyqAB1U4T",
        "outputId": "23359720-127f-4b3d-9857-48f70c4b4484"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "history_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>41.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>41.000000</td>\n",
              "      <td>41.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.250756</td>\n",
              "      <td>0.891133</td>\n",
              "      <td>0.952937</td>\n",
              "      <td>0.264266</td>\n",
              "      <td>0.892683</td>\n",
              "      <td>0.959750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.038571</td>\n",
              "      <td>0.015748</td>\n",
              "      <td>0.017249</td>\n",
              "      <td>0.020358</td>\n",
              "      <td>0.010918</td>\n",
              "      <td>0.003238</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.233673</td>\n",
              "      <td>0.799996</td>\n",
              "      <td>0.849166</td>\n",
              "      <td>0.236486</td>\n",
              "      <td>0.835305</td>\n",
              "      <td>0.951205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.236118</td>\n",
              "      <td>0.890286</td>\n",
              "      <td>0.953659</td>\n",
              "      <td>0.251932</td>\n",
              "      <td>0.892109</td>\n",
              "      <td>0.958815</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.240125</td>\n",
              "      <td>0.894862</td>\n",
              "      <td>0.956931</td>\n",
              "      <td>0.257829</td>\n",
              "      <td>0.894916</td>\n",
              "      <td>0.960680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.250152</td>\n",
              "      <td>0.897849</td>\n",
              "      <td>0.958915</td>\n",
              "      <td>0.268100</td>\n",
              "      <td>0.897674</td>\n",
              "      <td>0.961842</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.474800</td>\n",
              "      <td>0.899246</td>\n",
              "      <td>0.959894</td>\n",
              "      <td>0.342210</td>\n",
              "      <td>0.901584</td>\n",
              "      <td>0.964359</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  41.000000        41.000000  ...            41.000000  41.000000\n",
              "mean    0.250756         0.891133  ...             0.892683   0.959750\n",
              "std     0.038571         0.015748  ...             0.010918   0.003238\n",
              "min     0.233673         0.799996  ...             0.835305   0.951205\n",
              "25%     0.236118         0.890286  ...             0.892109   0.958815\n",
              "50%     0.240125         0.894862  ...             0.894916   0.960680\n",
              "75%     0.250152         0.897849  ...             0.897674   0.961842\n",
              "max     0.474800         0.899246  ...             0.901584   0.964359\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rXapaDp1U2O",
        "outputId": "d1db3644-4719-43bd-f2c8-30da12cdb6fd"
      },
      "source": [
        "train_acc = table1_nn.evaluate(train_X, train_y)\n",
        "test_acc = table1_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 2ms/step - loss: 0.2421 - binary_accuracy: 0.9000 - auc: 0.9598\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2449 - binary_accuracy: 0.8989 - auc: 0.9594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ADQEBa41Uz5",
        "outputId": "26ccfb0a-030e-47f3-939c-f4253b840db4"
      },
      "source": [
        "dict(zip(table1_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.9594447016716003,\n",
              " 'binary_accuracy': 0.8989260792732239,\n",
              " 'loss': 0.24493859708309174}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "3UfuVOqb1xJ1",
        "outputId": "a8d622be-9054-4a22-9820-ff3da76e6cec"
      },
      "source": [
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.23648624122142792\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUZfbA8e+ZkhlSqAkJvSgdFDAgFrCsBSvWxd5l7d1Vd93VdXXdn7vqNldXXbsuoOiKomJDAXWVgBTpHRJCSCCE9DLz/v54JxBCEiZ1kjvn8zzzzMy9d+aeuYQzd859ixhjUEop5VyuSAeglFKqeWmiV0oph9NEr5RSDqeJXimlHE4TvVJKOZwn0gFUl5iYaPr27RvpMJRSqk1ZuHBhjjEmqaZ1rS7R9+3bl7S0tEiHoZRSbYqIbK5tnZZulFLK4TTRK6WUw2miV0oph2t1NXqlVHQqLy8nPT2dkpKSSIfSqvn9fnr27InX6w37NZrolVKtQnp6OgkJCfTt2xcRiXQ4rZIxhp07d5Kenk6/fv3Cfp2WbpRSrUJJSQldunTRJF8HEaFLly71/tWjiV4p1Wpokj+4hhwjxyT6/JJynv5sDYu37o50KEop1ao4JtEHgoa/frGWH7fkRjoUpVQbFR8fH+kQmoVjEn2cz15XLiipiHAkSinVujgm0XvdLvxeFwWlmuiVUo1jjOHee+9l+PDhjBgxgmnTpgGQmZnJhAkTGDlyJMOHD2fevHkEAgGuuuqqvds+/fTTEY7+QI5qXhnv85CviV6pNu93HyxnxbY9TfqeQ7u356GzhoW17bvvvsvixYtZsmQJOTk5jBkzhgkTJvDWW29x6qmn8utf/5pAIEBRURGLFy8mIyODn376CYDdu1vfdULHnNGDTfRaulFKNdb8+fO5+OKLcbvdJCcnc9xxx7FgwQLGjBnDyy+/zMMPP8yyZctISEigf//+bNiwgVtvvZVPPvmE9u3bRzr8AzjrjN7voVDP6JVq88I9825pEyZMYO7cucyaNYurrrqKu+66iyuuuIIlS5Ywe/ZsnnvuOaZPn85LL70U6VD347gzei3dKKUaa/z48UybNo1AIEB2djZz585l7NixbN68meTkZK6//nquu+46Fi1aRE5ODsFgkPPPP59HH32URYsWRTr8AzjrjN7nZdvu4kiHoZRq484991y+++47Dj/8cESEJ554gpSUFF599VX+9Kc/4fV6iY+P57XXXiMjI4Orr76aYDAIwOOPPx7h6A8kxphIx7Cf1NRU09CJR+6ctpiFm3OZ+8sTmjgqpVRzW7lyJUOGDIl0GG1CTcdKRBYaY1Jr2t5RpZs4n1ubVyqlVDWOSvTxPq+2ulFKqWoclegT/B7KAkFKKwKRDkUppVoNRyX6+NAwCIWlmuiVUqqSIxO9lm+UUmofZyV6v030+aXlEY5EKaVaD0cl+gQ9o1dKqQM4KtHvHapYm1gqpZpZXWPXb9q0ieHDh7dgNHULK9GLyEQRWS0i60Tk/jq2O19EjIikhp73FZFiEVkcuj3XVIHXpLJ0o4leKaX2OegQCCLiBp4BTgbSgQUiMtMYs6LadgnA7cD31d5ivTFmZBPFW6fK0k2+lm6Uats+vh+2L2va90wZAaf9sdbV999/P7169eLmm28G4OGHH8bj8TBnzhxyc3MpLy/n0UcfZdKkSfXabUlJCTfeeCNpaWl4PB6eeuopTjjhBJYvX87VV19NWVkZwWCQGTNm0L17d37+85+Tnp5OIBDgN7/5DZMnT27Ux4bwxroZC6wzxmwAEJGpwCRgRbXtfg/8H3Bvo6NqoMozeh3BUilVX5MnT+aOO+7Ym+inT5/O7Nmzue2222jfvj05OTmMGzeOs88+u14TdD/zzDOICMuWLWPVqlWccsoprFmzhueee47bb7+dSy+9lLKyMgKBAB999BHdu3dn1qxZAOTl5TXJZwsn0fcAtlZ5ng4cWXUDERkN9DLGzBKR6om+n4j8COwBHjTGzGtMwHVp53XjEi3dKNXm1XHm3VxGjRrFjh072LZtG9nZ2XTq1ImUlBTuvPNO5s6di8vlIiMjg6ysLFJSUsJ+3/nz53PrrbcCMHjwYPr06cOaNWs46qijeOyxx0hPT+e8885jwIABjBgxgrvvvpv77ruPM888k/HjxzfJZ2v0xVgRcQFPAXfXsDoT6G2MGQXcBbwlIgeMyi8iU0QkTUTSsrOzGxOLHapYSzdKqQa48MILeeedd5g2bRqTJ0/mzTffJDs7m4ULF7J48WKSk5MpKSlpkn1dcsklzJw5k3bt2nH66afz5ZdfMnDgQBYtWsSIESN48MEHeeSRR5pkX+Ek+gygV5XnPUPLKiUAw4GvRGQTMA6YKSKpxphSY8xOAGPMQmA9MLD6DowxzxtjUo0xqUlJSQ37JJXB+L16Rq+UapDJkyczdepU3nnnHS688ELy8vLo2rUrXq+XOXPmsHnz5nq/5/jx43nzzTcBWLNmDVu2bGHQoEFs2LCB/v37c9tttzFp0iSWLl3Ktm3biI2N5bLLLuPee+9tsrHtwyndLAAGiEg/bIK/CLikcqUxJg9IrHwuIl8B9xhj0kQkCdhljAmISH9gALChSSKvRZzPre3olVINMmzYMPLz8+nRowfdunXj0ksv5ayzzmLEiBGkpqYyePDger/nTTfdxI033siIESPweDy88sor+Hw+pk+fzuuvv47X6yUlJYVf/epXLFiwgHvvvReXy4XX6+XZZ59tks8V1nj0InI68BfADbxkjHlMRB4B0owxM6tt+xX7Ev35wCNAORAEHjLGfFDXvhozHj3Aef/8htgYD29cd+TBN1ZKtRo6Hn346jsefVgzTBljPgI+qrbst7Vse3yVxzOAGeHso6nE+73kFesQCEopVclRUwmCbUuv0wkqpVrCsmXLuPzyy/db5vP5+P776t2JIstxiT7e59EavVJtlDGmXm3UI23EiBEsXry4RffZkOlfHTXWDdhOU9rqRqm2x+/3s3PnzgYlsmhhjGHnzp34/f56vc6ZZ/SlFQSDBper7ZwZKBXtevbsSXp6Oo3pSxMN/H4/PXv2rNdrHJnoAQrLKkjweyMcjVIqXF6vl379+kU6DEdyZOkGdBgEpZSq5LxEr5OPKKXUfpyX6PWMXiml9uO4RJ+gs0wppdR+HJfo957Ra+lGKaUAJyb6ylmm9IxeKaUAByd6PaNXSinLcYk+Tmv0Sim1H8cleq/bhd/r0kSvlFIhjkv0APE+nWVKKaUqOTLRJ/h1BEullKrkyERfObCZUkophyZ6nTdWKaX2cWSij/d5tR29UkqFODLRJ/g9FJTqvLFKKQUOTfQ6naBSSu3jzETv91BYGoh0GEop1So4M9H7PJQFgpRWaLJXSilHJvoEHcFSKaX2cmSij4vR8W6UUqqSIxN95Zj0+XpGr5RSzkz0OsuUUkrt48hEr7NMKaXUPs5M9KEz+sIyTfRKKeXMRK81eqWU2suRiT7B5wW0Rq+UUuDQRO/3unCJ1uiVUgrCTPQiMlFEVovIOhG5v47tzhcRIyKpVZY9EHrdahE5tSmCDiNeHZNeKaVCPAfbQETcwDPAyUA6sEBEZhpjVlTbLgG4Hfi+yrKhwEXAMKA78LmIDDTGNPvYBAl+r9bolVKK8M7oxwLrjDEbjDFlwFRgUg3b/R74P6CkyrJJwFRjTKkxZiOwLvR+zc6e0etQxUopFU6i7wFsrfI8PbRsLxEZDfQyxsyq72ubi45gqZRSVqMvxoqIC3gKuLsR7zFFRNJEJC07O7uxIQH2jF5nmVJKqfASfQbQq8rznqFllRKA4cBXIrIJGAfMDF2QPdhrATDGPG+MSTXGpCYlJdXvE9Qi3u+hoERLN0opFU6iXwAMEJF+IhKDvbg6s3KlMSbPGJNojOlrjOkL/A842xiTFtruIhHxiUg/YADwQ5N/ihrEx2irG6WUgjBa3RhjKkTkFmA24AZeMsYsF5FHgDRjzMw6XrtcRKYDK4AK4OaWaHEDlWf0muiVUuqgiR7AGPMR8FG1Zb+tZdvjqz1/DHisgfE1WLzPQ2FZgEDQ4HZJS+9eKaVaDUf2jIV9s0zpwGZKqWjn2ES/dwRLrdMrpaKccxO9jkmvlFKAkxN96Ixe29IrpaKd4xO9ntErpaKdcxO9X+eNVUopcHKi1zN6pZQCHJzoK2eZ0hq9UiraOTbRx/ncgDavVEopxyZ6j9tFO69ba/RKqajn2EQP9oKszjKllIp2zk70Om+sUkpFQaLXMemVUlHO+Ylez+iVUlHO2Ylea/RKKeXsRJ/g8+gwxUqpqOfoRK+zTCmllMMTfVyoRm+MiXQoSikVMY5O9PE+D+UBQ2lFMNKhKKVUxDg60SfoCJZKKeXsRK8jWCqlVLQkej2jV0pFMWcnei3dKKWUsxN95Zj0WrpRSkUzRyf6yjHp9YxeKRXNHJ3oK0s3OsuUUiqaOTrRa+lGKaUcnuj9Xhdul1BQqkMVK6Wil6MTvYiExqTXM3qlVPRydKKHyjHpA5EOQymlIsbxiT7B79HSjVIqqjk+0cfpLFNKqSgXVqIXkYkislpE1onI/TWsv0FElonIYhGZLyJDQ8v7ikhxaPliEXmuqT/AwWiNXikV7TwH20BE3MAzwMlAOrBARGYaY1ZU2ewtY8xzoe3PBp4CJobWrTfGjGzasMMX7/ewNbcoUrtXSqmIC+eMfiywzhizwRhTBkwFJlXdwBizp8rTOKDVzPSRoGf0SqkoF06i7wFsrfI8PbRsPyJys4isB54Abquyqp+I/CgiX4vI+EZF2wDxWqNXSkW5JrsYa4x5xhhzCHAf8GBocSbQ2xgzCrgLeEtE2ld/rYhMEZE0EUnLzs5uqpAAW7opKgsQCLaaHxlKKdWiwkn0GUCvKs97hpbVZipwDoAxptQYszP0eCGwHhhY/QXGmOeNManGmNSkpKRwYw9L5Zj0hWV6Vq+Uik7hJPoFwAAR6SciMcBFwMyqG4jIgCpPzwDWhpYnhS7mIiL9gQHAhqYIPFw6y5RSKtodtNWNMaZCRG4BZgNu4CVjzHIReQRIM8bMBG4RkZOAciAXuDL08gnAIyJSDgSBG4wxu5rjg9RGJx9RSkW7gyZ6AGPMR8BH1Zb9tsrj22t53QxgRmMCbKzKM/p8PaNXSkUp5/SMzcuA54+HFe/vtzhBz+iVUlHOOYk+vitkr4ZN8/dfrGPSK6WinHMSvdsLvcbC5m/3W1xZoy/UM3qlVJRyTqIH6HMMZC2H4ty9i/bW6DXRK6WilLMSfe+jAANbvt+7KC4mNEG4lm6UUlHKWYm+Zyq4vLD5m72LPG4X7bxuHZNeKRW1nJXove2gxxE11um11Y1SKlo5K9ED9DkKMhdDWeHeRQk+j7ajV0pFLQcm+mMgWAHpC/Yu0jN6pVQ0c16i7zUWxLVf+Sbe59HmlUqpqOW8RO/vACkjDkj0WrpRSkUr5yV6gN5H29JNRRmgk48opaKbMxN9n6OhogS2/QhojV4pFd2cm+hhb3v6+NC8scboLFNKqejjzEQflwiJg/bW6eP9HiqChtKKYIQDU0qplufMRA+2Pf3W7yEYIEHHpFdKRTEHJ/pjoHQPZP2kI1gqpaKagxN9ZZ3+W+JidPIRpVT0cm6i79ATOvaGzd/uPaPX0o1SKho5N9GDbU+/+VsS9IxeKRXFnJ3o+xwNRTl0KtkEoEMVK6WiksMT/TEAdNhhBzjTyUeUUtHI2Ym+yyEQl0Rs5g+ATieolIpOzk70ItDnaFxbv8XjEm1eqZSKSs5O9AB9jkHy0jk0JldLN0qpqBQFid62pz/au1pLN0qpqOT8RN91KPg6kCqr9IxeKRWVnJ/oXW7oPY7DA8u1Hb1SKio5P9ED9DmaHoF0PMXZkY5EKaVaXJQketuevl/RsggHopRSLS86En23wykTH0PKNNErpaJPdCR6TwwZ8cM5PLAi0pEopVSLCyvRi8hEEVktIutE5P4a1t8gIstEZLGIzBeRoVXWPRB63WoRObUpg6+PzE5jGMRmSjd+F6kQGmbnetizLdJRKKXasIMmehFxA88ApwFDgYurJvKQt4wxI4wxI4EngKdCrx0KXAQMAyYC/wy9X4sLjv0FGSaRwNvXQkleJEJomDfOh+lXRjoKpVQbFs4Z/VhgnTFmgzGmDJgKTKq6gTFmT5WncUDlLNyTgKnGmFJjzEZgXej9WtxRQ/vxsPdOfEWZMOueSIRQf7mbIXcjpP8AmUsjHY1Sqo0KJ9H3ALZWeZ4eWrYfEblZRNZjz+hvq89rW4LbJRx6xIn8reJ8WDYdlkyLRBj1s/kbey8uWPhyZGNRSrVZTXYx1hjzjDHmEOA+4MH6vFZEpohImoikZWc3X1v380b35O8Vk9jecTTMuht2bWi2fTWJjfMgtguM+DksnQ6l+ZGOSCnVBoWT6DOAXlWe9wwtq81U4Jz6vNYY87wxJtUYk5qUlBRGSA0zKCWBoT068gC32LPkGddDoBVPRrJpvu0DMOY6KCuAZe9EOiKlVBsUTqJfAAwQkX4iEoO9uDqz6gYiMqDK0zOAtaHHM4GLRMQnIv2AAcAPjQ+74c4f3ZM52/1sG/9HyEiDr/4YyXBql7sZ8rZA3/HQMxWSR0DaS2DMwV+rlFJVHDTRG2MqgFuA2cBKYLoxZrmIPCIiZ4c2u0VElovIYuAu4MrQa5cD04EVwCfAzcaYQDN8jrCdfXh3PC7h1T2jYNRlMO9Je+bc2myaZ+/7jbfj6qdeBduXQsaiiIallGp7xLSyM8TU1FSTlpbWrPu47tU0lqbv5tu7xuJ54XioKIEb5kNs52bdb728dwOs/RTuXW8TfckeeHIwDDsXznkm0tEppVoZEVlojEmtaV109Iyt5oIjerAjv5RvtpbCBf+Ggh3wwW2tpyxijP2V0fdYm+QB/O3hsAvhpxlQvDuy8Sml2pSoTPQnDO5Kh3ZeZixMh+6j4Ge/gZUfwKJXIx2albsJ8rba+nxVqddARTEsbYGmoaUFsGoWlBU2/76UUs0qKhO9z+Pm7MO7M3v5dvJLyuGoW6H/8fDJA5CzLtLh7btmUD3RdzscehzR/BdlgwF4+yqYegk8OcQel53rm29/SqlmFZWJHuD8I3pSWhHko2WZ4HLBOc+CxwfvtoIml5vmQ2wiJA06cF3qNZC9CrY045g9n/0W1n0G4++BASfBD8/D30fD6+fB6o/tF4FSqs2I2kR/eM8O9E+KY8bCULP+9t3hrL/CtkWRbXJpjG1xU7U+X9Ww88DXwZ7VN4fFb8F3/4CxU2xJ64KX4M7lcPyvYMcK+M9F8LeRMP8veq1AqTYiahO9iHD+6J78sGkXW3YW2YVDJ8HIy2D+U7A5QqNc5m6EPRm2WWVNYmJh5MWw4n0o3Nm0+97yPXxwuy1jnfr4vuUJKXD8fXDHMrjwVejQGz5/CN65pmn3r5RqFlGb6AHOHdUDEXj3x/R9C0/7I3TsA+9Nicwol7XV56s64moIlMHiN5tuv7u3wrRLoUNPuOBlcHsO3MbthWHnwNWzYPzdsGEOFOY0XQxKqWYR1Ym+e8d2HH1IF95dlMHe/gS+BDjvBcjLgI9+2fJBbZwHcV0hcWDt23QdDL2PtgOdBYON32dZIUy9GCpK4eJp4fUnGHYumCCs+rDx+1dKNauoTvQA543qyZZdRaRtzt23sNcYmHAvLJ1q2623lJraz9cm9Ro7KNvGrxu3z2AQ/nsjZC239fikOr5gqkoeDp362mapSqlWLeoT/cThKcTGuHl3Ufr+KybcCz3HwId3Ql56zS9uars2QP42m+gPZujZdmTLxl6UnfuErfef/HsYcHL4rxOBIWfDhq/1oqxSrVzUJ/o4n4fThnfjwyWZlJRXaTbo9sB5z9umhO/d0DJNCivHt6mrPl/J44ORl8Lqj+CHFyB9IZQXh7+vQLkdk/+rx+0F6KNurn+8QydBsBzWzK7/a5VSLaaGK27R5/zRPZixKJ1PV2Rx9uHd963o3B9O+z94/2b49u9w7B3NG8im+RCfDIkDDr4twNjrYfl/4aPQjFnihq5DbMeqbiPtfVyi/aWwa4Pt9LRrvX2cuxlMAHodCWc+dfBSUU26j4aE7rByJhw+uf6vV0q1CE30wLj+XejdOZaH3v+JzrExHDsgcd/KkZfaM9YvH4WBE+2F0OZQn/p8pY694Y6ldriEzCWwbTFkLrbx1tQiJybefnl1O9y2x+9yCAw+0/46aAiXC4acZYeOKC0AX3zD3kcp1aw00QMul/DaNWOZ8noaV7z0PQ+cNoTrxvdDRGzSPfMvsO4LmPdnOP/F5gli53rIzwyvPl+ViE34HXvbpAv2S2PPNpv8i3Ntcu9yCMQlNezMvS5DzoIf/gXrPrdNL5VSrU7U1+gr9U2M472bjmHi8BQe+2glt01dTFFZhV0Z1wXGXGtb4DTXmC976/MTGv9eItChBww+HUZdCn2OgviuTZ/kAfocbYdrWDnz4NsqpSJCE30VcT4Pz1wymvsmDubDpds475/f7us1e9Qt4I6BeU81z843zYf4FHvm3Za43PYLZc1sKC+JdDRKqRpooq9GRLjx+EN45eqxZOaVcNY/5vP1mmxISIYjrrJt63M3N+1ODza+TWs3ZJKd03bDV5GORClVA030tThuYBIf3HIs3Tr4uerlH3hmzjrKx90CCHzz16bd2c51UJBV+/g2rV2/CXagNS3fKNUqaaKvQ+8usbx709GceVh3/jR7NUc9s4ofE8/ALHrdXuxsKvVpP98aeWJg0ETbpj/SQzwrpQ6gif4gYmM8/O2ikbx0VSqje3fijvQTCAQqmPWvX/HuonSKy5qgI9XGeZDQzbaOaauGnG1b+LTGidZVZBjTeqbnjHKa6MMgIpw4OJnnr0jl7QcuYl3KGfyscBaPTZ/L2Mc+58H/LmNl5p6Gvfne9vPj22Z9vtIhJ4I3tmXGvmmu6Q03zoVPftU0A8VFu/ISeGmiHUJERZwm+nrqmuBn8IUP4aOcD1KXcPLQZN5OS+e0v87j+tfS+CmjnkMb56yBwh31bz/f2sTE2rFyVn3YvIly9cfwhx7w+cNNWyYqyYMZ18H/noE1Hzfd+0arzx+Grf+DH1+H/O2RjibqaYephkgcgAw/j+5r3uCpO+7jobOG8ep3m3hx3gbOXJHFSUOSueOkAQzv0cGOkfPTjFBCz4HCbCjaue9xSWhAsLae6MGWb1a8D1u/t233m8OPb4DLA/Ofhs3fwvn/ho69Gv++Xz5q/z3ikmDekzDo9Mj+wgoGIGet7emcuQQQOPHXEBMXuZjCtfZz+P5Z2+t61Yc22U+4N9JRRTUxrayGlpqaatLS0iIdxsFlLYdnj4bj7ocTHgBgT0k5r3xjE/6ekgouPrSCB8v/TlzWAhCXHW0yNtGOPxPbJXSfaIdVGHZuhD9QEyjZA386BMZcDxP/0PTvX1pg33/0ldBrrJ0Ny+Wx8/0OPr3h75uxCF440U6fmDQIZt0FV8yE/sc1XewHs2uDndUsc4lN7tuXQXmoD4enHQRKbXnvkmngbddycdVXwQ77/yIuCa6fA2/9HHZthNsX2z4XqtmIyEJjTGqN6zTRN8LUS22LmTt+An/7vYv3FJexcMaTHLnuaSqMm+lJtzLunBsZ3rNTBINtIW9NhqwVdgyepj4jXvYOzLgWrv7Y9sjduR7evgq2L4VxN8FJv7MtgOojGIAXToD8LLjlB3D74K+H2cHhrni/fu9lTMM+88a58Pq5EKwAbxx0O2zfoHTdR0KXAbDsbTtvwICTYfIbDR+fqDkZYxP7hq9hyleQPBSWv2f/jS55GwaeEuEAna2uRK81+saYcI+t7S54Yd+yPdtoP+NiTlj3R2L6HsW7R07n7ztTOfMf33LLW4vYmNNMFxJbiyFnQd4We1ba1Ja/Z3sP9xpnn3c5BK77HMb+Av73T3jpFHtmXB8LXrRn0RMfB38H8PrtkM0bvoKMheG/z7ov4KmhsP7L+u0/Lx3evtq2uLrpe3hgK1zziZ3ScuTF9gvH7bGPz3wa1n5q5+ptjc1Yf3jexnfqYzbJAww6w86YtvDlyMYW5TTRN0b3UXDoyfDdM7YlyNK34Z/jbO349D/jufK/XHX6eOb+8gRuOeFQvli5g5Oe+poH3l3G9jyHDhcw6HQ7XPKKJu48VVpgB04bOsmOmlnJ44PTn4DJb9ok/6/j7NDN4dizDb74PRzys/1LZ6nX2KQf7nAXxbl2KOv8bTDtctj2Y3ivKy+x21eU2vi7Dq67vJF6NZz2hK17v3s9BCrC209LyFoOn/7GjvA65rp9yz0xMOoyWPNJy03gow6gib6xJtxrL64+fzy8ex0kDoIb5tux4kM/4zu083LPqYP4+pfHc9mRvXln4VaO+9McHv94JbuLyiIbf1OL7Wx7+K6c2bRtqNd8AhUltY+QOeRMe9yTBsHbV8Jnvz14IvzkATtxyhl/3r/k4kuw9fpVH8KOVQeP7ZMHbG36kunQrjO8eWF4vyw+/iVsWwTnPhv+FI5H/sLOBrb8Pfvl0hqagpYXwzvXQruOMOmZA8tXR1xp/xYWvR6Z+JQm+kbrfST0P95ecPrZQ/Zndy0Dk3VN8PO7ScP58u7jOWNEN56fu4HxT8zhyU9XM2fVDtJzi2ht10waZMhZdliH+U/bMfKb4syzetmmJh17w1UfQeq1dpiKN8+Hol01b7v2M1jxX1t+q6mj2pE32n4B3/yl7rhWfQRL/mPfZ+CpcPm7tu7/+nk2+ddm4St2HP/xd+8bXjpcx9wGJzxox1368PbIJ/vPfgvZK+1F8bjEA9d36guH/sx+3pb4FZK5FD57CDZ9ox22QvRibFMoyYPSfOjQs14vW7V9D3+evYbPV2btXRYX4+bQ5AQGdo1nYHICA5LjGd2nE+393qaOuvkU7oRXz4Idy+3zmHg7/27vcfbWI7V+k5RUbW1z+hPhvWbRazDrbkhIsWWRboftW1deDM8cacs+N8yv/cLmx/fbuvNtP0KnPgeuL9pl3yc+Ga7/ct+F4PQ0+/kTB8JVH9pfCFWlL4SXJ9omtZe+0/DWKF88YpuCjp1iSzoNvfgdKAd3A/++Vn8C/5kM426uu4YULo4AABRpSURBVKXVqlkw9RK46C0YfEbD9hWOZe/A+7dARWhazcRBtuR1+EXQztmNIbTVTSu3u6iMNVkFrMnKZ21WPmt3FLAmq4CcglIA4n0eLh3Xm2uP7UfXBH+Eo62HvHTY8r99t6yfAGNr+INOg5+/Fl6Sq97aJlzpC2HaZbaGfvbf4bAL7fIvfm8nkbnyw7oHkstLh7+OtKOWnvHnA9e/c63tNzBlDqSM2H/dmtnwn4ttE82Lp+37EijIhuePs597yte21NVQxsCnD8J3/4Ajb4BTH9//+sXBlOTZlmNbvrPTQvY52n759Dpyv1ZkBwiU21+w2atsz9eEbnD9F3W3BApUwF9G2Iu0l80IP8ZwBQO2k9a3f4PeR8G5/7I9ztNegow08Phh+PlwxNXQM7Vt90KvhSb6Niq3sIyVmXv4z4KtzFq6DY/bxYVH9OQXEw6hd5fYSIdXfyV5sHWB7Xm64EWbfEdfcfDXTb3UniXftbJ+iQxs+eTtq2DzN3ZOgZGX2Au2Iy6Ac587+Ovfv9l+0dyxzE7eUmnF+zD9CltCOa6WzkA/vmFfP+LnNvGYILx+DqQvgGs/tc0nG8sYmP0r2+po+AVwzj/Da3pZkA1vnAc7VsCoy+3F1G2LbBNPcUHKYfuSfnkRZK+2nf5y1tjrD8FQCcbfAa79zF4bOZg5f4Cvn7Bt6jv1bdTH3k/RLnsisP5LeyH41Mf3b2abuQTSXrZNVMsKIHmEnf95+PmOSviNTvQiMhH4K+AGXjTG/LHa+ruA64AKIBu4xhizObQuACwLbbrFGHN2XfvSRF+zTTmF/GvuBmYsTCdgDGce1o0bjz+EwSl1nHm1VsbAS6dC7ia4deGBpY2qSvPhT4fWr2xTXaAcZv/aTnno8dvbrQtrridXl7MW/jEGjr0TTnrILivIhn8eCR162eaddZU95v4Zvvw9HH2rff7t323SP/yihn2Wmhhjr4d88Tvod5xtZ1/XGfnurfYLJy/DbjvgJLu8rNB+CW36xrYcS19gO2qB7ZjWub8tRyUOtIm98j7c3rp56fas/pg79h3LxspaYUtCeelwxpP2wm9tSvNtsv/hRVtWHHYunPFUw35VBQN2n7kb7d/xro32cV6GbbU16tIGf6SGalSiFxE3sAY4GUgHFgAXG2NWVNnmBOB7Y0yRiNwIHG+MmRxaV2CMCbsgq4m+bll7Svj3/I28+b/NFJYFOH5QEqN6daJ7Rz89OrWjR8d2pHTw4/O08l6I6Wnw4s9sq6UTH6x9u4aWbWqy+D+2pctpT9h26eGafgWsnwN3/gS+9vb5mk9s6aWyvXhtjLH7/OF5+3zsFDj9Tw3/DHX58U2YeSskD7O1/4TkA7fJWQuvnWOT3iXT6h6qorzE9tD1d4DO/Rpex6/qrYts/4Q7l9e/c1t1K2bCezfYE4XJr9ve0uEIBuxF9jl/sG38z33WNqg4mLwM+8tp9cewe4ttsVXJ5d13HWfXRnttprF/r/XU2ER/FPCwMebU0PMHAIwxj9ey/SjgH8aYY0LPNdE3g91FZbz23WamLdhKxu7i/daJQFK8j+4d25GU4CPe5yE2xh26eYjz2ft4n4dx/buQ0iFCdf8Z19nRLm9Jq328msaUbWoSDNb/fbYttnX1n/3Wlhzeuca2sBp/V5j7DMAHt9nxjX7+euMTXF3Wfma/iOKS4PL39m8BlrnEtgYCu67qBeqWsuZTeOtCuPCV2of9yN9urz1s+MpeyPcl2C9YX8K+W3mRbe3UI9X+Kmnfrf6xbPsRZlwPO9fast6Jv7Ed5qrLWWe/GJZMteW3AafYPg+d+kKnfvZLsH0Pe92lJA+eP8GWiH4x1zYGaCGNTfQXABONMdeFnl8OHGmMuaWW7f8BbDfGPBp6XgEsxpZ1/miMqbM3iyb6+iutCLA9r4SM3cVk5BazbXcJ23YXk7G7mJyCUorKAhSVVVBYGqC4fP/x8z0u4eyR3bl+fH+GdGvhMtDurfCPVDsY2vkvHLi+Kco2TeX182xvXxO0JYxrPrU9Vluj9DTbll9ccOnb0GO0LcW8NdmenV/+X0g8NDKxBQPw18PtMbxy5oHrFvzblroqSu0XgQnYv4PSfCjds+9xWREc9nP766gxw0GUFdnmoQtegK7D7N9h8jC7bttimP+U/eXg8dmOX0ffevDrC1kr7K/VbiPtZ2yKX0JhqCvRN+lfqohcBqQCVUeD6mOMyRCR/sCXIrLMGLO+2uumAFMAevfu3ZQhRQWfx02fLnH06XLwWmkgaCguD1BUWsHOwjKmp21l2oKtvLsog/EDEpkyoT/HHpqItMRFqo697HAD8560rUZ6HrH/+jWz6+4k1ZLG3wWvnGHHwjnnudab5MG2Krn2M3jjXHjlTHvhcd6T9prCFf+tdzPgJuVy2zr6l4/asYoqf3Fs+xE+uMN+mfY/wdbba+mP0qRiYm2LqgGn2Avnzx9vryFkpNmLu7729viNu2n/i/F1SR4KZ/3NdqD8/GE7JESENVnpRkROAv4OHGeMqbGniIi8AnxojHmntv3pGX3Lyysq580fNvPyN5vIzi9lcEoCUyb058zDuhPjaeY+daX58LdR0PkQ29ms6hdMLWWbYNDgcrVwa4nKpozdRu5rptna5W+HNy6ArGW2hc9l74Z3Abol4np6GIy70V6j+fIxe0Ydl2THHBp2XmRawxTmwMzbYPUsG8u4m2DMtfZXUEN8dK+9NlNXmaoJNbZ048FejP0ZkIG9GHuJMWZ5lW1GAe9gSzxrqyzvBBQZY0pFJBH4DphU9UJudZroI6e0IsD7i7fxwtwNrN1RQILf1vFdIoiASwRX6F4E4nweEuN9JMbHhO59JCbY513ifLhdEAhC0BgCQUPQGILG/qroHBdDv8TQL5CFr9ghhy98dd/Ze7WyjTGGr9Zk8+K8Dfxvwy5OGJTEJUf25riBXXG3dNJvS0ryYOl0W+ZoaMJqDtMuh41f2yGYC7LskCEnPhj5GI2BHStt3b2xw0FXlNlfgTtW2A514TRBbYSmaF55OvAXbPPKl4wxj4nII0CaMWamiHwOjAAyQy/ZYow5W0SOBv4FBLHDLfzFGPPvuvaliT7yKpPqZyuyqAgECRqbrI3ZP2kXlAbIyS8lp6CUnYVlBIL165MxKDmBMw7rxpnDu9J/xmn2AtYtC2w9NNTapvTyWfx3V29enLeRtTsKSGnv54TBSXy2Ygc5BaX06NiOyWN6MXlML5Lbt6HOZNFuw9fw2tn2l8aZT0OPIw7+mrYoLwP+NcHOP3H9l/XrEV5P2mFKNbtg0LC7uJycglJy8m3iB3v273bZeXfdIrhcdtmmnEJmLctkwaZcAC5J3MAfCh4k95jf0unkuyl78xLKNn/PCYFnyS4sZ0i39kyZ0I8zRthyUnkgyGcrsnjr+y3MX5eD2yWcNKQrlxzZh/GHJrZ8aUfVX/ZqW7Jrzdc7msKGr22/haGT4IKXm60spYletVqZecV8tGw7Hy7dxi2Zv2aMaxW3dfg7z+XdyH8CJ/L1Ifdw/fj+HH1Il1ovEG/KKeQ/P2zh7YXp7CosI8HvYVByAgNTEhiUnMCglAQGJifQOW7/Zo3GGHYXlZNdUMqOPaVkF5QgCBMGJh2wrVKNMu8p26Ht1MfhqJuaZRea6FWbsH39Yrq+cSLbXSl0D2SwddIMeo06KezXl1YE+HR5Ft9v3Mma7QWs2r6HPSX7RktMSvDRPzGOkoog2XtKyC4opTxw4N+/S2Bsv85MHJbCKcNS6N7x4LXaikCQXYVldI6LwePWQWFVNcbYxgVrPrGtjsbfAx16NOkuNNGrtmPWPbYFRnxKoztJGWPYkV/K6u35rMnKZ/X2fDbmFBLr85AU76Nrex9J8T6SEnx0TbD3haUBPl2xndnLt7MmqwCAw3t24JRhKZw6LIU4n5uN2YVs3Flo73PsbcuuIiqCBrdLSGlveyn37NSOnp1iQ/ft8Hnc5BSUkp1vb5WPcwpK2VVYRtf2fvtLJDmeAck1/wpRbVjJHvj8ITuyqrjtUAnH3llzD+YG0ESv2o7CnbYT1ahL4ZRHIxrKhuwCZi/P4pPl21mydfcB6/1eF327xNEv0d5SOvjJyS8lPbc4dCsic09JrUOid46LITE+hqQEHx1jY8jcXczarALyS/f9CkmM9zEwOZ7+SXF0jvPROdZLp7gYOsfF0Ck2xj6OjcHvddW770MwaAgYE7p2otc0WkzuJvj6T7ZnrzsGjpxi2+43ZiRTNNGrtqY413Z9b6EeheHIzCvmi5W2e8jexN7ef9AEWVYRZHteCVtziyirCJKUYJuhdomPwVtDiccYw/Y9Jazens/a0NDVa3YUsHlnIbuL6p4n1iXgdsneC99ul+xdFgja1lIVoRZTFUGz9wvI4xK6d2xHr87t6BX6BdKrc+zeXyTGQEFpOfkltnd15eOC0goCQUPX9n66d/DTrWM7khN8WroKV846+PqPtoVZTLztV3DUzXamrgbQRK+UA1QEguQVl5NbVE5uURm7CsvILSxjV1EZJeXBvWfoQWPs41AfhqAxuETwuGzyd7sqH7twu6CwLEB6bjFbdxWRnltETkHDp7d0CSS399MtlPjbed2UVgQpLQ9QWhGkrCJIaYV9XBEwdImPoXvHdnu/KKo+jvc1fWscYwwZu4sJBqF7R3/r+FLasRK+etwOfZ00GG76X4Na5rTYEAhKqebjcbvoEu+jS3wjxnYJQ3FZgPTcIlt+2l2MW4R4v4cEn4c4n+1El+C3j90ibN9Twra8YjJ3l5CZZ8dayswrZnlGHuUBg8/jIsbjwud143O7iPN56BznwiVCdkEp89fmsCO/hOrdMNp53cT53LSLcRMX46l276ZznI+UDj6S2/tJbu8npb2flA5+/F47cmt+STlrsvJZmZnPqu17WJVpr9NUlsa8bqFXp1j6dImlb2IcfbvE0Tcxjt6dY+vsES6A1+0ixu3C4xa8bhdet+wtnQWChj3F9ss4t6ic3VXu80sq8LiEmNAx8bpdxHji8Q18nM7driLJ7GJwMzS/1ESvlNpPuxg3A5ITGJBcxzwBVXSI9TIoJbxta1MeCJK1p4TMPDsgX2ZeCTn5pRSVByguC1BYWkFxub2vHKiv8v6AeNp5iY1xk5lXsndZgt/DkJT2nDu6B4NSEvC6XGzaWWhvOUV8v3FXje9VH1634HG5KKkINHiq2pG9uvHfCY0Ko0aa6JVSEed1u0ItlMKfOc0YQ35pBVl5JWTtKWX7nhKyQreCkgoO6RrP4JQEBndrT/cO/jovVhtjyC4oZVNOEVt3FdXZyztoDOVBQ3lFkPKAvZUFjH1cESQ2xk3H2Bg6xXntfWwMnWLt4wSfh4Cx25aFSlllocflAYPH3TwXxTXRK6XaJBGhvd9Le7837F8fdb1X1wQ/XRP8jO3XuNYvB+PClntiW7DlbCu4EqGUUqo5aaJXSimH00SvlFIOp4leKaUcThO9Uko5nCZ6pZRyOE30SinlcJrolVLK4VrdoGYikg1sbsRbJAI5TRROU9K46kfjqh+Nq36cGFcfY0xSTStaXaJvLBFJq20Et0jSuOpH46ofjat+oi0uLd0opZTDaaJXSimHc2Kifz7SAdRC46ofjat+NK76iaq4HFejV0optT8nntErpZSqQhO9Uko5nGMSvYhMFJHVIrJORO6PdDyVRGSTiCwTkcUiEtFZz0XkJRHZISI/VVnWWUQ+E5G1oftOrSSuh0UkI3TcFovI6S0cUy8RmSMiK0RkuYjcHloe0eNVR1yRPl5+EflBRJaE4vpdaHk/Efk+9P9ymoi04HQbdcb1iohsrHK8RrZkXFXic4vIjyLyYeh58xwvY0ybvwFuYD3QH4gBlgBDIx1XKLZNQGKk4wjFMgEYDfxUZdkTwP2hx/cD/9dK4noYuCeCx6obMDr0OAFYAwyN9PGqI65IHy8B4kOPvcD3wDhgOnBRaPlzwI2tJK5XgAsidbyqxHcX8BbwYeh5sxwvp5zRjwXWGWM2GGPKgKnApAjH1OoYY+YCu6otngS8Gnr8KnBOiwZFrXFFlDEm0xizKPQ4H1gJ9CDCx6uOuCLKWAWhp97QzQAnAu+ElkfieNUWV8SJSE/gDODF0HOhmY6XUxJ9D2BrlefptII//hADfCoiC0VkSqSDqUGyMSYz9Hg7kBzJYKq5RUSWhko7LV5SqiQifYFR2LPBVnO8qsUFET5eoTLEYmAH8Bn2V/ZuY0xFaJOI/L+sHpcxpvJ4PRY6Xk+LiK+l4wL+AvwSCIaed6GZjpdTEn1rdqwxZjRwGnCziEyIdEC1Mfb3Yqs42wGeBQ4BRgKZwJORCEJE4oEZwB3GmD1V10XyeNUQV8SPlzEmYIwZCfTE/soe3NIx1KR6XCIyHHgAG98YoDNwX0vGJCJnAjuMMQtbYn9OSfQZQK8qz3uGlkWcMSYjdL8DeA/7H6A1yRKRbgCh+x0RjgcAY0xW6D9oEHiBCBw3EfFik+mbxph3Q4sjfrxqiqs1HK9KxpjdwBzgKKCjiHhCqyL6/7JKXBNDJTBjjCkFXqblj9cxwNkisglbaj4R+CvNdLyckugXAANCV6xjgIuAmRGOCRGJE5GEysfAKcBPdb+qxc0Ergw9vhJ4P4Kx7FWZTEPOpYWPW6he+m9gpTHmqSqrInq8aourFRyvJBHpGHrcDjgZe/1gDnBBaLNIHK+a4lpV5ctasHXwFj1expgHjDE9jTF9sfnqS2PMpTTX8Yr0VeemugGnY1sgrAd+Hel4QjH1x7YAWgIsj3RcwH+wP+vLsfW/a7F1wS+AtcDnQOdWEtfrwDJgKTa5dmvhmI7FlmWWAotDt9MjfbzqiCvSx+sw4MfQ/n8Cfhta3h/4AVgHvA34WklcX4aO10/AG4Ra5kTiBhzPvlY3zXK8dAgEpZRyOKeUbpRSStVCE71SSjmcJnqllHI4TfRKKeVwmuiVUsrhNNErpZTDaaJXSimH+386MAstN/mSSAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "aT57qZ5W1xHc",
        "outputId": "8121341e-f8c6-45cd-f581-ae7c3750b5d9"
      },
      "source": [
        "history_df.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.9643588662147522\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "kOu5es-F1xE-",
        "outputId": "d2adf997-fb91-40d2-aa8a-aea833d44512"
      },
      "source": [
        "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.901584267616272\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Z1xGJNl1xCc",
        "outputId": "4f2207e3-da6f-42ae-deb7-7d9e1218cb60"
      },
      "source": [
        "preds1 = table1_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.7504779 ],\n",
              "       [0.01452219],\n",
              "       [0.58809745],\n",
              "       ...,\n",
              "       [0.01452219],\n",
              "       [0.01452219],\n",
              "       [0.5692853 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjJXhtRp1w-H",
        "outputId": "7a77fbcb-4053-4c70-a3ac-0a1200de8c43"
      },
      "source": [
        "len(preds1[preds1 < 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lH9o4roL1-mQ",
        "outputId": "c6bed9a8-0aa9-4b9d-d2ad-0d1bf7e5b618"
      },
      "source": [
        "len(preds1[preds1 >= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8573"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmjS9QFa2Amb",
        "outputId": "cd3be0f8-f51e-4080-d6ee-c30d73285332"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "fMSXQfooetvq",
        "outputId": "19e0d5d4-1f90-482b-d0ad-46b85bfcebf8"
      },
      "source": [
        "preds_df = pd.DataFrame(preds1, columns=['preds'])\n",
        "\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.750478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.014522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.588097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.014522</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.532912</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      preds\n",
              "0  0.750478\n",
              "1  0.014522\n",
              "2  0.588097\n",
              "3  0.014522\n",
              "4  0.532912"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "DGXsB-RUfDoh",
        "outputId": "308b0f54-d0dd-4652-9b67-f5f7659e2a53"
      },
      "source": [
        "preds_df = pd.concat([preds_df, val_y.reset_index(drop=True), val_X.reset_index()], axis=1)\n",
        "\n",
        "preds_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>phishing</th>\n",
              "      <th>index</th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_underline_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>qty_questionmark_url</th>\n",
              "      <th>qty_equal_url</th>\n",
              "      <th>qty_at_url</th>\n",
              "      <th>qty_and_url</th>\n",
              "      <th>qty_exclamation_url</th>\n",
              "      <th>qty_space_url</th>\n",
              "      <th>qty_tilde_url</th>\n",
              "      <th>qty_comma_url</th>\n",
              "      <th>qty_plus_url</th>\n",
              "      <th>qty_asterisk_url</th>\n",
              "      <th>qty_hashtag_url</th>\n",
              "      <th>qty_dollar_url</th>\n",
              "      <th>qty_percent_url</th>\n",
              "      <th>qty_tld_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>email_in_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.750478</td>\n",
              "      <td>1</td>\n",
              "      <td>62575</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>51</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.014522</td>\n",
              "      <td>0</td>\n",
              "      <td>38126</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.588097</td>\n",
              "      <td>0</td>\n",
              "      <td>1617</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>34</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.014522</td>\n",
              "      <td>0</td>\n",
              "      <td>8228</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.532912</td>\n",
              "      <td>1</td>\n",
              "      <td>55594</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22157</th>\n",
              "      <td>0.014522</td>\n",
              "      <td>0</td>\n",
              "      <td>65294</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22158</th>\n",
              "      <td>0.014522</td>\n",
              "      <td>0</td>\n",
              "      <td>10038</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22159</th>\n",
              "      <td>0.014522</td>\n",
              "      <td>0</td>\n",
              "      <td>43642</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22160</th>\n",
              "      <td>0.014522</td>\n",
              "      <td>0</td>\n",
              "      <td>73632</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22161</th>\n",
              "      <td>0.569285</td>\n",
              "      <td>1</td>\n",
              "      <td>25895</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22162 rows × 23 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          preds  phishing  index  ...  qty_tld_url  length_url  email_in_url\n",
              "0      0.750478         1  62575  ...            1          51             0\n",
              "1      0.014522         0  38126  ...            1          26             0\n",
              "2      0.588097         0   1617  ...            1          34             0\n",
              "3      0.014522         0   8228  ...            1          14             0\n",
              "4      0.532912         1  55594  ...            1          28             0\n",
              "...         ...       ...    ...  ...          ...         ...           ...\n",
              "22157  0.014522         0  65294  ...            1          17             0\n",
              "22158  0.014522         0  10038  ...            1          16             0\n",
              "22159  0.014522         0  43642  ...            1          23             0\n",
              "22160  0.014522         0  73632  ...            1          26             0\n",
              "22161  0.569285         1  25895  ...            1          32             0\n",
              "\n",
              "[22162 rows x 23 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWW3TtqRoQPH"
      },
      "source": [
        "# NN on URL attributes (table 1) [model as function]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Ml38x5Q8oQPH",
        "outputId": "ab09e8f9-b106-4557-a7d1-02debdc05151"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table1 = ['qty_dot_url', 'qty_hyphen_url', 'qty_underline_url', 'qty_slash_url', 'qty_questionmark_url', 'qty_equal_url',\n",
        "                   'qty_at_url', 'qty_and_url', 'qty_exclamation_url', 'qty_space_url', 'qty_tilde_url', 'qty_comma_url',\n",
        "                   'qty_plus_url', 'qty_asterisk_url', 'qty_hashtag_url', 'qty_dollar_url', 'qty_percent_url', 'qty_tld_url',\n",
        "                   'length_url', 'email_in_url'] \n",
        "\n",
        "X = full_df[features_table1]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_underline_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>qty_questionmark_url</th>\n",
              "      <th>qty_equal_url</th>\n",
              "      <th>qty_at_url</th>\n",
              "      <th>qty_and_url</th>\n",
              "      <th>qty_exclamation_url</th>\n",
              "      <th>qty_space_url</th>\n",
              "      <th>qty_tilde_url</th>\n",
              "      <th>qty_comma_url</th>\n",
              "      <th>qty_plus_url</th>\n",
              "      <th>qty_asterisk_url</th>\n",
              "      <th>qty_hashtag_url</th>\n",
              "      <th>qty_dollar_url</th>\n",
              "      <th>qty_percent_url</th>\n",
              "      <th>qty_tld_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>email_in_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.089803</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.089803</td>\n",
              "      <td>0.987829</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.224231</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.074744</td>\n",
              "      <td>0.971666</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.123797</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.061898</td>\n",
              "      <td>0.990375</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.076640</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.038320</td>\n",
              "      <td>0.996322</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.071202</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035601</td>\n",
              "      <td>0.996826</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_url  qty_hyphen_url  ...  length_url  email_in_url\n",
              "5676      0.089803             0.0  ...    0.987829           0.0\n",
              "39002     0.224231             0.0  ...    0.971666           0.0\n",
              "1732      0.123797             0.0  ...    0.990375           0.0\n",
              "39668     0.076640             0.0  ...    0.996322           0.0\n",
              "82035     0.071202             0.0  ...    0.996826           0.0\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7aJEbp4oQPI"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_1():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[20]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEUVFSMLoQPI"
      },
      "source": [
        "mod1 = KerasClassifier(build_fn=phish_nn_1,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUJiDk-9oQPI",
        "outputId": "2b85c813-4cf9-494c-d6b4-dc74a2c9f2fc"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_1=cross_val_score(mod1,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5891 - accuracy: 0.6881 - val_loss: 0.3395 - val_accuracy: 0.8876\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3511 - accuracy: 0.8832 - val_loss: 0.3480 - val_accuracy: 0.8889\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3282 - accuracy: 0.8859 - val_loss: 0.3593 - val_accuracy: 0.8797\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8870 - val_loss: 0.3584 - val_accuracy: 0.8797\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3021 - accuracy: 0.8896 - val_loss: 0.4257 - val_accuracy: 0.8756\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8881 - val_loss: 0.4323 - val_accuracy: 0.8748\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2888 - accuracy: 0.8906 - val_loss: 0.3668 - val_accuracy: 0.8824\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8898 - val_loss: 0.3478 - val_accuracy: 0.8798\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2728 - accuracy: 0.8926 - val_loss: 0.3643 - val_accuracy: 0.8932\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8911 - val_loss: 0.3348 - val_accuracy: 0.8907\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8896 - val_loss: 0.3122 - val_accuracy: 0.8866\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2696 - accuracy: 0.8904 - val_loss: 0.3800 - val_accuracy: 0.8790\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2696 - accuracy: 0.8904 - val_loss: 0.3948 - val_accuracy: 0.8790\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2656 - accuracy: 0.8910 - val_loss: 0.3001 - val_accuracy: 0.8835\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8882 - val_loss: 0.2830 - val_accuracy: 0.8917\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8933 - val_loss: 0.2855 - val_accuracy: 0.8920\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8906 - val_loss: 0.3043 - val_accuracy: 0.8873\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.8891 - val_loss: 0.3130 - val_accuracy: 0.8942\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2683 - accuracy: 0.8890 - val_loss: 0.3649 - val_accuracy: 0.8940\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8907 - val_loss: 0.2790 - val_accuracy: 0.8930\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2605 - accuracy: 0.8901 - val_loss: 0.3249 - val_accuracy: 0.8950\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8895 - val_loss: 0.2814 - val_accuracy: 0.8986\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2588 - accuracy: 0.8918 - val_loss: 0.2887 - val_accuracy: 0.8948\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8927 - val_loss: 0.3816 - val_accuracy: 0.8775\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8903 - val_loss: 0.2964 - val_accuracy: 0.8900\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8907 - val_loss: 0.3339 - val_accuracy: 0.8828\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2563 - accuracy: 0.8917 - val_loss: 0.2908 - val_accuracy: 0.8871\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2590 - accuracy: 0.8924 - val_loss: 0.2937 - val_accuracy: 0.8913\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2565 - accuracy: 0.8896 - val_loss: 0.2708 - val_accuracy: 0.8839\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2605 - accuracy: 0.8896 - val_loss: 0.2767 - val_accuracy: 0.8860\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2518 - accuracy: 0.8937 - val_loss: 0.3132 - val_accuracy: 0.8862\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8920 - val_loss: 0.2825 - val_accuracy: 0.8884\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8902 - val_loss: 0.2852 - val_accuracy: 0.8912\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2600 - accuracy: 0.8899 - val_loss: 0.2862 - val_accuracy: 0.8836\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.8920 - val_loss: 0.2663 - val_accuracy: 0.8886\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2610 - accuracy: 0.8912 - val_loss: 0.2782 - val_accuracy: 0.8840\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2594 - accuracy: 0.8900 - val_loss: 0.2790 - val_accuracy: 0.8881\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2836 - accuracy: 0.8943\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2619 - accuracy: 0.8905\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2613 - accuracy: 0.8913\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2613 - accuracy: 0.8913\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2591 - accuracy: 0.8915\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2584 - accuracy: 0.8913\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2587 - accuracy: 0.8920\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2587 - accuracy: 0.8919\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2588 - accuracy: 0.8920\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2571 - accuracy: 0.8935\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2587 - accuracy: 0.8915\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2504 - accuracy: 0.8915\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.5914 - accuracy: 0.6684 - val_loss: 0.3386 - val_accuracy: 0.8851\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3656 - accuracy: 0.8767 - val_loss: 0.3559 - val_accuracy: 0.8756\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3329 - accuracy: 0.8842 - val_loss: 0.3375 - val_accuracy: 0.8890\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3175 - accuracy: 0.8874 - val_loss: 0.3342 - val_accuracy: 0.8910\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3101 - accuracy: 0.8852 - val_loss: 0.4228 - val_accuracy: 0.8906\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2952 - accuracy: 0.8861 - val_loss: 0.3137 - val_accuracy: 0.8823\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2854 - accuracy: 0.8851 - val_loss: 0.2965 - val_accuracy: 0.8960\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2874 - accuracy: 0.8885 - val_loss: 0.3283 - val_accuracy: 0.8879\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2809 - accuracy: 0.8872 - val_loss: 0.3137 - val_accuracy: 0.8939\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2746 - accuracy: 0.8896 - val_loss: 0.3415 - val_accuracy: 0.8870\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2733 - accuracy: 0.8888 - val_loss: 0.3281 - val_accuracy: 0.8841\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2698 - accuracy: 0.8912 - val_loss: 0.2924 - val_accuracy: 0.8948\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2680 - accuracy: 0.8920 - val_loss: 0.3695 - val_accuracy: 0.8966\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2717 - accuracy: 0.8883 - val_loss: 0.2668 - val_accuracy: 0.8864\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8899 - val_loss: 0.2897 - val_accuracy: 0.8985\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8939 - val_loss: 0.3202 - val_accuracy: 0.8944\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2641 - accuracy: 0.8895 - val_loss: 0.3814 - val_accuracy: 0.8762\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2671 - accuracy: 0.8903 - val_loss: 0.2823 - val_accuracy: 0.8915\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2629 - accuracy: 0.8912 - val_loss: 0.3327 - val_accuracy: 0.8838\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8922 - val_loss: 0.2917 - val_accuracy: 0.8776\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2608 - accuracy: 0.8911 - val_loss: 0.2626 - val_accuracy: 0.8982\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8914 - val_loss: 0.2932 - val_accuracy: 0.8900\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2608 - accuracy: 0.8924 - val_loss: 0.2486 - val_accuracy: 0.8948\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8914 - val_loss: 0.2844 - val_accuracy: 0.8914\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2605 - accuracy: 0.8931 - val_loss: 0.3178 - val_accuracy: 0.8880\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2580 - accuracy: 0.8949 - val_loss: 0.2656 - val_accuracy: 0.8956\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8928 - val_loss: 0.2604 - val_accuracy: 0.8915\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2518 - accuracy: 0.8919 - val_loss: 0.2620 - val_accuracy: 0.8996\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8921 - val_loss: 0.2638 - val_accuracy: 0.8933\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8925 - val_loss: 0.2753 - val_accuracy: 0.8895\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2570 - accuracy: 0.8911 - val_loss: 0.3064 - val_accuracy: 0.8812\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2601 - accuracy: 0.8890 - val_loss: 0.2908 - val_accuracy: 0.8943\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2569 - accuracy: 0.8916 - val_loss: 0.2823 - val_accuracy: 0.8886\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2577 - accuracy: 0.8933 - val_loss: 0.2789 - val_accuracy: 0.8917\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8922 - val_loss: 0.3034 - val_accuracy: 0.8934\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8935 - val_loss: 0.2517 - val_accuracy: 0.8931\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8952 - val_loss: 0.2659 - val_accuracy: 0.8919\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8913 - val_loss: 0.2650 - val_accuracy: 0.8894\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2580 - accuracy: 0.8932 - val_loss: 0.3134 - val_accuracy: 0.8825\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2546 - accuracy: 0.8941 - val_loss: 0.2715 - val_accuracy: 0.8905\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2504 - accuracy: 0.8940 - val_loss: 0.2485 - val_accuracy: 0.8963\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2550 - accuracy: 0.8952 - val_loss: 0.2554 - val_accuracy: 0.8936\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8940 - val_loss: 0.2806 - val_accuracy: 0.8927\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2647 - accuracy: 0.8947\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2575 - accuracy: 0.8924\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2572 - accuracy: 0.8925\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2556 - accuracy: 0.8941\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2550 - accuracy: 0.8930\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2557 - accuracy: 0.8930\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2552 - accuracy: 0.8935\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2546 - accuracy: 0.8934\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2537 - accuracy: 0.8931\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2518 - accuracy: 0.8939\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2503 - accuracy: 0.8940\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2484 - accuracy: 0.8926\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.5940 - accuracy: 0.6730 - val_loss: 0.3382 - val_accuracy: 0.8836\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3595 - accuracy: 0.8807 - val_loss: 0.3124 - val_accuracy: 0.8888\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3303 - accuracy: 0.8849 - val_loss: 0.3129 - val_accuracy: 0.8913\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3116 - accuracy: 0.8904 - val_loss: 0.3049 - val_accuracy: 0.8908\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3046 - accuracy: 0.8867 - val_loss: 0.3229 - val_accuracy: 0.8966\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2927 - accuracy: 0.8897 - val_loss: 0.2998 - val_accuracy: 0.8932\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2878 - accuracy: 0.8922 - val_loss: 0.3144 - val_accuracy: 0.8857\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8892 - val_loss: 0.3643 - val_accuracy: 0.8890\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2775 - accuracy: 0.8897 - val_loss: 0.3261 - val_accuracy: 0.8846\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2763 - accuracy: 0.8881 - val_loss: 0.3046 - val_accuracy: 0.8944\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2765 - accuracy: 0.8864 - val_loss: 0.3462 - val_accuracy: 0.8879\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2716 - accuracy: 0.8899 - val_loss: 0.3266 - val_accuracy: 0.8972\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2685 - accuracy: 0.8907 - val_loss: 0.3732 - val_accuracy: 0.8927\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8907 - val_loss: 0.3130 - val_accuracy: 0.8996\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2654 - accuracy: 0.8899 - val_loss: 0.3015 - val_accuracy: 0.8956\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8900 - val_loss: 0.2734 - val_accuracy: 0.8986\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8913 - val_loss: 0.3138 - val_accuracy: 0.8836\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2632 - accuracy: 0.8924 - val_loss: 0.2899 - val_accuracy: 0.9000\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2646 - accuracy: 0.8902 - val_loss: 0.2812 - val_accuracy: 0.8938\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2619 - accuracy: 0.8920 - val_loss: 0.2761 - val_accuracy: 0.8927\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8908 - val_loss: 0.3306 - val_accuracy: 0.8861\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2652 - accuracy: 0.8908 - val_loss: 0.2674 - val_accuracy: 0.8902\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8905 - val_loss: 0.2860 - val_accuracy: 0.8966\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2595 - accuracy: 0.8931 - val_loss: 0.2628 - val_accuracy: 0.8940\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8870 - val_loss: 0.2612 - val_accuracy: 0.8948\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8929 - val_loss: 0.2572 - val_accuracy: 0.8921\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8905 - val_loss: 0.2822 - val_accuracy: 0.8909\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8893 - val_loss: 0.2738 - val_accuracy: 0.8944\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8913 - val_loss: 0.2796 - val_accuracy: 0.8860\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2546 - accuracy: 0.8929 - val_loss: 0.2605 - val_accuracy: 0.8887\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2576 - accuracy: 0.8915 - val_loss: 0.2762 - val_accuracy: 0.8913\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8955 - val_loss: 0.2622 - val_accuracy: 0.8886\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.8919 - val_loss: 0.2594 - val_accuracy: 0.8911\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2930 - accuracy: 0.8955\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2603 - accuracy: 0.8912\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2609 - accuracy: 0.8898\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2587 - accuracy: 0.8909\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2584 - accuracy: 0.8911\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2563 - accuracy: 0.8924\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2556 - accuracy: 0.8916\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2560 - accuracy: 0.8925\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2563 - accuracy: 0.8915\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2555 - accuracy: 0.8918\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2551 - accuracy: 0.8919\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2562 - accuracy: 0.8923\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6000 - accuracy: 0.6817 - val_loss: 0.3395 - val_accuracy: 0.8878\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3627 - accuracy: 0.8810 - val_loss: 0.3278 - val_accuracy: 0.8869\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3349 - accuracy: 0.8853 - val_loss: 0.3214 - val_accuracy: 0.8851\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8850 - val_loss: 0.3474 - val_accuracy: 0.8862\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3033 - accuracy: 0.8867 - val_loss: 0.4070 - val_accuracy: 0.8774\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2951 - accuracy: 0.8891 - val_loss: 0.4168 - val_accuracy: 0.8712\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2862 - accuracy: 0.8892 - val_loss: 0.3347 - val_accuracy: 0.8937\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2879 - accuracy: 0.8890 - val_loss: 0.3702 - val_accuracy: 0.8849\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2820 - accuracy: 0.8870 - val_loss: 0.2878 - val_accuracy: 0.8918\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8911 - val_loss: 0.3583 - val_accuracy: 0.8824\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2794 - accuracy: 0.8839 - val_loss: 0.3640 - val_accuracy: 0.8799\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8897 - val_loss: 0.4849 - val_accuracy: 0.8772\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8895 - val_loss: 0.4587 - val_accuracy: 0.8766\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2742 - accuracy: 0.8836 - val_loss: 0.3226 - val_accuracy: 0.8939\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2721 - accuracy: 0.8874 - val_loss: 0.4071 - val_accuracy: 0.8748\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2686 - accuracy: 0.8878 - val_loss: 0.3206 - val_accuracy: 0.8867\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2642 - accuracy: 0.8913 - val_loss: 0.3355 - val_accuracy: 0.8809\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8891 - val_loss: 0.3950 - val_accuracy: 0.8694\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2658 - accuracy: 0.8896 - val_loss: 0.2914 - val_accuracy: 0.8877\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2641 - accuracy: 0.8880 - val_loss: 0.2994 - val_accuracy: 0.8859\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2585 - accuracy: 0.8915 - val_loss: 0.2982 - val_accuracy: 0.8873\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.8904 - val_loss: 0.2978 - val_accuracy: 0.8869\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2588 - accuracy: 0.8924 - val_loss: 0.3285 - val_accuracy: 0.8819\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2603 - accuracy: 0.8896 - val_loss: 0.2630 - val_accuracy: 0.8899\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.8921 - val_loss: 0.3214 - val_accuracy: 0.8873\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2606 - accuracy: 0.8907 - val_loss: 0.3338 - val_accuracy: 0.8775\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2621 - accuracy: 0.8913 - val_loss: 0.3040 - val_accuracy: 0.8895\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2548 - accuracy: 0.8954 - val_loss: 0.2660 - val_accuracy: 0.8896\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2549 - accuracy: 0.8902 - val_loss: 0.2890 - val_accuracy: 0.8861\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3249 - accuracy: 0.8893\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2656 - accuracy: 0.8906\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2624 - accuracy: 0.8900\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2612 - accuracy: 0.8914\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2609 - accuracy: 0.8913\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2588 - accuracy: 0.8916\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2591 - accuracy: 0.8918\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2576 - accuracy: 0.8926\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2577 - accuracy: 0.8939\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2560 - accuracy: 0.8934\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2569 - accuracy: 0.8929\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2837 - accuracy: 0.8929\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6069 - accuracy: 0.6610 - val_loss: 0.3471 - val_accuracy: 0.8864\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3586 - accuracy: 0.8843 - val_loss: 0.3461 - val_accuracy: 0.8800\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3312 - accuracy: 0.8883 - val_loss: 0.3114 - val_accuracy: 0.8932\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3167 - accuracy: 0.8897 - val_loss: 0.3118 - val_accuracy: 0.8963\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3072 - accuracy: 0.8900 - val_loss: 0.3695 - val_accuracy: 0.8880\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2965 - accuracy: 0.8895 - val_loss: 0.3458 - val_accuracy: 0.8821\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2862 - accuracy: 0.8891 - val_loss: 0.3119 - val_accuracy: 0.8933\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2798 - accuracy: 0.8905 - val_loss: 0.3531 - val_accuracy: 0.8941\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2785 - accuracy: 0.8910 - val_loss: 0.3346 - val_accuracy: 0.8978\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2784 - accuracy: 0.8906 - val_loss: 0.3857 - val_accuracy: 0.8834\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2714 - accuracy: 0.8883 - val_loss: 0.3547 - val_accuracy: 0.8913\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.8905 - val_loss: 0.3660 - val_accuracy: 0.8856\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2681 - accuracy: 0.8903 - val_loss: 0.2810 - val_accuracy: 0.8951\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2706 - accuracy: 0.8890 - val_loss: 0.3001 - val_accuracy: 0.8909\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8872 - val_loss: 0.3084 - val_accuracy: 0.8970\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8917 - val_loss: 0.3006 - val_accuracy: 0.8968\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2610 - accuracy: 0.8942 - val_loss: 0.3013 - val_accuracy: 0.8859\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8920 - val_loss: 0.2995 - val_accuracy: 0.8836\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2577 - accuracy: 0.8931 - val_loss: 0.2937 - val_accuracy: 0.8844\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2625 - accuracy: 0.8907 - val_loss: 0.3079 - val_accuracy: 0.8821\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2574 - accuracy: 0.8918 - val_loss: 0.2830 - val_accuracy: 0.8965\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2615 - accuracy: 0.8919 - val_loss: 0.2815 - val_accuracy: 0.8882\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8893 - val_loss: 0.2677 - val_accuracy: 0.8961\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8928 - val_loss: 0.3304 - val_accuracy: 0.8764\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3373 - accuracy: 0.8928\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2716 - accuracy: 0.8899\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2686 - accuracy: 0.8904\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2655 - accuracy: 0.8916\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2654 - accuracy: 0.8910\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2623 - accuracy: 0.8912\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2598 - accuracy: 0.8921\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2605 - accuracy: 0.8924\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2586 - accuracy: 0.8923\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2575 - accuracy: 0.8927\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2570 - accuracy: 0.8922\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2765 - accuracy: 0.8904\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roPlvwRQoQPI",
        "outputId": "0e1eec6a-8c7a-4415-e681-84870231782e"
      },
      "source": [
        "print(round(cv_results_1.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8919\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljDjpjixoQPI",
        "outputId": "d2e4f352-0d98-45bd-f107-fb693b973a91"
      },
      "source": [
        "print(round(cv_results_1.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0009\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMaHhEwua3xA"
      },
      "source": [
        "cv_results_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gyWb7DzYoQPI",
        "outputId": "6c53613e-0fad-40e5-f0de-34255560027e"
      },
      "source": [
        "cv1_preds = cross_val_predict(mod1, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6106 - accuracy: 0.6690 - val_loss: 0.3452 - val_accuracy: 0.8832\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3634 - accuracy: 0.8828 - val_loss: 0.3454 - val_accuracy: 0.8853\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3374 - accuracy: 0.8863 - val_loss: 0.3257 - val_accuracy: 0.8894\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3158 - accuracy: 0.8870 - val_loss: 0.4184 - val_accuracy: 0.8777\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3023 - accuracy: 0.8913 - val_loss: 0.3898 - val_accuracy: 0.8844\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2892 - accuracy: 0.8909 - val_loss: 0.3412 - val_accuracy: 0.8828\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2859 - accuracy: 0.8894 - val_loss: 0.4007 - val_accuracy: 0.8860\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2803 - accuracy: 0.8892 - val_loss: 0.2862 - val_accuracy: 0.8934\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2844 - accuracy: 0.8874 - val_loss: 0.3355 - val_accuracy: 0.8896\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2739 - accuracy: 0.8890 - val_loss: 0.4190 - val_accuracy: 0.8799\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2758 - accuracy: 0.8873 - val_loss: 0.3356 - val_accuracy: 0.8939\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8883 - val_loss: 0.3044 - val_accuracy: 0.8914\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2714 - accuracy: 0.8900 - val_loss: 0.3770 - val_accuracy: 0.8944\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8904 - val_loss: 0.3129 - val_accuracy: 0.8914\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8891 - val_loss: 0.3150 - val_accuracy: 0.8802\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2685 - accuracy: 0.8887 - val_loss: 0.2803 - val_accuracy: 0.8888\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8907 - val_loss: 0.3004 - val_accuracy: 0.8867\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2658 - accuracy: 0.8894 - val_loss: 0.3638 - val_accuracy: 0.8769\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8878 - val_loss: 0.3633 - val_accuracy: 0.8876\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8882 - val_loss: 0.2929 - val_accuracy: 0.8931\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8929 - val_loss: 0.3061 - val_accuracy: 0.8836\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2556 - accuracy: 0.8922 - val_loss: 0.3097 - val_accuracy: 0.8883\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8909 - val_loss: 0.3132 - val_accuracy: 0.8891\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8908 - val_loss: 0.3033 - val_accuracy: 0.8866\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8899 - val_loss: 0.2776 - val_accuracy: 0.8863\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8907 - val_loss: 0.2813 - val_accuracy: 0.8822\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2642 - accuracy: 0.8884 - val_loss: 0.3360 - val_accuracy: 0.8878\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8905 - val_loss: 0.2693 - val_accuracy: 0.8887\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3789 - accuracy: 0.8905\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2668 - accuracy: 0.8906\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2659 - accuracy: 0.8912\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2640 - accuracy: 0.8905\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2620 - accuracy: 0.8918\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2599 - accuracy: 0.8923\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2596 - accuracy: 0.8912\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2602 - accuracy: 0.8920\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2587 - accuracy: 0.8914\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2583 - accuracy: 0.8927\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2576 - accuracy: 0.8920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 5ms/step - loss: 0.6072 - accuracy: 0.6751 - val_loss: 0.3431 - val_accuracy: 0.8838\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3612 - accuracy: 0.8852 - val_loss: 0.3748 - val_accuracy: 0.8899\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3296 - accuracy: 0.8895 - val_loss: 0.3320 - val_accuracy: 0.8910\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8862 - val_loss: 0.3728 - val_accuracy: 0.8907\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3061 - accuracy: 0.8868 - val_loss: 0.3375 - val_accuracy: 0.8941\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2976 - accuracy: 0.8890 - val_loss: 0.3221 - val_accuracy: 0.8933\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2915 - accuracy: 0.8882 - val_loss: 0.3340 - val_accuracy: 0.8973\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8881 - val_loss: 0.3392 - val_accuracy: 0.8858\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8887 - val_loss: 0.3166 - val_accuracy: 0.8971\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8868 - val_loss: 0.3266 - val_accuracy: 0.8850\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2693 - accuracy: 0.8927 - val_loss: 0.3628 - val_accuracy: 0.8936\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2736 - accuracy: 0.8911 - val_loss: 0.2818 - val_accuracy: 0.8934\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2716 - accuracy: 0.8918 - val_loss: 0.3421 - val_accuracy: 0.8899\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2707 - accuracy: 0.8904 - val_loss: 0.3518 - val_accuracy: 0.8868\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2722 - accuracy: 0.8906 - val_loss: 0.3064 - val_accuracy: 0.8941\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2679 - accuracy: 0.8908 - val_loss: 0.3361 - val_accuracy: 0.8922\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8877 - val_loss: 0.3310 - val_accuracy: 0.8866\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8902 - val_loss: 0.3254 - val_accuracy: 0.8872\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8888 - val_loss: 0.3184 - val_accuracy: 0.8857\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2624 - accuracy: 0.8906 - val_loss: 0.2662 - val_accuracy: 0.8904\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8889 - val_loss: 0.3154 - val_accuracy: 0.8904\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2641 - accuracy: 0.8893 - val_loss: 0.2761 - val_accuracy: 0.8896\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3368 - accuracy: 0.8931\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2780 - accuracy: 0.8911\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2737 - accuracy: 0.8910\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2713 - accuracy: 0.8911\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2696 - accuracy: 0.8914\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2678 - accuracy: 0.8903\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2658 - accuracy: 0.8907\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2652 - accuracy: 0.8911\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2639 - accuracy: 0.8907\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2606 - accuracy: 0.8922\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2612 - accuracy: 0.8926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.5903 - accuracy: 0.6876 - val_loss: 0.3395 - val_accuracy: 0.8857\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3589 - accuracy: 0.8801 - val_loss: 0.3409 - val_accuracy: 0.8804\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3357 - accuracy: 0.8814 - val_loss: 0.3542 - val_accuracy: 0.8828\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3147 - accuracy: 0.8878 - val_loss: 0.3403 - val_accuracy: 0.8780\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3032 - accuracy: 0.8864 - val_loss: 0.3475 - val_accuracy: 0.8851\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8891 - val_loss: 0.4383 - val_accuracy: 0.8672\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2907 - accuracy: 0.8867 - val_loss: 0.3539 - val_accuracy: 0.8735\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2877 - accuracy: 0.8871 - val_loss: 0.3641 - val_accuracy: 0.8831\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2830 - accuracy: 0.8858 - val_loss: 0.3863 - val_accuracy: 0.8778\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8863 - val_loss: 0.3641 - val_accuracy: 0.8830\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2748 - accuracy: 0.8884 - val_loss: 0.3724 - val_accuracy: 0.8717\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2670 - accuracy: 0.8903 - val_loss: 0.3524 - val_accuracy: 0.8820\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8897 - val_loss: 0.3408 - val_accuracy: 0.8874\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2678 - accuracy: 0.8895 - val_loss: 0.3473 - val_accuracy: 0.8851\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8883 - val_loss: 0.3393 - val_accuracy: 0.8884\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8897 - val_loss: 0.3632 - val_accuracy: 0.8834\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2608 - accuracy: 0.8930 - val_loss: 0.3768 - val_accuracy: 0.8874\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8924 - val_loss: 0.3657 - val_accuracy: 0.8878\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2593 - accuracy: 0.8912 - val_loss: 0.3267 - val_accuracy: 0.8797\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8922 - val_loss: 0.3214 - val_accuracy: 0.8879\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8886 - val_loss: 0.4170 - val_accuracy: 0.8892\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2619 - accuracy: 0.8905 - val_loss: 0.4232 - val_accuracy: 0.8713\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2610 - accuracy: 0.8912 - val_loss: 0.3177 - val_accuracy: 0.8875\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2603 - accuracy: 0.8902 - val_loss: 0.3494 - val_accuracy: 0.8913\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2546 - accuracy: 0.8947 - val_loss: 0.3636 - val_accuracy: 0.8800\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2552 - accuracy: 0.8924 - val_loss: 0.3056 - val_accuracy: 0.8852\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2593 - accuracy: 0.8903 - val_loss: 0.3693 - val_accuracy: 0.8790\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2503 - accuracy: 0.8961 - val_loss: 0.2986 - val_accuracy: 0.8822\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8898 - val_loss: 0.4034 - val_accuracy: 0.8814\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8919 - val_loss: 0.2771 - val_accuracy: 0.8924\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2561 - accuracy: 0.8925 - val_loss: 0.3439 - val_accuracy: 0.8883\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2531 - accuracy: 0.8928 - val_loss: 0.3771 - val_accuracy: 0.8785\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8906 - val_loss: 0.2809 - val_accuracy: 0.8906\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8900 - val_loss: 0.2890 - val_accuracy: 0.8839\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2531 - accuracy: 0.8948 - val_loss: 0.3747 - val_accuracy: 0.8744\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2556 - accuracy: 0.8930 - val_loss: 0.2946 - val_accuracy: 0.8929\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2491 - accuracy: 0.8958 - val_loss: 0.2928 - val_accuracy: 0.8948\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2504 - accuracy: 0.8948 - val_loss: 0.2902 - val_accuracy: 0.8888\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2527 - accuracy: 0.8920 - val_loss: 0.3261 - val_accuracy: 0.8828\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2542 - accuracy: 0.8932 - val_loss: 0.2603 - val_accuracy: 0.8932\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2547 - accuracy: 0.8935 - val_loss: 0.2889 - val_accuracy: 0.8946\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2484 - accuracy: 0.8936 - val_loss: 0.2682 - val_accuracy: 0.8837\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2505 - accuracy: 0.8955 - val_loss: 0.2883 - val_accuracy: 0.8853\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2495 - accuracy: 0.8942 - val_loss: 0.2429 - val_accuracy: 0.8954\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2496 - accuracy: 0.8945 - val_loss: 0.3163 - val_accuracy: 0.8792\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2556 - accuracy: 0.8931 - val_loss: 0.2614 - val_accuracy: 0.8850\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2456 - accuracy: 0.8955 - val_loss: 0.2850 - val_accuracy: 0.8879\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2476 - accuracy: 0.8945 - val_loss: 0.2713 - val_accuracy: 0.8896\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2518 - accuracy: 0.8946 - val_loss: 0.2570 - val_accuracy: 0.8949\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2546 - accuracy: 0.8932 - val_loss: 0.2549 - val_accuracy: 0.8897\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2590 - accuracy: 0.8857\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2473 - accuracy: 0.8953\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2475 - accuracy: 0.8945\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2472 - accuracy: 0.8953\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2460 - accuracy: 0.8944\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2465 - accuracy: 0.8955\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2469 - accuracy: 0.8941\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2447 - accuracy: 0.8942\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2452 - accuracy: 0.8963\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2441 - accuracy: 0.8958\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2452 - accuracy: 0.8958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5971 - accuracy: 0.6784 - val_loss: 0.3470 - val_accuracy: 0.8859\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3594 - accuracy: 0.8849 - val_loss: 0.3496 - val_accuracy: 0.8818\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3299 - accuracy: 0.8902 - val_loss: 0.3874 - val_accuracy: 0.8816\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3155 - accuracy: 0.8896 - val_loss: 0.3405 - val_accuracy: 0.8840\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2987 - accuracy: 0.8910 - val_loss: 0.3574 - val_accuracy: 0.8924\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2966 - accuracy: 0.8884 - val_loss: 0.3259 - val_accuracy: 0.8934\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2904 - accuracy: 0.8847 - val_loss: 0.3406 - val_accuracy: 0.8770\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2808 - accuracy: 0.8907 - val_loss: 0.2920 - val_accuracy: 0.8916\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2790 - accuracy: 0.8887 - val_loss: 0.4221 - val_accuracy: 0.8783\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2801 - accuracy: 0.8854 - val_loss: 0.3005 - val_accuracy: 0.8879\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8895 - val_loss: 0.3244 - val_accuracy: 0.8902\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2695 - accuracy: 0.8909 - val_loss: 0.3856 - val_accuracy: 0.8833\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8882 - val_loss: 0.3106 - val_accuracy: 0.8859\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.8867 - val_loss: 0.2833 - val_accuracy: 0.8932\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2685 - accuracy: 0.8887 - val_loss: 0.3226 - val_accuracy: 0.8888\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8908 - val_loss: 0.3046 - val_accuracy: 0.8909\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8896 - val_loss: 0.3216 - val_accuracy: 0.8820\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2627 - accuracy: 0.8905 - val_loss: 0.2819 - val_accuracy: 0.8926\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8906 - val_loss: 0.3361 - val_accuracy: 0.8891\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2624 - accuracy: 0.8900 - val_loss: 0.3317 - val_accuracy: 0.8852\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8893 - val_loss: 0.2886 - val_accuracy: 0.8940\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8914 - val_loss: 0.2979 - val_accuracy: 0.8800\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2561 - accuracy: 0.8943 - val_loss: 0.3381 - val_accuracy: 0.8811\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2623 - accuracy: 0.8892 - val_loss: 0.3054 - val_accuracy: 0.8834\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8907 - val_loss: 0.3429 - val_accuracy: 0.8834\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2569 - accuracy: 0.8915 - val_loss: 0.2670 - val_accuracy: 0.8898\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8899 - val_loss: 0.2874 - val_accuracy: 0.8884\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2570 - accuracy: 0.8905 - val_loss: 0.3038 - val_accuracy: 0.8864\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2560 - accuracy: 0.8914 - val_loss: 0.3064 - val_accuracy: 0.8832\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2573 - accuracy: 0.8894 - val_loss: 0.2824 - val_accuracy: 0.8911\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8886 - val_loss: 0.2516 - val_accuracy: 0.8932\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8907 - val_loss: 0.2628 - val_accuracy: 0.8957\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2564 - accuracy: 0.8919 - val_loss: 0.2888 - val_accuracy: 0.8881\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2622 - accuracy: 0.8895 - val_loss: 0.2791 - val_accuracy: 0.8824\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8897 - val_loss: 0.2683 - val_accuracy: 0.8910\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2588 - accuracy: 0.8931 - val_loss: 0.2584 - val_accuracy: 0.8935\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2532 - accuracy: 0.8917 - val_loss: 0.2813 - val_accuracy: 0.8885\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2563 - accuracy: 0.8919 - val_loss: 0.2872 - val_accuracy: 0.8862\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2620 - accuracy: 0.8890 - val_loss: 0.2638 - val_accuracy: 0.8959\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2543 - accuracy: 0.8950 - val_loss: 0.2810 - val_accuracy: 0.8897\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2536 - accuracy: 0.8961 - val_loss: 0.2698 - val_accuracy: 0.8925\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2566 - accuracy: 0.8894 - val_loss: 0.2548 - val_accuracy: 0.8905\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2538 - accuracy: 0.8965 - val_loss: 0.2604 - val_accuracy: 0.8989\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2528 - accuracy: 0.8932 - val_loss: 0.2783 - val_accuracy: 0.8827\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.8955 - val_loss: 0.2788 - val_accuracy: 0.8934\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8923 - val_loss: 0.3238 - val_accuracy: 0.8812\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2522 - accuracy: 0.8960 - val_loss: 0.2730 - val_accuracy: 0.8924\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2496 - accuracy: 0.8975 - val_loss: 0.2961 - val_accuracy: 0.8941\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2542 - accuracy: 0.8928 - val_loss: 0.2974 - val_accuracy: 0.8774\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8953 - val_loss: 0.2779 - val_accuracy: 0.8899\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2807 - accuracy: 0.8864\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2534 - accuracy: 0.8945\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2544 - accuracy: 0.8930\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2533 - accuracy: 0.8944\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2534 - accuracy: 0.8947\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2530 - accuracy: 0.8956\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2531 - accuracy: 0.8947\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2525 - accuracy: 0.8960\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2536 - accuracy: 0.8950\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2530 - accuracy: 0.8955\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2528 - accuracy: 0.8950\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.5798 - accuracy: 0.6981 - val_loss: 0.3385 - val_accuracy: 0.8860\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3520 - accuracy: 0.8881 - val_loss: 0.3478 - val_accuracy: 0.8868\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8870 - val_loss: 0.3572 - val_accuracy: 0.8869\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3164 - accuracy: 0.8876 - val_loss: 0.3690 - val_accuracy: 0.8806\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3075 - accuracy: 0.8871 - val_loss: 0.3503 - val_accuracy: 0.8924\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2923 - accuracy: 0.8891 - val_loss: 0.3310 - val_accuracy: 0.8924\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2886 - accuracy: 0.8878 - val_loss: 0.3462 - val_accuracy: 0.8848\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2814 - accuracy: 0.8907 - val_loss: 0.3868 - val_accuracy: 0.8817\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2803 - accuracy: 0.8873 - val_loss: 0.3199 - val_accuracy: 0.8912\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2755 - accuracy: 0.8865 - val_loss: 0.3422 - val_accuracy: 0.8821\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2703 - accuracy: 0.8886 - val_loss: 0.3348 - val_accuracy: 0.8938\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2717 - accuracy: 0.8897 - val_loss: 0.3919 - val_accuracy: 0.8813\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2656 - accuracy: 0.8911 - val_loss: 0.3372 - val_accuracy: 0.8816\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2717 - accuracy: 0.8880 - val_loss: 0.3399 - val_accuracy: 0.8908\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2650 - accuracy: 0.8904 - val_loss: 0.3780 - val_accuracy: 0.8867\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.8895 - val_loss: 0.2855 - val_accuracy: 0.8937\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2680 - accuracy: 0.8890 - val_loss: 0.3579 - val_accuracy: 0.8882\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2654 - accuracy: 0.8870 - val_loss: 0.3282 - val_accuracy: 0.8919\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2664 - accuracy: 0.8926 - val_loss: 0.3147 - val_accuracy: 0.8916\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2617 - accuracy: 0.8930 - val_loss: 0.4257 - val_accuracy: 0.8798\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2627 - accuracy: 0.8918 - val_loss: 0.3172 - val_accuracy: 0.8897\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2605 - accuracy: 0.8920 - val_loss: 0.2836 - val_accuracy: 0.8841\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2631 - accuracy: 0.8889 - val_loss: 0.3124 - val_accuracy: 0.8877\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2593 - accuracy: 0.8929 - val_loss: 0.3485 - val_accuracy: 0.8870\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2608 - accuracy: 0.8900 - val_loss: 0.3288 - val_accuracy: 0.8856\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8890 - val_loss: 0.2834 - val_accuracy: 0.8909\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3380 - accuracy: 0.8900\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2689 - accuracy: 0.8889\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2679 - accuracy: 0.8890\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2669 - accuracy: 0.8900\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2636 - accuracy: 0.8907\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2627 - accuracy: 0.8913\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2609 - accuracy: 0.8912\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2605 - accuracy: 0.8918\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2608 - accuracy: 0.8912\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2588 - accuracy: 0.8918\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2585 - accuracy: 0.8919\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1nwsR7BJaRn",
        "outputId": "87632607-afc5-42ae-94fb-861cbc410109"
      },
      "source": [
        "cm1 = confusion_matrix(y, cv1_preds)\n",
        "print(cm1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[48872  9128]\n",
            " [ 1153 29494]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY7KpWP5oQPI"
      },
      "source": [
        "'''dropout_rate = [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4]\n",
        "param_grid = dict(dropout_rate=dropout_rate)\n",
        "grid1 = GridSearchCV(estimator=mod1, param_grid=param_grid, cv=10)\n",
        "grid_result1 = grid1.fit(X, y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result1.best_score_, grid_result1.best_params_))\n",
        "for params, mean_score, scores in grid_result1.grid_scores_:\n",
        "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEE8ajVx5v0-"
      },
      "source": [
        "# neural network on dataset attributes based on domain URL (table 2)\n",
        "\n",
        "### (21 features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "qzDFHL-u5v0_",
        "outputId": "a5382844-3ad8-4ff3-b152-29f9f5eb578d"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table1 = ['qty_dot_domain', 'qty_hyphen_domain', 'qty_underline_domain', 'qty_slash_domain', 'qty_questionmark_domain', 'qty_equal_domain', 'qty_at_domain', 'qty_and_domain',\n",
        "                   'qty_exclamation_domain', 'qty_space_domain', 'qty_tilde_domain', 'qty_comma_domain', 'qty_plus_domain', 'qty_asterisk_domain', 'qty_hashtag_domain', 'qty_dollar_domain',\n",
        "                   'qty_percent_domain', 'qty_vowels_domain', 'domain_length', 'domain_in_ip', 'server_client_domain'] \n",
        "\n",
        "X = full_df[features_table1]\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_hyphen_domain</th>\n",
              "      <th>qty_underline_domain</th>\n",
              "      <th>qty_slash_domain</th>\n",
              "      <th>qty_questionmark_domain</th>\n",
              "      <th>qty_equal_domain</th>\n",
              "      <th>qty_at_domain</th>\n",
              "      <th>qty_and_domain</th>\n",
              "      <th>qty_exclamation_domain</th>\n",
              "      <th>qty_space_domain</th>\n",
              "      <th>qty_tilde_domain</th>\n",
              "      <th>qty_comma_domain</th>\n",
              "      <th>qty_plus_domain</th>\n",
              "      <th>qty_asterisk_domain</th>\n",
              "      <th>qty_hashtag_domain</th>\n",
              "      <th>qty_dollar_domain</th>\n",
              "      <th>qty_percent_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>domain_in_ip</th>\n",
              "      <th>server_client_domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>28</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_domain  qty_hyphen_domain  ...  domain_in_ip  server_client_domain\n",
              "5676                1                  0  ...             0                     0\n",
              "39002               3                  0  ...             0                     0\n",
              "1732                2                  0  ...             0                     0\n",
              "39668               2                  0  ...             0                     0\n",
              "82035               2                  0  ...             0                     0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMsYr1fj5v0_",
        "outputId": "7608afaf-7a0d-40f8-e175-234c343500da"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88647, 21)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7rwOTpK5v0_",
        "outputId": "b04a6cde-ae91-424d-b3cb-037170ab43e9"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "table2_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[21]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "table2_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "table2_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                1408      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 14,305\n",
            "Trainable params: 14,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NdHGBv_5v1A",
        "outputId": "e69faf36-1586-4bc6-9662-84089a30eac2"
      },
      "source": [
        "history = table2_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 8ms/step - loss: 0.6673 - binary_accuracy: 0.6329 - auc: 0.5016 - val_loss: 0.6406 - val_binary_accuracy: 0.6592 - val_auc: 0.6125\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6349 - binary_accuracy: 0.6551 - auc: 0.5897 - val_loss: 0.6041 - val_binary_accuracy: 0.6592 - val_auc: 0.7060\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6108 - binary_accuracy: 0.6508 - auc: 0.6778 - val_loss: 0.6089 - val_binary_accuracy: 0.6592 - val_auc: 0.7175\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5934 - binary_accuracy: 0.6623 - auc: 0.7053 - val_loss: 0.6016 - val_binary_accuracy: 0.7087 - val_auc: 0.7296\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5807 - binary_accuracy: 0.7131 - auc: 0.7210 - val_loss: 0.5596 - val_binary_accuracy: 0.7247 - val_auc: 0.7406\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5695 - binary_accuracy: 0.7208 - auc: 0.7253 - val_loss: 0.5500 - val_binary_accuracy: 0.7333 - val_auc: 0.7456\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5606 - binary_accuracy: 0.7284 - auc: 0.7302 - val_loss: 0.5446 - val_binary_accuracy: 0.7404 - val_auc: 0.7538\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5590 - binary_accuracy: 0.7275 - auc: 0.7366 - val_loss: 0.5457 - val_binary_accuracy: 0.7371 - val_auc: 0.7536\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5591 - binary_accuracy: 0.7264 - auc: 0.7357 - val_loss: 0.5468 - val_binary_accuracy: 0.7319 - val_auc: 0.7583\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5570 - binary_accuracy: 0.7314 - auc: 0.7373 - val_loss: 0.5439 - val_binary_accuracy: 0.7349 - val_auc: 0.7566\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5562 - binary_accuracy: 0.7313 - auc: 0.7427 - val_loss: 0.5432 - val_binary_accuracy: 0.7411 - val_auc: 0.7566\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5522 - binary_accuracy: 0.7340 - auc: 0.7416 - val_loss: 0.5466 - val_binary_accuracy: 0.7405 - val_auc: 0.7603\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5511 - binary_accuracy: 0.7388 - auc: 0.7442 - val_loss: 0.5409 - val_binary_accuracy: 0.7431 - val_auc: 0.7572\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5521 - binary_accuracy: 0.7344 - auc: 0.7419 - val_loss: 0.5465 - val_binary_accuracy: 0.7350 - val_auc: 0.7579\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5543 - binary_accuracy: 0.7335 - auc: 0.7431 - val_loss: 0.5424 - val_binary_accuracy: 0.7398 - val_auc: 0.7553\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5520 - binary_accuracy: 0.7341 - auc: 0.7441 - val_loss: 0.5402 - val_binary_accuracy: 0.7456 - val_auc: 0.7600\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5487 - binary_accuracy: 0.7381 - auc: 0.7490 - val_loss: 0.5379 - val_binary_accuracy: 0.7458 - val_auc: 0.7642\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5521 - binary_accuracy: 0.7367 - auc: 0.7423 - val_loss: 0.5389 - val_binary_accuracy: 0.7459 - val_auc: 0.7602\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5487 - binary_accuracy: 0.7369 - auc: 0.7493 - val_loss: 0.5522 - val_binary_accuracy: 0.7340 - val_auc: 0.7574\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5569 - binary_accuracy: 0.7347 - auc: 0.7383 - val_loss: 0.5355 - val_binary_accuracy: 0.7487 - val_auc: 0.7609\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5464 - binary_accuracy: 0.7404 - auc: 0.7504 - val_loss: 0.5401 - val_binary_accuracy: 0.7489 - val_auc: 0.7644\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5490 - binary_accuracy: 0.7392 - auc: 0.7472 - val_loss: 0.5466 - val_binary_accuracy: 0.7466 - val_auc: 0.7652\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5457 - binary_accuracy: 0.7390 - auc: 0.7500 - val_loss: 0.5402 - val_binary_accuracy: 0.7458 - val_auc: 0.7614\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5451 - binary_accuracy: 0.7417 - auc: 0.7499 - val_loss: 0.5476 - val_binary_accuracy: 0.7461 - val_auc: 0.7624\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5459 - binary_accuracy: 0.7425 - auc: 0.7504 - val_loss: 0.5354 - val_binary_accuracy: 0.7478 - val_auc: 0.7647\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5461 - binary_accuracy: 0.7428 - auc: 0.7503 - val_loss: 0.5373 - val_binary_accuracy: 0.7475 - val_auc: 0.7626\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5398 - binary_accuracy: 0.7460 - auc: 0.7586 - val_loss: 0.5404 - val_binary_accuracy: 0.7462 - val_auc: 0.7614\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5427 - binary_accuracy: 0.7421 - auc: 0.7554 - val_loss: 0.5405 - val_binary_accuracy: 0.7436 - val_auc: 0.7635\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5426 - binary_accuracy: 0.7440 - auc: 0.7567 - val_loss: 0.5337 - val_binary_accuracy: 0.7488 - val_auc: 0.7676\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5440 - binary_accuracy: 0.7422 - auc: 0.7543 - val_loss: 0.5351 - val_binary_accuracy: 0.7523 - val_auc: 0.7678\n",
            "Epoch 31/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5419 - binary_accuracy: 0.7427 - auc: 0.7562 - val_loss: 0.5340 - val_binary_accuracy: 0.7480 - val_auc: 0.7668\n",
            "Epoch 32/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5435 - binary_accuracy: 0.7420 - auc: 0.7559 - val_loss: 0.5325 - val_binary_accuracy: 0.7498 - val_auc: 0.7654\n",
            "Epoch 33/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5422 - binary_accuracy: 0.7446 - auc: 0.7577 - val_loss: 0.5298 - val_binary_accuracy: 0.7495 - val_auc: 0.7692\n",
            "Epoch 34/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5463 - binary_accuracy: 0.7400 - auc: 0.7538 - val_loss: 0.5415 - val_binary_accuracy: 0.7438 - val_auc: 0.7650\n",
            "Epoch 35/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5433 - binary_accuracy: 0.7421 - auc: 0.7553 - val_loss: 0.5378 - val_binary_accuracy: 0.7495 - val_auc: 0.7711\n",
            "Epoch 36/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5442 - binary_accuracy: 0.7429 - auc: 0.7573 - val_loss: 0.5363 - val_binary_accuracy: 0.7489 - val_auc: 0.7699\n",
            "Epoch 37/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5452 - binary_accuracy: 0.7418 - auc: 0.7533 - val_loss: 0.5329 - val_binary_accuracy: 0.7519 - val_auc: 0.7729\n",
            "Epoch 38/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5390 - binary_accuracy: 0.7478 - auc: 0.7621 - val_loss: 0.5350 - val_binary_accuracy: 0.7498 - val_auc: 0.7752\n",
            "Epoch 39/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5431 - binary_accuracy: 0.7420 - auc: 0.7561 - val_loss: 0.5326 - val_binary_accuracy: 0.7488 - val_auc: 0.7722\n",
            "Epoch 40/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5418 - binary_accuracy: 0.7444 - auc: 0.7576 - val_loss: 0.5374 - val_binary_accuracy: 0.7494 - val_auc: 0.7700\n",
            "Epoch 41/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5419 - binary_accuracy: 0.7440 - auc: 0.7573 - val_loss: 0.5339 - val_binary_accuracy: 0.7501 - val_auc: 0.7719\n",
            "Epoch 42/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5381 - binary_accuracy: 0.7467 - auc: 0.7632 - val_loss: 0.5271 - val_binary_accuracy: 0.7539 - val_auc: 0.7753\n",
            "Epoch 43/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5425 - binary_accuracy: 0.7416 - auc: 0.7575 - val_loss: 0.5336 - val_binary_accuracy: 0.7505 - val_auc: 0.7739\n",
            "Epoch 44/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5376 - binary_accuracy: 0.7466 - auc: 0.7644 - val_loss: 0.5339 - val_binary_accuracy: 0.7509 - val_auc: 0.7748\n",
            "Epoch 45/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5357 - binary_accuracy: 0.7481 - auc: 0.7639 - val_loss: 0.5333 - val_binary_accuracy: 0.7502 - val_auc: 0.7741\n",
            "Epoch 46/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5407 - binary_accuracy: 0.7459 - auc: 0.7611 - val_loss: 0.5290 - val_binary_accuracy: 0.7506 - val_auc: 0.7761\n",
            "Epoch 47/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5405 - binary_accuracy: 0.7442 - auc: 0.7629 - val_loss: 0.5333 - val_binary_accuracy: 0.7527 - val_auc: 0.7765\n",
            "Epoch 48/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5402 - binary_accuracy: 0.7458 - auc: 0.7604 - val_loss: 0.5298 - val_binary_accuracy: 0.7538 - val_auc: 0.7767\n",
            "Epoch 49/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5332 - binary_accuracy: 0.7518 - auc: 0.7677 - val_loss: 0.5285 - val_binary_accuracy: 0.7506 - val_auc: 0.7761\n",
            "Epoch 50/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5409 - binary_accuracy: 0.7436 - auc: 0.7600 - val_loss: 0.5378 - val_binary_accuracy: 0.7501 - val_auc: 0.7764\n",
            "Epoch 51/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5372 - binary_accuracy: 0.7463 - auc: 0.7636 - val_loss: 0.5303 - val_binary_accuracy: 0.7500 - val_auc: 0.7761\n",
            "Epoch 52/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5416 - binary_accuracy: 0.7453 - auc: 0.7597 - val_loss: 0.5313 - val_binary_accuracy: 0.7492 - val_auc: 0.7773\n",
            "Epoch 53/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5373 - binary_accuracy: 0.7447 - auc: 0.7651 - val_loss: 0.5317 - val_binary_accuracy: 0.7541 - val_auc: 0.7763\n",
            "Epoch 54/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5381 - binary_accuracy: 0.7467 - auc: 0.7629 - val_loss: 0.5329 - val_binary_accuracy: 0.7486 - val_auc: 0.7734\n",
            "Epoch 55/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5407 - binary_accuracy: 0.7375 - auc: 0.7626 - val_loss: 0.5323 - val_binary_accuracy: 0.7508 - val_auc: 0.7758\n",
            "Epoch 56/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5371 - binary_accuracy: 0.7471 - auc: 0.7645 - val_loss: 0.5296 - val_binary_accuracy: 0.7549 - val_auc: 0.7776\n",
            "Epoch 57/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5384 - binary_accuracy: 0.7455 - auc: 0.7641 - val_loss: 0.5378 - val_binary_accuracy: 0.7512 - val_auc: 0.7773\n",
            "Epoch 58/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5333 - binary_accuracy: 0.7510 - auc: 0.7657 - val_loss: 0.5308 - val_binary_accuracy: 0.7553 - val_auc: 0.7744\n",
            "Epoch 59/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5393 - binary_accuracy: 0.7431 - auc: 0.7624 - val_loss: 0.5329 - val_binary_accuracy: 0.7521 - val_auc: 0.7774\n",
            "Epoch 60/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5338 - binary_accuracy: 0.7519 - auc: 0.7682 - val_loss: 0.5340 - val_binary_accuracy: 0.7519 - val_auc: 0.7779\n",
            "Epoch 61/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5399 - binary_accuracy: 0.7437 - auc: 0.7623 - val_loss: 0.5304 - val_binary_accuracy: 0.7519 - val_auc: 0.7784\n",
            "Epoch 62/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5380 - binary_accuracy: 0.7453 - auc: 0.7637 - val_loss: 0.5312 - val_binary_accuracy: 0.7507 - val_auc: 0.7759\n",
            "Epoch 63/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5330 - binary_accuracy: 0.7501 - auc: 0.7703 - val_loss: 0.5328 - val_binary_accuracy: 0.7522 - val_auc: 0.7765\n",
            "Epoch 64/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5359 - binary_accuracy: 0.7487 - auc: 0.7651 - val_loss: 0.5376 - val_binary_accuracy: 0.7512 - val_auc: 0.7779\n",
            "Epoch 65/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5374 - binary_accuracy: 0.7469 - auc: 0.7625 - val_loss: 0.5311 - val_binary_accuracy: 0.7515 - val_auc: 0.7768\n",
            "Epoch 66/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5317 - binary_accuracy: 0.7520 - auc: 0.7693 - val_loss: 0.5289 - val_binary_accuracy: 0.7534 - val_auc: 0.7786\n",
            "Epoch 67/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5347 - binary_accuracy: 0.7480 - auc: 0.7675 - val_loss: 0.5305 - val_binary_accuracy: 0.7526 - val_auc: 0.7756\n",
            "Epoch 68/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5393 - binary_accuracy: 0.7406 - auc: 0.7642 - val_loss: 0.5330 - val_binary_accuracy: 0.7507 - val_auc: 0.7776\n",
            "Epoch 69/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5390 - binary_accuracy: 0.7461 - auc: 0.7652 - val_loss: 0.5284 - val_binary_accuracy: 0.7526 - val_auc: 0.7776\n",
            "Epoch 70/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5369 - binary_accuracy: 0.7457 - auc: 0.7668 - val_loss: 0.5385 - val_binary_accuracy: 0.7517 - val_auc: 0.7765\n",
            "Epoch 71/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5360 - binary_accuracy: 0.7501 - auc: 0.7669 - val_loss: 0.5367 - val_binary_accuracy: 0.7438 - val_auc: 0.7729\n",
            "Epoch 72/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5391 - binary_accuracy: 0.7432 - auc: 0.7626 - val_loss: 0.5340 - val_binary_accuracy: 0.7498 - val_auc: 0.7748\n",
            "Epoch 73/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5389 - binary_accuracy: 0.7447 - auc: 0.7642 - val_loss: 0.5288 - val_binary_accuracy: 0.7524 - val_auc: 0.7775\n",
            "Epoch 74/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5359 - binary_accuracy: 0.7475 - auc: 0.7673 - val_loss: 0.5402 - val_binary_accuracy: 0.7459 - val_auc: 0.7730\n",
            "Epoch 75/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5432 - binary_accuracy: 0.7430 - auc: 0.7595 - val_loss: 0.5336 - val_binary_accuracy: 0.7507 - val_auc: 0.7774\n",
            "Epoch 76/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5372 - binary_accuracy: 0.7461 - auc: 0.7663 - val_loss: 0.5287 - val_binary_accuracy: 0.7536 - val_auc: 0.7796\n",
            "Epoch 77/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5337 - binary_accuracy: 0.7486 - auc: 0.7685 - val_loss: 0.5359 - val_binary_accuracy: 0.7510 - val_auc: 0.7787\n",
            "Epoch 78/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5370 - binary_accuracy: 0.7464 - auc: 0.7646 - val_loss: 0.5311 - val_binary_accuracy: 0.7520 - val_auc: 0.7788\n",
            "Epoch 79/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5372 - binary_accuracy: 0.7487 - auc: 0.7664 - val_loss: 0.5347 - val_binary_accuracy: 0.7538 - val_auc: 0.7807\n",
            "Epoch 80/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5368 - binary_accuracy: 0.7472 - auc: 0.7658 - val_loss: 0.5342 - val_binary_accuracy: 0.7506 - val_auc: 0.7774\n",
            "Epoch 81/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5329 - binary_accuracy: 0.7466 - auc: 0.7711 - val_loss: 0.5355 - val_binary_accuracy: 0.7531 - val_auc: 0.7792\n",
            "Epoch 82/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5328 - binary_accuracy: 0.7502 - auc: 0.7687 - val_loss: 0.5379 - val_binary_accuracy: 0.7472 - val_auc: 0.7750\n",
            "Epoch 83/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5388 - binary_accuracy: 0.7445 - auc: 0.7646 - val_loss: 0.5350 - val_binary_accuracy: 0.7487 - val_auc: 0.7789\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "XLkpBFPI5v1A",
        "outputId": "07a535bb-34c0-4244-c8aa-c94621abe601"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "history_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>83.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>83.000000</td>\n",
              "      <td>83.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.546830</td>\n",
              "      <td>0.738396</td>\n",
              "      <td>0.750385</td>\n",
              "      <td>0.540109</td>\n",
              "      <td>0.744266</td>\n",
              "      <td>0.766182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.019015</td>\n",
              "      <td>0.019392</td>\n",
              "      <td>0.033776</td>\n",
              "      <td>0.018086</td>\n",
              "      <td>0.018062</td>\n",
              "      <td>0.021565</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.535144</td>\n",
              "      <td>0.648316</td>\n",
              "      <td>0.506675</td>\n",
              "      <td>0.527051</td>\n",
              "      <td>0.659180</td>\n",
              "      <td>0.612541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.537947</td>\n",
              "      <td>0.739713</td>\n",
              "      <td>0.748606</td>\n",
              "      <td>0.532541</td>\n",
              "      <td>0.745789</td>\n",
              "      <td>0.761926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.540744</td>\n",
              "      <td>0.744279</td>\n",
              "      <td>0.759433</td>\n",
              "      <td>0.535062</td>\n",
              "      <td>0.749524</td>\n",
              "      <td>0.773008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.548001</td>\n",
              "      <td>0.746353</td>\n",
              "      <td>0.764124</td>\n",
              "      <td>0.540335</td>\n",
              "      <td>0.751579</td>\n",
              "      <td>0.776602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.652841</td>\n",
              "      <td>0.749909</td>\n",
              "      <td>0.767385</td>\n",
              "      <td>0.640649</td>\n",
              "      <td>0.755339</td>\n",
              "      <td>0.780659</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  83.000000        83.000000  ...            83.000000  83.000000\n",
              "mean    0.546830         0.738396  ...             0.744266   0.766182\n",
              "std     0.019015         0.019392  ...             0.018062   0.021565\n",
              "min     0.535144         0.648316  ...             0.659180   0.612541\n",
              "25%     0.537947         0.739713  ...             0.745789   0.761926\n",
              "50%     0.540744         0.744279  ...             0.749524   0.773008\n",
              "75%     0.548001         0.746353  ...             0.751579   0.776602\n",
              "max     0.652841         0.749909  ...             0.755339   0.780659\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oORlWgjV5v1A",
        "outputId": "e0e78c7f-ead2-484c-fedd-0432e238ab2e"
      },
      "source": [
        "train_acc = table2_nn.evaluate(train_X, train_y)\n",
        "test_acc = table2_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 1ms/step - loss: 0.5321 - binary_accuracy: 0.7543 - auc: 0.7753\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5342 - binary_accuracy: 0.7532 - auc: 0.7706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyukqIQl5v1B",
        "outputId": "fd04da97-0f7d-4cb1-a06e-f7c2005190ca"
      },
      "source": [
        "dict(zip(table2_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.7705593109130859,\n",
              " 'binary_accuracy': 0.7532262206077576,\n",
              " 'loss': 0.5342294573783875}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "UnwKtRHe5v1B",
        "outputId": "c364f0fa-e774-4bbb-d1df-9c89f9bf406f"
      },
      "source": [
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.5270507335662842\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "oVanKhZb5v1B",
        "outputId": "ece44b81-d97b-4ff9-d226-e0754dcd19df"
      },
      "source": [
        "history_df.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.7806586027145386\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "qc4qp7DE5v1B",
        "outputId": "c2c15074-7144-44c8-ed25-4acf8092fc89"
      },
      "source": [
        "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.7553394436836243\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvtrjw_75v1C",
        "outputId": "37f7e68f-a465-46d2-bfa1-78a02b74e963"
      },
      "source": [
        "preds1 = table2_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.18894672],\n",
              "       [0.27802005],\n",
              "       [0.19450375],\n",
              "       ...,\n",
              "       [0.1833435 ],\n",
              "       [0.4506697 ],\n",
              "       [0.218847  ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8A4gcB-5v1C",
        "outputId": "6a631997-6bc0-4252-e627-ec8ecdf1f033"
      },
      "source": [
        "len(preds1[preds1 < 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15746"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu6Dwolx5v1D",
        "outputId": "47afecbb-3dbd-4b01-cd76-07aa3bbd4386"
      },
      "source": [
        "len(preds1[preds1 >= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6416"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OimbNDWP5v1D",
        "outputId": "3fc98f1a-6666-4384-c59e-296bc98e845e"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "jL9nSu8Mq2TV",
        "outputId": "415c0bc6-bc02-47d5-93e2-169e07d7a783"
      },
      "source": [
        "preds_df = pd.DataFrame(preds1, columns = ['preds'])\n",
        "\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.188947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.278020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.194504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.235895</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.634766</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      preds\n",
              "0  0.188947\n",
              "1  0.278020\n",
              "2  0.194504\n",
              "3  0.235895\n",
              "4  0.634766"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "XFjbvOXPq12a",
        "outputId": "c8f41d05-c1db-4ad7-ed9f-31836b2c5472"
      },
      "source": [
        "preds_df = pd.concat([preds_df, val_y.reset_index(drop=True), val_X.reset_index()], axis=1)\n",
        "\n",
        "preds_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>phishing</th>\n",
              "      <th>index</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_hyphen_domain</th>\n",
              "      <th>qty_underline_domain</th>\n",
              "      <th>qty_slash_domain</th>\n",
              "      <th>qty_questionmark_domain</th>\n",
              "      <th>qty_equal_domain</th>\n",
              "      <th>qty_at_domain</th>\n",
              "      <th>qty_and_domain</th>\n",
              "      <th>qty_exclamation_domain</th>\n",
              "      <th>qty_space_domain</th>\n",
              "      <th>qty_tilde_domain</th>\n",
              "      <th>qty_comma_domain</th>\n",
              "      <th>qty_plus_domain</th>\n",
              "      <th>qty_asterisk_domain</th>\n",
              "      <th>qty_hashtag_domain</th>\n",
              "      <th>qty_dollar_domain</th>\n",
              "      <th>qty_percent_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>domain_in_ip</th>\n",
              "      <th>server_client_domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.188947</td>\n",
              "      <td>1</td>\n",
              "      <td>62575</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.278020</td>\n",
              "      <td>0</td>\n",
              "      <td>38126</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.194504</td>\n",
              "      <td>0</td>\n",
              "      <td>1617</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.235895</td>\n",
              "      <td>0</td>\n",
              "      <td>8228</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.634766</td>\n",
              "      <td>1</td>\n",
              "      <td>55594</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22157</th>\n",
              "      <td>0.170489</td>\n",
              "      <td>0</td>\n",
              "      <td>65294</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>17</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22158</th>\n",
              "      <td>0.185905</td>\n",
              "      <td>0</td>\n",
              "      <td>10038</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22159</th>\n",
              "      <td>0.183343</td>\n",
              "      <td>0</td>\n",
              "      <td>43642</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22160</th>\n",
              "      <td>0.450670</td>\n",
              "      <td>0</td>\n",
              "      <td>73632</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>26</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22161</th>\n",
              "      <td>0.218847</td>\n",
              "      <td>1</td>\n",
              "      <td>25895</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22162 rows × 24 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          preds  phishing  ...  domain_in_ip  server_client_domain\n",
              "0      0.188947         1  ...             0                     0\n",
              "1      0.278020         0  ...             0                     0\n",
              "2      0.194504         0  ...             0                     0\n",
              "3      0.235895         0  ...             0                     0\n",
              "4      0.634766         1  ...             0                     0\n",
              "...         ...       ...  ...           ...                   ...\n",
              "22157  0.170489         0  ...             0                     0\n",
              "22158  0.185905         0  ...             0                     0\n",
              "22159  0.183343         0  ...             0                     1\n",
              "22160  0.450670         0  ...             0                     0\n",
              "22161  0.218847         1  ...             0                     0\n",
              "\n",
              "[22162 rows x 24 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3LoAg3Yq1zz",
        "outputId": "1832a284-53db-4ec1-edca-dc9262e0fbc1"
      },
      "source": [
        "pred_classes = np.argmax(preds1, axis=1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(val_y, pred_classes)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14519     0]\n",
            " [ 7643     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khptwsq3oQPJ"
      },
      "source": [
        "# NN on domain URL attributes (table 2) [model as function]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "vRO2QS8aoQPJ",
        "outputId": "fc87fad3-9877-46cb-a966-3158b39e82e1"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table2 = ['qty_dot_domain', 'qty_hyphen_domain', 'qty_underline_domain', 'qty_slash_domain', 'qty_questionmark_domain', 'qty_equal_domain', 'qty_at_domain', 'qty_and_domain',\n",
        "                   'qty_exclamation_domain', 'qty_space_domain', 'qty_tilde_domain', 'qty_comma_domain', 'qty_plus_domain', 'qty_asterisk_domain', 'qty_hashtag_domain', 'qty_dollar_domain',\n",
        "                   'qty_percent_domain', 'qty_vowels_domain', 'domain_length', 'domain_in_ip', 'server_client_domain'] \n",
        "\n",
        "X = full_df[features_table2]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_hyphen_domain</th>\n",
              "      <th>qty_underline_domain</th>\n",
              "      <th>qty_slash_domain</th>\n",
              "      <th>qty_questionmark_domain</th>\n",
              "      <th>qty_equal_domain</th>\n",
              "      <th>qty_at_domain</th>\n",
              "      <th>qty_and_domain</th>\n",
              "      <th>qty_exclamation_domain</th>\n",
              "      <th>qty_space_domain</th>\n",
              "      <th>qty_tilde_domain</th>\n",
              "      <th>qty_comma_domain</th>\n",
              "      <th>qty_plus_domain</th>\n",
              "      <th>qty_asterisk_domain</th>\n",
              "      <th>qty_hashtag_domain</th>\n",
              "      <th>qty_dollar_domain</th>\n",
              "      <th>qty_percent_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>domain_in_ip</th>\n",
              "      <th>server_client_domain</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.097590</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.195180</td>\n",
              "      <td>0.975900</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.222375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.148250</td>\n",
              "      <td>0.963624</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.118470</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.296174</td>\n",
              "      <td>0.947758</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.072500</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.326250</td>\n",
              "      <td>0.942499</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.069130</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.241955</td>\n",
              "      <td>0.967822</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_domain  qty_hyphen_domain  ...  domain_in_ip  server_client_domain\n",
              "5676         0.097590                0.0  ...           0.0                   0.0\n",
              "39002        0.222375                0.0  ...           0.0                   0.0\n",
              "1732         0.118470                0.0  ...           0.0                   0.0\n",
              "39668        0.072500                0.0  ...           0.0                   0.0\n",
              "82035        0.069130                0.0  ...           0.0                   0.0\n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIkS_bXAoQPJ"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_2():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[21]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNPtk2_foQPJ"
      },
      "source": [
        "mod2 = KerasClassifier(build_fn=phish_nn_2,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuuzoVILoQPJ",
        "outputId": "4dfb4687-7267-41cd-82e9-78fa75eb764f"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_2=cross_val_score(mod2,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6531 - accuracy: 0.6492 - val_loss: 0.6182 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6167 - accuracy: 0.6519 - val_loss: 0.6194 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6038 - accuracy: 0.6528 - val_loss: 0.6075 - val_accuracy: 0.6898\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5952 - accuracy: 0.6943 - val_loss: 0.6040 - val_accuracy: 0.6957\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5891 - accuracy: 0.6964 - val_loss: 0.5979 - val_accuracy: 0.7151\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5895 - accuracy: 0.7028 - val_loss: 0.5961 - val_accuracy: 0.6989\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5858 - accuracy: 0.7041 - val_loss: 0.5819 - val_accuracy: 0.7199\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5790 - accuracy: 0.7102 - val_loss: 0.5959 - val_accuracy: 0.6856\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5838 - accuracy: 0.7077 - val_loss: 0.5862 - val_accuracy: 0.7112\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5807 - accuracy: 0.7113 - val_loss: 0.5776 - val_accuracy: 0.6959\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5806 - accuracy: 0.7050 - val_loss: 0.5692 - val_accuracy: 0.7229\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5818 - accuracy: 0.7093 - val_loss: 0.5697 - val_accuracy: 0.7238\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5819 - accuracy: 0.7079 - val_loss: 0.5742 - val_accuracy: 0.7206\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5771 - accuracy: 0.7118 - val_loss: 0.5751 - val_accuracy: 0.7159\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5765 - accuracy: 0.7128 - val_loss: 0.5723 - val_accuracy: 0.7221\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5771 - accuracy: 0.7154 - val_loss: 0.5735 - val_accuracy: 0.7266\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5755 - accuracy: 0.7099 - val_loss: 0.5751 - val_accuracy: 0.7224\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5798 - accuracy: 0.7094 - val_loss: 0.5767 - val_accuracy: 0.7263\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5748 - accuracy: 0.7177 - val_loss: 0.5753 - val_accuracy: 0.7244\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5754 - accuracy: 0.7166 - val_loss: 0.5841 - val_accuracy: 0.7215\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7143 - val_loss: 0.5745 - val_accuracy: 0.7183\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7195 - val_loss: 0.5666 - val_accuracy: 0.7236\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5716 - accuracy: 0.7205 - val_loss: 0.5670 - val_accuracy: 0.7259\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5704 - accuracy: 0.7221 - val_loss: 0.5698 - val_accuracy: 0.7254\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5752 - accuracy: 0.7170 - val_loss: 0.5706 - val_accuracy: 0.7258\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5740 - accuracy: 0.7182 - val_loss: 0.5604 - val_accuracy: 0.7269\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5709 - accuracy: 0.7213 - val_loss: 0.5793 - val_accuracy: 0.7225\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5701 - accuracy: 0.7237 - val_loss: 0.5790 - val_accuracy: 0.7087\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5711 - accuracy: 0.7199 - val_loss: 0.5637 - val_accuracy: 0.7334\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5683 - accuracy: 0.7255 - val_loss: 0.5661 - val_accuracy: 0.7299\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5698 - accuracy: 0.7210 - val_loss: 0.5864 - val_accuracy: 0.7188\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5736 - accuracy: 0.7165 - val_loss: 0.5697 - val_accuracy: 0.7315\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5680 - accuracy: 0.7236 - val_loss: 0.5926 - val_accuracy: 0.7101\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5718 - accuracy: 0.7207 - val_loss: 0.5661 - val_accuracy: 0.7259\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5707 - accuracy: 0.7218 - val_loss: 0.5677 - val_accuracy: 0.7302\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5716 - accuracy: 0.7178 - val_loss: 0.5840 - val_accuracy: 0.7207\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5734 - accuracy: 0.7191 - val_loss: 0.5788 - val_accuracy: 0.7215\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5682 - accuracy: 0.7227 - val_loss: 0.5731 - val_accuracy: 0.7310\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5685 - accuracy: 0.7224 - val_loss: 0.5627 - val_accuracy: 0.7333\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5722 - accuracy: 0.7204 - val_loss: 0.5658 - val_accuracy: 0.7312\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5704 - accuracy: 0.7226 - val_loss: 0.5622 - val_accuracy: 0.7322\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5695 - accuracy: 0.7216 - val_loss: 0.5693 - val_accuracy: 0.7318\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5714 - accuracy: 0.7192 - val_loss: 0.5824 - val_accuracy: 0.7281\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5699 - accuracy: 0.7249 - val_loss: 0.5798 - val_accuracy: 0.7194\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5666 - accuracy: 0.7305\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5717 - accuracy: 0.7191\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5703 - accuracy: 0.7208\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5708 - accuracy: 0.7214\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5696 - accuracy: 0.7212\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5696 - accuracy: 0.7220\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5693 - accuracy: 0.7212\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5691 - accuracy: 0.7234\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5697 - accuracy: 0.7234\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5685 - accuracy: 0.7216\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5671 - accuracy: 0.7235\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5750 - accuracy: 0.7303\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6513 - accuracy: 0.6490 - val_loss: 0.6139 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.6199 - accuracy: 0.6518 - val_loss: 0.6022 - val_accuracy: 0.6883\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.6000 - accuracy: 0.6819 - val_loss: 0.5852 - val_accuracy: 0.7040\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5946 - accuracy: 0.6914 - val_loss: 0.5813 - val_accuracy: 0.7097\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5853 - accuracy: 0.7027 - val_loss: 0.5788 - val_accuracy: 0.7127\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5842 - accuracy: 0.7063 - val_loss: 0.5783 - val_accuracy: 0.7120\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5847 - accuracy: 0.7073 - val_loss: 0.5820 - val_accuracy: 0.7182\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5792 - accuracy: 0.7116 - val_loss: 0.5717 - val_accuracy: 0.7106\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5795 - accuracy: 0.7034 - val_loss: 0.5748 - val_accuracy: 0.7157\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5789 - accuracy: 0.7076 - val_loss: 0.5737 - val_accuracy: 0.7165\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5787 - accuracy: 0.7068 - val_loss: 0.5848 - val_accuracy: 0.7077\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5812 - accuracy: 0.7077 - val_loss: 0.5661 - val_accuracy: 0.7218\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5728 - accuracy: 0.7126 - val_loss: 0.5658 - val_accuracy: 0.7113\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5777 - accuracy: 0.7104 - val_loss: 0.5736 - val_accuracy: 0.7151\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5759 - accuracy: 0.7125 - val_loss: 0.5746 - val_accuracy: 0.7203\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5757 - accuracy: 0.7114 - val_loss: 0.5653 - val_accuracy: 0.7233\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5785 - accuracy: 0.7085 - val_loss: 0.5648 - val_accuracy: 0.7190\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5734 - accuracy: 0.7138 - val_loss: 0.5644 - val_accuracy: 0.7224\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.7109 - val_loss: 0.5649 - val_accuracy: 0.7201\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5750 - accuracy: 0.7127 - val_loss: 0.5643 - val_accuracy: 0.7151\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5747 - accuracy: 0.7122 - val_loss: 0.5776 - val_accuracy: 0.7041\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5716 - accuracy: 0.7110 - val_loss: 0.5711 - val_accuracy: 0.7254\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5694 - accuracy: 0.7164 - val_loss: 0.5724 - val_accuracy: 0.7070\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5718 - accuracy: 0.7138 - val_loss: 0.5665 - val_accuracy: 0.7170\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7126 - val_loss: 0.5713 - val_accuracy: 0.7209\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5717 - accuracy: 0.7135 - val_loss: 0.5706 - val_accuracy: 0.7214\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5697 - accuracy: 0.7145 - val_loss: 0.5720 - val_accuracy: 0.7164\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5724 - accuracy: 0.7120 - val_loss: 0.5747 - val_accuracy: 0.7199\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5716 - accuracy: 0.7143 - val_loss: 0.5676 - val_accuracy: 0.7181\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5737 - accuracy: 0.7133 - val_loss: 0.5764 - val_accuracy: 0.7055\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5714 - accuracy: 0.7108 - val_loss: 0.5678 - val_accuracy: 0.7114\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5698 - accuracy: 0.7166 - val_loss: 0.5694 - val_accuracy: 0.7171\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5772 - accuracy: 0.7110 - val_loss: 0.5624 - val_accuracy: 0.7308\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7111 - val_loss: 0.5623 - val_accuracy: 0.7247\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7170 - val_loss: 0.5636 - val_accuracy: 0.7245\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5704 - accuracy: 0.7173 - val_loss: 0.5734 - val_accuracy: 0.7231\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5726 - accuracy: 0.7120 - val_loss: 0.5659 - val_accuracy: 0.7174\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7113 - val_loss: 0.5626 - val_accuracy: 0.7240\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5683 - accuracy: 0.7178 - val_loss: 0.5655 - val_accuracy: 0.7174\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.7110 - val_loss: 0.5619 - val_accuracy: 0.7235\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5725 - accuracy: 0.7167 - val_loss: 0.5620 - val_accuracy: 0.7276\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5685 - accuracy: 0.7189 - val_loss: 0.5733 - val_accuracy: 0.7115\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5724 - accuracy: 0.7144 - val_loss: 0.5616 - val_accuracy: 0.7311\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5669 - accuracy: 0.7204 - val_loss: 0.5633 - val_accuracy: 0.7253\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5720 - accuracy: 0.7126 - val_loss: 0.5652 - val_accuracy: 0.7245\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5690 - accuracy: 0.7197 - val_loss: 0.5709 - val_accuracy: 0.7100\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5701 - accuracy: 0.7179 - val_loss: 0.5629 - val_accuracy: 0.7297\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5723 - accuracy: 0.7169 - val_loss: 0.5650 - val_accuracy: 0.7220\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5705 - accuracy: 0.7155 - val_loss: 0.5654 - val_accuracy: 0.7276\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5752 - accuracy: 0.7118 - val_loss: 0.5696 - val_accuracy: 0.7243\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5720 - accuracy: 0.7244\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5701 - accuracy: 0.7178\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5705 - accuracy: 0.7168\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5700 - accuracy: 0.7192\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5716 - accuracy: 0.7159\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5696 - accuracy: 0.7183\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5697 - accuracy: 0.7177\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5694 - accuracy: 0.7182\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5682 - accuracy: 0.7206\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5706 - accuracy: 0.7183\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5696 - accuracy: 0.7186\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.5592 - accuracy: 0.7316\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6530 - accuracy: 0.6487 - val_loss: 0.6306 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6233 - accuracy: 0.6531 - val_loss: 0.6280 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6078 - accuracy: 0.6518 - val_loss: 0.6122 - val_accuracy: 0.6592\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6023 - accuracy: 0.6607 - val_loss: 0.6220 - val_accuracy: 0.6918\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5959 - accuracy: 0.6901 - val_loss: 0.6049 - val_accuracy: 0.7099\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5906 - accuracy: 0.6969 - val_loss: 0.6047 - val_accuracy: 0.7041\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5827 - accuracy: 0.7086 - val_loss: 0.5940 - val_accuracy: 0.7192\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5884 - accuracy: 0.6993 - val_loss: 0.5782 - val_accuracy: 0.7230\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5859 - accuracy: 0.7077 - val_loss: 0.5929 - val_accuracy: 0.6911\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5823 - accuracy: 0.7076 - val_loss: 0.6039 - val_accuracy: 0.7159\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5824 - accuracy: 0.7082 - val_loss: 0.5871 - val_accuracy: 0.7148\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5814 - accuracy: 0.7067 - val_loss: 0.5784 - val_accuracy: 0.7078\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5778 - accuracy: 0.7098 - val_loss: 0.5796 - val_accuracy: 0.7196\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5787 - accuracy: 0.7122 - val_loss: 0.5809 - val_accuracy: 0.7121\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5793 - accuracy: 0.7097 - val_loss: 0.5948 - val_accuracy: 0.7136\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5786 - accuracy: 0.7139 - val_loss: 0.5770 - val_accuracy: 0.7121\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5799 - accuracy: 0.7090 - val_loss: 0.5862 - val_accuracy: 0.7200\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5763 - accuracy: 0.7120 - val_loss: 0.5810 - val_accuracy: 0.7183\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5803 - accuracy: 0.7104 - val_loss: 0.5756 - val_accuracy: 0.7221\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.7138 - val_loss: 0.5831 - val_accuracy: 0.7237\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5776 - accuracy: 0.7115 - val_loss: 0.5714 - val_accuracy: 0.7222\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5704 - accuracy: 0.7166 - val_loss: 0.5754 - val_accuracy: 0.7224\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5738 - accuracy: 0.7151 - val_loss: 0.5673 - val_accuracy: 0.7208\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.7131 - val_loss: 0.5895 - val_accuracy: 0.7141\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5753 - accuracy: 0.7150 - val_loss: 0.5721 - val_accuracy: 0.7221\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5746 - accuracy: 0.7178 - val_loss: 0.5742 - val_accuracy: 0.7248\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5722 - accuracy: 0.7199 - val_loss: 0.5724 - val_accuracy: 0.7200\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7154 - val_loss: 0.5718 - val_accuracy: 0.7134\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5708 - accuracy: 0.7181 - val_loss: 0.5693 - val_accuracy: 0.7191\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5726 - accuracy: 0.7161 - val_loss: 0.5706 - val_accuracy: 0.7148\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5764 - accuracy: 0.7143 - val_loss: 0.5684 - val_accuracy: 0.7282\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5725 - accuracy: 0.7164 - val_loss: 0.5624 - val_accuracy: 0.7250\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5768 - accuracy: 0.7163 - val_loss: 0.5716 - val_accuracy: 0.7090\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5694 - accuracy: 0.7175 - val_loss: 0.5692 - val_accuracy: 0.7254\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5728 - accuracy: 0.7161 - val_loss: 0.5673 - val_accuracy: 0.7211\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5713 - accuracy: 0.7196 - val_loss: 0.5758 - val_accuracy: 0.7239\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7165 - val_loss: 0.5679 - val_accuracy: 0.7214\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5732 - accuracy: 0.7156 - val_loss: 0.5713 - val_accuracy: 0.7165\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5695 - accuracy: 0.7205 - val_loss: 0.5593 - val_accuracy: 0.7274\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5699 - accuracy: 0.7197 - val_loss: 0.5646 - val_accuracy: 0.7297\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5728 - accuracy: 0.7163 - val_loss: 0.5767 - val_accuracy: 0.7192\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5713 - accuracy: 0.7190 - val_loss: 0.5632 - val_accuracy: 0.7304\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7160 - val_loss: 0.5684 - val_accuracy: 0.7239\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5694 - accuracy: 0.7210 - val_loss: 0.5712 - val_accuracy: 0.7317\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5692 - accuracy: 0.7226 - val_loss: 0.5646 - val_accuracy: 0.7296\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5737 - accuracy: 0.7196 - val_loss: 0.5664 - val_accuracy: 0.7224\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5683 - accuracy: 0.7216 - val_loss: 0.5699 - val_accuracy: 0.7239\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5709 - accuracy: 0.7173 - val_loss: 0.5704 - val_accuracy: 0.7294\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5666 - accuracy: 0.7249 - val_loss: 0.5679 - val_accuracy: 0.7257\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5689 - accuracy: 0.7195 - val_loss: 0.5709 - val_accuracy: 0.7255\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5733 - accuracy: 0.7262\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5696 - accuracy: 0.7207\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5710 - accuracy: 0.7170\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5681 - accuracy: 0.7229\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5710 - accuracy: 0.7184\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5692 - accuracy: 0.7215\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5680 - accuracy: 0.7217\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5689 - accuracy: 0.7221\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5685 - accuracy: 0.7213\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5681 - accuracy: 0.7222\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5687 - accuracy: 0.7229\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5675 - accuracy: 0.7340\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6516 - accuracy: 0.6516 - val_loss: 0.6161 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6215 - accuracy: 0.6485 - val_loss: 0.6127 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6060 - accuracy: 0.6552 - val_loss: 0.6131 - val_accuracy: 0.6980\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5975 - accuracy: 0.6818 - val_loss: 0.5945 - val_accuracy: 0.7039\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5958 - accuracy: 0.6972 - val_loss: 0.5908 - val_accuracy: 0.7095\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5945 - accuracy: 0.6973 - val_loss: 0.5880 - val_accuracy: 0.7116\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5885 - accuracy: 0.6988 - val_loss: 0.5892 - val_accuracy: 0.7124\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5863 - accuracy: 0.7028 - val_loss: 0.5870 - val_accuracy: 0.7085\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5844 - accuracy: 0.7054 - val_loss: 0.5831 - val_accuracy: 0.7054\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5822 - accuracy: 0.7065 - val_loss: 0.5751 - val_accuracy: 0.7193\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5832 - accuracy: 0.7081 - val_loss: 0.5786 - val_accuracy: 0.7159\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5802 - accuracy: 0.7115 - val_loss: 0.5904 - val_accuracy: 0.7019\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5841 - accuracy: 0.7081 - val_loss: 0.6027 - val_accuracy: 0.6882\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5849 - accuracy: 0.7052 - val_loss: 0.5810 - val_accuracy: 0.7096\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5792 - accuracy: 0.7152 - val_loss: 0.5694 - val_accuracy: 0.7237\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5774 - accuracy: 0.7121 - val_loss: 0.5779 - val_accuracy: 0.7184\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5807 - accuracy: 0.7109 - val_loss: 0.5812 - val_accuracy: 0.7142\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5818 - accuracy: 0.7098 - val_loss: 0.5787 - val_accuracy: 0.6996\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5777 - accuracy: 0.7143 - val_loss: 0.5697 - val_accuracy: 0.7234\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5801 - accuracy: 0.7101 - val_loss: 0.5723 - val_accuracy: 0.7226\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5771 - accuracy: 0.7130 - val_loss: 0.5857 - val_accuracy: 0.7016\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5736 - accuracy: 0.7149 - val_loss: 0.5730 - val_accuracy: 0.7061\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5772 - accuracy: 0.7094 - val_loss: 0.5633 - val_accuracy: 0.7232\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5753 - accuracy: 0.7147 - val_loss: 0.5754 - val_accuracy: 0.7187\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5745 - accuracy: 0.7130 - val_loss: 0.5678 - val_accuracy: 0.7312\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5770 - accuracy: 0.7115 - val_loss: 0.5717 - val_accuracy: 0.7093\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5732 - accuracy: 0.7153 - val_loss: 0.5755 - val_accuracy: 0.7285\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5762 - accuracy: 0.7151 - val_loss: 0.5689 - val_accuracy: 0.7235\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5713 - accuracy: 0.7187 - val_loss: 0.5624 - val_accuracy: 0.7239\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7132 - val_loss: 0.5740 - val_accuracy: 0.7155\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7167 - val_loss: 0.5659 - val_accuracy: 0.7248\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7160 - val_loss: 0.5674 - val_accuracy: 0.7213\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5720 - accuracy: 0.7186 - val_loss: 0.5663 - val_accuracy: 0.7196\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5728 - accuracy: 0.7210 - val_loss: 0.5730 - val_accuracy: 0.7146\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5723 - accuracy: 0.7173 - val_loss: 0.5687 - val_accuracy: 0.7191\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5726 - accuracy: 0.7191 - val_loss: 0.5710 - val_accuracy: 0.7308\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7159 - val_loss: 0.5646 - val_accuracy: 0.7273\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5703 - accuracy: 0.7198 - val_loss: 0.5634 - val_accuracy: 0.7300\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5718 - accuracy: 0.7178 - val_loss: 0.5649 - val_accuracy: 0.7189\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5728 - accuracy: 0.7156 - val_loss: 0.5670 - val_accuracy: 0.7211\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5698 - accuracy: 0.7281\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5752 - accuracy: 0.7144\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5742 - accuracy: 0.7143\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5744 - accuracy: 0.7143\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5738 - accuracy: 0.7152\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5730 - accuracy: 0.7179\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5731 - accuracy: 0.7174\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5721 - accuracy: 0.7169\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5730 - accuracy: 0.7162\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5731 - accuracy: 0.7157\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5712 - accuracy: 0.7178\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5641 - accuracy: 0.7336\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6540 - accuracy: 0.6443 - val_loss: 0.6134 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6131 - accuracy: 0.6562 - val_loss: 0.6035 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5967 - accuracy: 0.6787 - val_loss: 0.6048 - val_accuracy: 0.6972\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5973 - accuracy: 0.6962 - val_loss: 0.5901 - val_accuracy: 0.7146\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5906 - accuracy: 0.7026 - val_loss: 0.5801 - val_accuracy: 0.7136\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5873 - accuracy: 0.7054 - val_loss: 0.5931 - val_accuracy: 0.7041\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.7043 - val_loss: 0.5724 - val_accuracy: 0.7176\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5855 - accuracy: 0.7021 - val_loss: 0.5770 - val_accuracy: 0.7129\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5806 - accuracy: 0.7108 - val_loss: 0.5719 - val_accuracy: 0.7189\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5815 - accuracy: 0.7095 - val_loss: 0.5808 - val_accuracy: 0.7165\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5785 - accuracy: 0.7109 - val_loss: 0.5856 - val_accuracy: 0.7091\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5824 - accuracy: 0.7045 - val_loss: 0.5807 - val_accuracy: 0.7132\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5807 - accuracy: 0.7074 - val_loss: 0.5791 - val_accuracy: 0.7123\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5771 - accuracy: 0.7137 - val_loss: 0.5729 - val_accuracy: 0.7198\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5787 - accuracy: 0.7112 - val_loss: 0.5794 - val_accuracy: 0.7215\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5787 - accuracy: 0.7088 - val_loss: 0.5742 - val_accuracy: 0.7225\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5795 - accuracy: 0.7139 - val_loss: 0.5843 - val_accuracy: 0.7064\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5801 - accuracy: 0.7087 - val_loss: 0.5843 - val_accuracy: 0.7107\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5806 - accuracy: 0.7109 - val_loss: 0.5850 - val_accuracy: 0.7091\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5772 - accuracy: 0.7106 - val_loss: 0.5757 - val_accuracy: 0.7167\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5755 - accuracy: 0.7168 - val_loss: 0.5890 - val_accuracy: 0.7114\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5763 - accuracy: 0.7146 - val_loss: 0.5728 - val_accuracy: 0.7207\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5775 - accuracy: 0.7124 - val_loss: 0.5855 - val_accuracy: 0.7105\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5797 - accuracy: 0.7096 - val_loss: 0.5840 - val_accuracy: 0.7205\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5715 - accuracy: 0.7177 - val_loss: 0.5795 - val_accuracy: 0.7107\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5784 - accuracy: 0.7120 - val_loss: 0.5833 - val_accuracy: 0.7052\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7161 - val_loss: 0.5976 - val_accuracy: 0.6954\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5767 - accuracy: 0.7105 - val_loss: 0.5797 - val_accuracy: 0.7258\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5742 - accuracy: 0.7152 - val_loss: 0.5705 - val_accuracy: 0.7238\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.7134 - val_loss: 0.5916 - val_accuracy: 0.7079\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7128 - val_loss: 0.5715 - val_accuracy: 0.7231\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.7124 - val_loss: 0.5819 - val_accuracy: 0.7167\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5738 - accuracy: 0.7145 - val_loss: 0.5686 - val_accuracy: 0.7160\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7108 - val_loss: 0.5713 - val_accuracy: 0.7171\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5708 - accuracy: 0.7149 - val_loss: 0.5724 - val_accuracy: 0.7166\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5725 - accuracy: 0.7158 - val_loss: 0.5715 - val_accuracy: 0.7143\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5697 - accuracy: 0.7151 - val_loss: 0.5801 - val_accuracy: 0.7197\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5752 - accuracy: 0.7139 - val_loss: 0.5744 - val_accuracy: 0.7052\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5740 - accuracy: 0.7166 - val_loss: 0.5707 - val_accuracy: 0.7190\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5716 - accuracy: 0.7162 - val_loss: 0.5741 - val_accuracy: 0.7129\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5730 - accuracy: 0.7122 - val_loss: 0.5789 - val_accuracy: 0.7243\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5706 - accuracy: 0.7165 - val_loss: 0.5849 - val_accuracy: 0.7080\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5759 - accuracy: 0.7092 - val_loss: 0.5825 - val_accuracy: 0.7240\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5812 - accuracy: 0.7242\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5755 - accuracy: 0.7117\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5742 - accuracy: 0.7122\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5750 - accuracy: 0.7114\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5736 - accuracy: 0.7127\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5739 - accuracy: 0.7122\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5738 - accuracy: 0.7123\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5726 - accuracy: 0.7152\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5727 - accuracy: 0.7140\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5723 - accuracy: 0.7154\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5715 - accuracy: 0.7168\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.5725 - accuracy: 0.7232\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8eXFWsloQPJ",
        "outputId": "f1e231ad-6dd5-4f38-e8da-41cc2be96d0f"
      },
      "source": [
        "print(round(cv_results_2.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7305\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oE2hx6iZoQPJ",
        "outputId": "9130a040-0f4e-4600-ea6d-9d3e3a05dcff"
      },
      "source": [
        "print(round(cv_results_2.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ki8paVnoQPK",
        "outputId": "bbc56a1f-c64d-42ac-f323-582b8f8c2615"
      },
      "source": [
        "cv2_preds = cross_val_predict(mod2, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6548 - accuracy: 0.6422 - val_loss: 0.6108 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6146 - accuracy: 0.6535 - val_loss: 0.6344 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5982 - accuracy: 0.6723 - val_loss: 0.5947 - val_accuracy: 0.7041\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5957 - accuracy: 0.6898 - val_loss: 0.5951 - val_accuracy: 0.6970\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5880 - accuracy: 0.7033 - val_loss: 0.6000 - val_accuracy: 0.7125\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5861 - accuracy: 0.7094 - val_loss: 0.5888 - val_accuracy: 0.7166\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5877 - accuracy: 0.7027 - val_loss: 0.5815 - val_accuracy: 0.7157\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5865 - accuracy: 0.7058 - val_loss: 0.5935 - val_accuracy: 0.6880\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.7058 - val_loss: 0.5771 - val_accuracy: 0.7174\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5802 - accuracy: 0.7110 - val_loss: 0.5803 - val_accuracy: 0.7184\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5810 - accuracy: 0.7084 - val_loss: 0.5844 - val_accuracy: 0.7062\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5807 - accuracy: 0.7098 - val_loss: 0.5887 - val_accuracy: 0.7158\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5798 - accuracy: 0.7115 - val_loss: 0.5878 - val_accuracy: 0.7000\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5821 - accuracy: 0.7095 - val_loss: 0.5777 - val_accuracy: 0.7182\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5793 - accuracy: 0.7102 - val_loss: 0.5774 - val_accuracy: 0.7155\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5786 - accuracy: 0.7110 - val_loss: 0.5776 - val_accuracy: 0.7095\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5785 - accuracy: 0.7115 - val_loss: 0.5799 - val_accuracy: 0.7152\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5768 - accuracy: 0.7101 - val_loss: 0.5787 - val_accuracy: 0.7136\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5786 - accuracy: 0.7116 - val_loss: 0.5846 - val_accuracy: 0.7038\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5761 - accuracy: 0.7148 - val_loss: 0.5814 - val_accuracy: 0.7228\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5805 - accuracy: 0.7086 - val_loss: 0.5766 - val_accuracy: 0.7245\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5741 - accuracy: 0.7160 - val_loss: 0.5780 - val_accuracy: 0.7168\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5775 - accuracy: 0.7125 - val_loss: 0.5722 - val_accuracy: 0.7239\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5804 - accuracy: 0.7096 - val_loss: 0.5857 - val_accuracy: 0.6968\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5756 - accuracy: 0.7150 - val_loss: 0.5795 - val_accuracy: 0.7123\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5789 - accuracy: 0.7133 - val_loss: 0.5705 - val_accuracy: 0.7153\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5730 - accuracy: 0.7164 - val_loss: 0.5711 - val_accuracy: 0.7216\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7138 - val_loss: 0.5794 - val_accuracy: 0.7157\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7139 - val_loss: 0.5684 - val_accuracy: 0.7109\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5801 - accuracy: 0.7095 - val_loss: 0.5664 - val_accuracy: 0.7256\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5681 - accuracy: 0.7204 - val_loss: 0.5733 - val_accuracy: 0.7242\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5727 - accuracy: 0.7200 - val_loss: 0.5695 - val_accuracy: 0.7162\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5722 - accuracy: 0.7176 - val_loss: 0.5707 - val_accuracy: 0.7232\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5738 - accuracy: 0.7175 - val_loss: 0.5745 - val_accuracy: 0.7190\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7147 - val_loss: 0.5709 - val_accuracy: 0.7263\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5765 - accuracy: 0.7111 - val_loss: 0.5704 - val_accuracy: 0.7148\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5751 - accuracy: 0.7141 - val_loss: 0.5687 - val_accuracy: 0.7242\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5701 - accuracy: 0.7191 - val_loss: 0.5631 - val_accuracy: 0.7238\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5757 - accuracy: 0.7167 - val_loss: 0.5734 - val_accuracy: 0.7166\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5708 - accuracy: 0.7182 - val_loss: 0.5696 - val_accuracy: 0.7257\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.7114 - val_loss: 0.5688 - val_accuracy: 0.7191\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5714 - accuracy: 0.7172 - val_loss: 0.5799 - val_accuracy: 0.7159\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5752 - accuracy: 0.7113 - val_loss: 0.5696 - val_accuracy: 0.7132\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5738 - accuracy: 0.7146 - val_loss: 0.5741 - val_accuracy: 0.7145\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5711 - accuracy: 0.7179 - val_loss: 0.5717 - val_accuracy: 0.7147\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.7151 - val_loss: 0.5690 - val_accuracy: 0.7233\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5685 - accuracy: 0.7208 - val_loss: 0.5676 - val_accuracy: 0.7255\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.7173 - val_loss: 0.5710 - val_accuracy: 0.7226\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5701 - accuracy: 0.7191 - val_loss: 0.5658 - val_accuracy: 0.7265\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5693 - accuracy: 0.7167 - val_loss: 0.5783 - val_accuracy: 0.7231\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5796 - accuracy: 0.7229\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7176\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5714 - accuracy: 0.7152\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5720 - accuracy: 0.7164\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5710 - accuracy: 0.7183\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5714 - accuracy: 0.7186\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7168\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5707 - accuracy: 0.7178\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5704 - accuracy: 0.7204\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5700 - accuracy: 0.7207\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5693 - accuracy: 0.7211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6572 - accuracy: 0.6367 - val_loss: 0.6126 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.6193 - accuracy: 0.6540 - val_loss: 0.6181 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.6006 - accuracy: 0.6610 - val_loss: 0.6018 - val_accuracy: 0.7081\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5885 - accuracy: 0.7016 - val_loss: 0.6041 - val_accuracy: 0.7224\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5879 - accuracy: 0.7054 - val_loss: 0.5858 - val_accuracy: 0.7179\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5886 - accuracy: 0.7034 - val_loss: 0.5828 - val_accuracy: 0.6998\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5852 - accuracy: 0.7094 - val_loss: 0.5801 - val_accuracy: 0.7002\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5857 - accuracy: 0.7059 - val_loss: 0.5886 - val_accuracy: 0.7073\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5829 - accuracy: 0.7079 - val_loss: 0.5927 - val_accuracy: 0.7027\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5799 - accuracy: 0.7123 - val_loss: 0.5787 - val_accuracy: 0.7195\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5800 - accuracy: 0.7088 - val_loss: 0.5860 - val_accuracy: 0.7158\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5810 - accuracy: 0.7098 - val_loss: 0.5797 - val_accuracy: 0.7182\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5781 - accuracy: 0.7105 - val_loss: 0.5905 - val_accuracy: 0.6931\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5773 - accuracy: 0.7114 - val_loss: 0.5812 - val_accuracy: 0.7238\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5806 - accuracy: 0.7113 - val_loss: 0.5783 - val_accuracy: 0.7236\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5785 - accuracy: 0.7120 - val_loss: 0.5757 - val_accuracy: 0.7164\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5778 - accuracy: 0.7125 - val_loss: 0.5812 - val_accuracy: 0.7224\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5773 - accuracy: 0.7130 - val_loss: 0.5782 - val_accuracy: 0.7186\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5740 - accuracy: 0.7144 - val_loss: 0.5737 - val_accuracy: 0.7247\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5700 - accuracy: 0.7189 - val_loss: 0.5813 - val_accuracy: 0.7223\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5762 - accuracy: 0.7134 - val_loss: 0.5775 - val_accuracy: 0.7223\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5724 - accuracy: 0.7169 - val_loss: 0.5770 - val_accuracy: 0.7106\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5766 - accuracy: 0.7156 - val_loss: 0.5755 - val_accuracy: 0.7171\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5766 - accuracy: 0.7111 - val_loss: 0.5820 - val_accuracy: 0.7065\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5734 - accuracy: 0.7133 - val_loss: 0.5700 - val_accuracy: 0.7220\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5736 - accuracy: 0.7150 - val_loss: 0.5689 - val_accuracy: 0.7257\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5740 - accuracy: 0.7165 - val_loss: 0.5713 - val_accuracy: 0.7149\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7184 - val_loss: 0.5654 - val_accuracy: 0.7232\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5714 - accuracy: 0.7198 - val_loss: 0.5743 - val_accuracy: 0.7096\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5723 - accuracy: 0.7179 - val_loss: 0.5775 - val_accuracy: 0.7216\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5724 - accuracy: 0.7191 - val_loss: 0.5755 - val_accuracy: 0.7211\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5726 - accuracy: 0.7166 - val_loss: 0.5658 - val_accuracy: 0.7304\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5701 - accuracy: 0.7221 - val_loss: 0.5693 - val_accuracy: 0.7184\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5718 - accuracy: 0.7205 - val_loss: 0.5841 - val_accuracy: 0.7298\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5696 - accuracy: 0.7217 - val_loss: 0.5711 - val_accuracy: 0.7174\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5726 - accuracy: 0.7186 - val_loss: 0.5739 - val_accuracy: 0.7228\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5746 - accuracy: 0.7160 - val_loss: 0.5733 - val_accuracy: 0.7201\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5666 - accuracy: 0.7250 - val_loss: 0.5820 - val_accuracy: 0.7295\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5685 - accuracy: 0.7243 - val_loss: 0.5675 - val_accuracy: 0.7291\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5697 - accuracy: 0.7222 - val_loss: 0.5702 - val_accuracy: 0.7329\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5688 - accuracy: 0.7214 - val_loss: 0.5935 - val_accuracy: 0.7001\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5697 - accuracy: 0.7217 - val_loss: 0.5743 - val_accuracy: 0.7167\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5672 - accuracy: 0.7222 - val_loss: 0.5767 - val_accuracy: 0.7289\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5673 - accuracy: 0.7246 - val_loss: 0.5815 - val_accuracy: 0.7283\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5666 - accuracy: 0.7243 - val_loss: 0.5754 - val_accuracy: 0.7272\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5708 - accuracy: 0.7195 - val_loss: 0.5777 - val_accuracy: 0.7262\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5689 - accuracy: 0.7212 - val_loss: 0.5941 - val_accuracy: 0.7096\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5673 - accuracy: 0.7229 - val_loss: 0.5813 - val_accuracy: 0.7286\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5681 - accuracy: 0.7244 - val_loss: 0.5746 - val_accuracy: 0.7212\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5701 - accuracy: 0.7206 - val_loss: 0.5812 - val_accuracy: 0.7264\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5823 - accuracy: 0.7275\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5679 - accuracy: 0.7232\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5671 - accuracy: 0.7245\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5690 - accuracy: 0.7212\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5679 - accuracy: 0.7207\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5673 - accuracy: 0.7238\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5683 - accuracy: 0.7230\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5668 - accuracy: 0.7237\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5672 - accuracy: 0.7224\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5669 - accuracy: 0.7230\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5669 - accuracy: 0.7227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6539 - accuracy: 0.6502 - val_loss: 0.6167 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6229 - accuracy: 0.6515 - val_loss: 0.6091 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6110 - accuracy: 0.6537 - val_loss: 0.6123 - val_accuracy: 0.6847\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6014 - accuracy: 0.6892 - val_loss: 0.5958 - val_accuracy: 0.6980\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5915 - accuracy: 0.6991 - val_loss: 0.6018 - val_accuracy: 0.7065\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5871 - accuracy: 0.7029 - val_loss: 0.5853 - val_accuracy: 0.7199\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5818 - accuracy: 0.7065 - val_loss: 0.5788 - val_accuracy: 0.7171\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5847 - accuracy: 0.7061 - val_loss: 0.5762 - val_accuracy: 0.7198\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5814 - accuracy: 0.7057 - val_loss: 0.5880 - val_accuracy: 0.7094\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5810 - accuracy: 0.7056 - val_loss: 0.5712 - val_accuracy: 0.7223\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5768 - accuracy: 0.7125 - val_loss: 0.5758 - val_accuracy: 0.7149\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5782 - accuracy: 0.7086 - val_loss: 0.5793 - val_accuracy: 0.7084\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5740 - accuracy: 0.7118 - val_loss: 0.5773 - val_accuracy: 0.7059\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7140 - val_loss: 0.5780 - val_accuracy: 0.6996\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5787 - accuracy: 0.7058 - val_loss: 0.5707 - val_accuracy: 0.7201\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5756 - accuracy: 0.7095 - val_loss: 0.5767 - val_accuracy: 0.7232\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5780 - accuracy: 0.7097 - val_loss: 0.5692 - val_accuracy: 0.7211\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7122 - val_loss: 0.5811 - val_accuracy: 0.7242\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5777 - accuracy: 0.7103 - val_loss: 0.5672 - val_accuracy: 0.7155\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5699 - accuracy: 0.7150 - val_loss: 0.5697 - val_accuracy: 0.7197\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7127 - val_loss: 0.5655 - val_accuracy: 0.7230\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5712 - accuracy: 0.7157 - val_loss: 0.5872 - val_accuracy: 0.7063\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5737 - accuracy: 0.7138 - val_loss: 0.5685 - val_accuracy: 0.7266\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5684 - accuracy: 0.7214 - val_loss: 0.5710 - val_accuracy: 0.7287\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5738 - accuracy: 0.7138 - val_loss: 0.5647 - val_accuracy: 0.7269\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5719 - accuracy: 0.7152 - val_loss: 0.5772 - val_accuracy: 0.7112\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5711 - accuracy: 0.7156 - val_loss: 0.5663 - val_accuracy: 0.7181\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5736 - accuracy: 0.7142 - val_loss: 0.5664 - val_accuracy: 0.7258\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7109 - val_loss: 0.5669 - val_accuracy: 0.7234\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5719 - accuracy: 0.7182 - val_loss: 0.5678 - val_accuracy: 0.7236\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5741 - accuracy: 0.7141 - val_loss: 0.5833 - val_accuracy: 0.6963\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5725 - accuracy: 0.7154 - val_loss: 0.5693 - val_accuracy: 0.7157\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5721 - accuracy: 0.7144 - val_loss: 0.5602 - val_accuracy: 0.7307\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5680 - accuracy: 0.7206 - val_loss: 0.5862 - val_accuracy: 0.7145\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5749 - accuracy: 0.7149 - val_loss: 0.5774 - val_accuracy: 0.7198\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5725 - accuracy: 0.7160 - val_loss: 0.5618 - val_accuracy: 0.7282\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5711 - accuracy: 0.7180 - val_loss: 0.5634 - val_accuracy: 0.7302\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5721 - accuracy: 0.7176 - val_loss: 0.5661 - val_accuracy: 0.7287\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5718 - accuracy: 0.7158 - val_loss: 0.5644 - val_accuracy: 0.7307\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5663 - accuracy: 0.7218 - val_loss: 0.5789 - val_accuracy: 0.7179\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5715 - accuracy: 0.7172 - val_loss: 0.5628 - val_accuracy: 0.7305\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5693 - accuracy: 0.7206 - val_loss: 0.5623 - val_accuracy: 0.7318\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5712 - accuracy: 0.7183 - val_loss: 0.5596 - val_accuracy: 0.7328\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5708 - accuracy: 0.7159 - val_loss: 0.5596 - val_accuracy: 0.7353\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5709 - accuracy: 0.7181 - val_loss: 0.5654 - val_accuracy: 0.7257\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5699 - accuracy: 0.7191 - val_loss: 0.5797 - val_accuracy: 0.7106\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5715 - accuracy: 0.7172 - val_loss: 0.5659 - val_accuracy: 0.7210\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5678 - accuracy: 0.7222 - val_loss: 0.5617 - val_accuracy: 0.7329\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5675 - accuracy: 0.7195 - val_loss: 0.5643 - val_accuracy: 0.7308\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5664 - accuracy: 0.7230 - val_loss: 0.5680 - val_accuracy: 0.7268\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5710 - accuracy: 0.7256\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5675 - accuracy: 0.7207\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5683 - accuracy: 0.7195\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5662 - accuracy: 0.7228\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5675 - accuracy: 0.7228\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5666 - accuracy: 0.7228\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5655 - accuracy: 0.7244\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5665 - accuracy: 0.7212\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5683 - accuracy: 0.7216\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5666 - accuracy: 0.7224\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5660 - accuracy: 0.7237\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6513 - accuracy: 0.6546 - val_loss: 0.6189 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6194 - accuracy: 0.6536 - val_loss: 0.6064 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6039 - accuracy: 0.6557 - val_loss: 0.6072 - val_accuracy: 0.6957\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5979 - accuracy: 0.6908 - val_loss: 0.5895 - val_accuracy: 0.6964\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5897 - accuracy: 0.6995 - val_loss: 0.5859 - val_accuracy: 0.7115\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5855 - accuracy: 0.7032 - val_loss: 0.5882 - val_accuracy: 0.7168\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5838 - accuracy: 0.7077 - val_loss: 0.5912 - val_accuracy: 0.7035\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5869 - accuracy: 0.7019 - val_loss: 0.5775 - val_accuracy: 0.7187\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5844 - accuracy: 0.7022 - val_loss: 0.5827 - val_accuracy: 0.7025\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5805 - accuracy: 0.7083 - val_loss: 0.5735 - val_accuracy: 0.7211\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5806 - accuracy: 0.7068 - val_loss: 0.5773 - val_accuracy: 0.7143\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5804 - accuracy: 0.7085 - val_loss: 0.5825 - val_accuracy: 0.7189\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5773 - accuracy: 0.7134 - val_loss: 0.5761 - val_accuracy: 0.7170\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5799 - accuracy: 0.7126 - val_loss: 0.5740 - val_accuracy: 0.7154\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5766 - accuracy: 0.7094 - val_loss: 0.5828 - val_accuracy: 0.7190\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5780 - accuracy: 0.7108 - val_loss: 0.5724 - val_accuracy: 0.7219\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7163 - val_loss: 0.5845 - val_accuracy: 0.7026\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5757 - accuracy: 0.7098 - val_loss: 0.5751 - val_accuracy: 0.7218\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5756 - accuracy: 0.7149 - val_loss: 0.5744 - val_accuracy: 0.7170\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5779 - accuracy: 0.7134 - val_loss: 0.5814 - val_accuracy: 0.7034\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5750 - accuracy: 0.7127 - val_loss: 0.5854 - val_accuracy: 0.7173\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5728 - accuracy: 0.7189 - val_loss: 0.5709 - val_accuracy: 0.7210\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5742 - accuracy: 0.7174 - val_loss: 0.5785 - val_accuracy: 0.7179\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5749 - accuracy: 0.7172 - val_loss: 0.5636 - val_accuracy: 0.7276\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5762 - accuracy: 0.7116 - val_loss: 0.5793 - val_accuracy: 0.7060\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7139 - val_loss: 0.5779 - val_accuracy: 0.7130\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5742 - accuracy: 0.7184 - val_loss: 0.5677 - val_accuracy: 0.7160\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5722 - accuracy: 0.7171 - val_loss: 0.5641 - val_accuracy: 0.7243\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5751 - accuracy: 0.7116 - val_loss: 0.5676 - val_accuracy: 0.7261\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5716 - accuracy: 0.7179 - val_loss: 0.5736 - val_accuracy: 0.7214\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5730 - accuracy: 0.7165 - val_loss: 0.5910 - val_accuracy: 0.7118\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5717 - accuracy: 0.7207 - val_loss: 0.5843 - val_accuracy: 0.7050\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5738 - accuracy: 0.7127 - val_loss: 0.5698 - val_accuracy: 0.7093\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5722 - accuracy: 0.7173 - val_loss: 0.5662 - val_accuracy: 0.7249\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5703 - accuracy: 0.7214 - val_loss: 0.5714 - val_accuracy: 0.7259\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5716 - accuracy: 0.7179 - val_loss: 0.5652 - val_accuracy: 0.7211\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5695 - accuracy: 0.7205 - val_loss: 0.5690 - val_accuracy: 0.7261\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5694 - accuracy: 0.7246 - val_loss: 0.5697 - val_accuracy: 0.7220\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5703 - accuracy: 0.7212 - val_loss: 0.5598 - val_accuracy: 0.7299\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5715 - accuracy: 0.7201 - val_loss: 0.5662 - val_accuracy: 0.7305\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7173 - val_loss: 0.5816 - val_accuracy: 0.7202\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.7160 - val_loss: 0.5664 - val_accuracy: 0.7215\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5688 - accuracy: 0.7240 - val_loss: 0.5671 - val_accuracy: 0.7307\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5705 - accuracy: 0.7198 - val_loss: 0.5744 - val_accuracy: 0.7178\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5734 - accuracy: 0.7195 - val_loss: 0.5617 - val_accuracy: 0.7324\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5691 - accuracy: 0.7204 - val_loss: 0.5634 - val_accuracy: 0.7300\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5655 - accuracy: 0.7261 - val_loss: 0.5677 - val_accuracy: 0.7273\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5677 - accuracy: 0.7215 - val_loss: 0.5685 - val_accuracy: 0.7273\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5656 - accuracy: 0.7257 - val_loss: 0.5663 - val_accuracy: 0.7274\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5668 - accuracy: 0.7239 - val_loss: 0.5649 - val_accuracy: 0.7248\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5675 - accuracy: 0.7228\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5693 - accuracy: 0.7217\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5693 - accuracy: 0.7211\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5705 - accuracy: 0.7196\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5698 - accuracy: 0.7220\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5697 - accuracy: 0.7203\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5700 - accuracy: 0.7220\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5690 - accuracy: 0.7217\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5677 - accuracy: 0.7232\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5681 - accuracy: 0.7215\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5691 - accuracy: 0.7212\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6534 - accuracy: 0.6472 - val_loss: 0.6239 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.6174 - accuracy: 0.6532 - val_loss: 0.6232 - val_accuracy: 0.6592\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6066 - accuracy: 0.6548 - val_loss: 0.6159 - val_accuracy: 0.6912\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5993 - accuracy: 0.6834 - val_loss: 0.6144 - val_accuracy: 0.6847\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5964 - accuracy: 0.6957 - val_loss: 0.6002 - val_accuracy: 0.7045\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5892 - accuracy: 0.6976 - val_loss: 0.6000 - val_accuracy: 0.7102\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5910 - accuracy: 0.6979 - val_loss: 0.5897 - val_accuracy: 0.7077\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5865 - accuracy: 0.7031 - val_loss: 0.5908 - val_accuracy: 0.7115\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5837 - accuracy: 0.7101 - val_loss: 0.5825 - val_accuracy: 0.7163\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5811 - accuracy: 0.7101 - val_loss: 0.5954 - val_accuracy: 0.7070\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5829 - accuracy: 0.7068 - val_loss: 0.5857 - val_accuracy: 0.7053\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5781 - accuracy: 0.7144 - val_loss: 0.5768 - val_accuracy: 0.7136\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5815 - accuracy: 0.7082 - val_loss: 0.5781 - val_accuracy: 0.7186\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5820 - accuracy: 0.7091 - val_loss: 0.5708 - val_accuracy: 0.7194\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5820 - accuracy: 0.7077 - val_loss: 0.5846 - val_accuracy: 0.7223\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5801 - accuracy: 0.7120 - val_loss: 0.5899 - val_accuracy: 0.6895\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.7131 - val_loss: 0.5732 - val_accuracy: 0.7193\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5789 - accuracy: 0.7109 - val_loss: 0.5743 - val_accuracy: 0.7145\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5795 - accuracy: 0.7112 - val_loss: 0.5891 - val_accuracy: 0.7116\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5770 - accuracy: 0.7131 - val_loss: 0.5827 - val_accuracy: 0.7077\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5788 - accuracy: 0.7147 - val_loss: 0.5829 - val_accuracy: 0.7224\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5776 - accuracy: 0.7122 - val_loss: 0.5692 - val_accuracy: 0.7236\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5750 - accuracy: 0.7164 - val_loss: 0.5826 - val_accuracy: 0.6995\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5765 - accuracy: 0.7129 - val_loss: 0.5705 - val_accuracy: 0.7214\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5763 - accuracy: 0.7138 - val_loss: 0.5785 - val_accuracy: 0.7079\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5748 - accuracy: 0.7142 - val_loss: 0.5833 - val_accuracy: 0.7170\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5721 - accuracy: 0.7182 - val_loss: 0.5835 - val_accuracy: 0.7222\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5732 - accuracy: 0.7167 - val_loss: 0.5709 - val_accuracy: 0.7268\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5737 - accuracy: 0.7167 - val_loss: 0.5710 - val_accuracy: 0.7250\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5717 - accuracy: 0.7216 - val_loss: 0.5726 - val_accuracy: 0.7240\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5705 - accuracy: 0.7205 - val_loss: 0.5805 - val_accuracy: 0.7035\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5702 - accuracy: 0.7204 - val_loss: 0.5859 - val_accuracy: 0.7094\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5717 - accuracy: 0.7172 - val_loss: 0.5800 - val_accuracy: 0.7144\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7181 - val_loss: 0.5684 - val_accuracy: 0.7268\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5755 - accuracy: 0.7175 - val_loss: 0.5707 - val_accuracy: 0.7208\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5688 - accuracy: 0.7250 - val_loss: 0.5846 - val_accuracy: 0.7269\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5724 - accuracy: 0.7163 - val_loss: 0.5870 - val_accuracy: 0.7129\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5684 - accuracy: 0.7237 - val_loss: 0.5777 - val_accuracy: 0.7232\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5712 - accuracy: 0.7190 - val_loss: 0.5839 - val_accuracy: 0.7250\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5726 - accuracy: 0.7180 - val_loss: 0.5808 - val_accuracy: 0.7276\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7185 - val_loss: 0.5758 - val_accuracy: 0.7241\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5734 - accuracy: 0.7227 - val_loss: 0.5740 - val_accuracy: 0.7197\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7158 - val_loss: 0.5821 - val_accuracy: 0.7267\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5721 - accuracy: 0.7222 - val_loss: 0.5686 - val_accuracy: 0.7280\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5704 - accuracy: 0.7210 - val_loss: 0.5676 - val_accuracy: 0.7337\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5702 - accuracy: 0.7230 - val_loss: 0.5765 - val_accuracy: 0.7266\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5700 - accuracy: 0.7243 - val_loss: 0.5791 - val_accuracy: 0.7268\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.7192 - val_loss: 0.5756 - val_accuracy: 0.7265\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5700 - accuracy: 0.7226 - val_loss: 0.5764 - val_accuracy: 0.7272\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5653 - accuracy: 0.7263 - val_loss: 0.5784 - val_accuracy: 0.7304\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5794 - accuracy: 0.7313\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5688 - accuracy: 0.7225\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5679 - accuracy: 0.7233\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5671 - accuracy: 0.7239\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5684 - accuracy: 0.7220\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5661 - accuracy: 0.7257\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5670 - accuracy: 0.7233\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5680 - accuracy: 0.7219\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5679 - accuracy: 0.7226\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5665 - accuracy: 0.7255\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5672 - accuracy: 0.7236\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMZRKn-5Jer5",
        "outputId": "563756c4-0ef6-4b1b-c338-702695581188"
      },
      "source": [
        "cm2 = confusion_matrix(y, cv2_preds)\n",
        "print(cm2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[50438  7562]\n",
            " [16573 14074]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "HqdD-xy1oQPK",
        "outputId": "5e5d7d5b-0021-4ccb-b187-e2611e5e6658"
      },
      "source": [
        "'''dropout_rate = [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4]\n",
        "param_grid = dict(dropout_rate=dropout_rate)\n",
        "grid2 = GridSearchCV(estimator=mod2, param_grid=param_grid, cv=10)\n",
        "grid_result2 = grid2.fit(X, y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))\n",
        "for params, mean_score, scores in grid_result2.grid_scores_:\n",
        "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'dropout_rate = [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4]\\nparam_grid = dict(dropout_rate=dropout_rate)\\ngrid2 = GridSearchCV(estimator=mod2, param_grid=param_grid, cv=10)\\ngrid_result2 = grid2.fit(X, y)\\n# summarize results\\nprint(\"Best: %f using %s\" % (grid_result2.best_score_, grid_result2.best_params_))\\nfor params, mean_score, scores in grid_result2.grid_scores_:\\n    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jq7E6v5l56Zn"
      },
      "source": [
        "# neural network on dataset attributes based on URL directory (table 3)\n",
        "\n",
        "### (18 features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "EUJdsMJi56Zs",
        "outputId": "ddb89f2d-ebf6-4472-c6d6-f2345cc9ab34"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table1 = ['qty_dot_directory', 'qty_hyphen_directory', 'qty_underline_directory', 'qty_slash_directory', 'qty_questionmark_directory', 'qty_equal_directory', 'qty_at_directory', 'qty_and_directory',\n",
        "                   'qty_exclamation_directory', 'qty_space_directory', 'qty_tilde_directory', 'qty_comma_directory', 'qty_plus_directory', 'qty_asterisk_directory', 'qty_hashtag_directory', 'qty_dollar_directory',\n",
        "                   'qty_percent_directory', 'directory_length'] \n",
        "\n",
        "X = full_df[features_table1]\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_directory</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>qty_questionmark_directory</th>\n",
              "      <th>qty_equal_directory</th>\n",
              "      <th>qty_at_directory</th>\n",
              "      <th>qty_and_directory</th>\n",
              "      <th>qty_exclamation_directory</th>\n",
              "      <th>qty_space_directory</th>\n",
              "      <th>qty_tilde_directory</th>\n",
              "      <th>qty_comma_directory</th>\n",
              "      <th>qty_plus_directory</th>\n",
              "      <th>qty_asterisk_directory</th>\n",
              "      <th>qty_hashtag_directory</th>\n",
              "      <th>qty_dollar_directory</th>\n",
              "      <th>qty_percent_directory</th>\n",
              "      <th>directory_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_directory  ...  directory_length\n",
              "5676                   0  ...                 1\n",
              "39002                  0  ...                 0\n",
              "1732                   0  ...                 0\n",
              "39668                  0  ...                 0\n",
              "82035                  0  ...                 0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT50IOwp56Zt",
        "outputId": "aea873c3-6e2f-4377-ea81-60cd363ebac1"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88647, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qllfY70A56Zt",
        "outputId": "08dd5429-4470-4a11-d621-979184891b10"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "table3_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[18]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "table3_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "table3_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                1216      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 14,113\n",
            "Trainable params: 14,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SA3KK3456Zu",
        "outputId": "30a0dd94-1085-4d2c-dc34-511d787ef87b"
      },
      "source": [
        "history = table3_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6248 - binary_accuracy: 0.8519 - auc: 0.8770 - val_loss: 0.5612 - val_binary_accuracy: 0.8894 - val_auc: 0.9424\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5363 - binary_accuracy: 0.8905 - auc: 0.9183 - val_loss: 0.5088 - val_binary_accuracy: 0.8892 - val_auc: 0.9499\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4742 - binary_accuracy: 0.8883 - auc: 0.9178 - val_loss: 0.4830 - val_binary_accuracy: 0.8880 - val_auc: 0.9093\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4297 - binary_accuracy: 0.8896 - auc: 0.9348 - val_loss: 0.4347 - val_binary_accuracy: 0.8850 - val_auc: 0.9479\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3990 - binary_accuracy: 0.8903 - auc: 0.9448 - val_loss: 0.3879 - val_binary_accuracy: 0.8909 - val_auc: 0.9502\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3780 - binary_accuracy: 0.8863 - auc: 0.9440 - val_loss: 0.3782 - val_binary_accuracy: 0.8905 - val_auc: 0.9492\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3571 - binary_accuracy: 0.8901 - auc: 0.9453 - val_loss: 0.3747 - val_binary_accuracy: 0.8870 - val_auc: 0.9513\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3440 - binary_accuracy: 0.8866 - auc: 0.9443 - val_loss: 0.3662 - val_binary_accuracy: 0.8855 - val_auc: 0.9514\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3236 - binary_accuracy: 0.8904 - auc: 0.9485 - val_loss: 0.3898 - val_binary_accuracy: 0.8736 - val_auc: 0.9513\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3156 - binary_accuracy: 0.8903 - auc: 0.9468 - val_loss: 0.3630 - val_binary_accuracy: 0.8804 - val_auc: 0.9517\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3060 - binary_accuracy: 0.8902 - auc: 0.9481 - val_loss: 0.3489 - val_binary_accuracy: 0.8763 - val_auc: 0.9511\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3008 - binary_accuracy: 0.8862 - auc: 0.9463 - val_loss: 0.3308 - val_binary_accuracy: 0.8794 - val_auc: 0.9520\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2976 - binary_accuracy: 0.8865 - auc: 0.9461 - val_loss: 0.3745 - val_binary_accuracy: 0.8382 - val_auc: 0.9516\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2946 - binary_accuracy: 0.8870 - auc: 0.9455 - val_loss: 0.3309 - val_binary_accuracy: 0.8752 - val_auc: 0.9520\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2890 - binary_accuracy: 0.8858 - auc: 0.9450 - val_loss: 0.3711 - val_binary_accuracy: 0.8297 - val_auc: 0.9528\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2813 - binary_accuracy: 0.8908 - auc: 0.9478 - val_loss: 0.3527 - val_binary_accuracy: 0.8352 - val_auc: 0.9531\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2777 - binary_accuracy: 0.8906 - auc: 0.9476 - val_loss: 0.3690 - val_binary_accuracy: 0.7864 - val_auc: 0.9528\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2784 - binary_accuracy: 0.8874 - auc: 0.9465 - val_loss: 0.3259 - val_binary_accuracy: 0.8511 - val_auc: 0.9528\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2734 - binary_accuracy: 0.8873 - auc: 0.9468 - val_loss: 0.3326 - val_binary_accuracy: 0.8497 - val_auc: 0.9535\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2663 - binary_accuracy: 0.8921 - auc: 0.9497 - val_loss: 0.3472 - val_binary_accuracy: 0.8191 - val_auc: 0.9531\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2674 - binary_accuracy: 0.8921 - auc: 0.9488 - val_loss: 0.3356 - val_binary_accuracy: 0.8330 - val_auc: 0.9540\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2643 - binary_accuracy: 0.8930 - auc: 0.9497 - val_loss: 0.4019 - val_binary_accuracy: 0.6983 - val_auc: 0.9538\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2650 - binary_accuracy: 0.8904 - auc: 0.9490 - val_loss: 0.3283 - val_binary_accuracy: 0.8392 - val_auc: 0.9541\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2624 - binary_accuracy: 0.8926 - auc: 0.9500 - val_loss: 0.3479 - val_binary_accuracy: 0.8006 - val_auc: 0.9543\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2613 - binary_accuracy: 0.8932 - auc: 0.9496 - val_loss: 0.3469 - val_binary_accuracy: 0.7911 - val_auc: 0.9535\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2609 - binary_accuracy: 0.8922 - auc: 0.9491 - val_loss: 0.3376 - val_binary_accuracy: 0.8107 - val_auc: 0.9532\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2630 - binary_accuracy: 0.8889 - auc: 0.9496 - val_loss: 0.4114 - val_binary_accuracy: 0.6702 - val_auc: 0.9538\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2629 - binary_accuracy: 0.8887 - auc: 0.9488 - val_loss: 0.3339 - val_binary_accuracy: 0.8196 - val_auc: 0.9541\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - binary_accuracy: 0.8942 - auc: 0.9499 - val_loss: 0.3744 - val_binary_accuracy: 0.7424 - val_auc: 0.9544\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2594 - binary_accuracy: 0.8942 - auc: 0.9499 - val_loss: 0.3550 - val_binary_accuracy: 0.7805 - val_auc: 0.9545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "I1nOMrcw56Zu",
        "outputId": "aad6195a-0645-4384-a83e-f978fb0bfe4a"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "history_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.319737</td>\n",
              "      <td>0.889122</td>\n",
              "      <td>0.943041</td>\n",
              "      <td>0.376790</td>\n",
              "      <td>0.836179</td>\n",
              "      <td>0.950632</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.083141</td>\n",
              "      <td>0.002131</td>\n",
              "      <td>0.012207</td>\n",
              "      <td>0.055468</td>\n",
              "      <td>0.057525</td>\n",
              "      <td>0.008181</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.261288</td>\n",
              "      <td>0.880294</td>\n",
              "      <td>0.899099</td>\n",
              "      <td>0.325880</td>\n",
              "      <td>0.670159</td>\n",
              "      <td>0.909301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.267265</td>\n",
              "      <td>0.888019</td>\n",
              "      <td>0.945481</td>\n",
              "      <td>0.339897</td>\n",
              "      <td>0.812820</td>\n",
              "      <td>0.951271</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.282830</td>\n",
              "      <td>0.889458</td>\n",
              "      <td>0.947195</td>\n",
              "      <td>0.364583</td>\n",
              "      <td>0.844480</td>\n",
              "      <td>0.952792</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.334759</td>\n",
              "      <td>0.889872</td>\n",
              "      <td>0.948225</td>\n",
              "      <td>0.385462</td>\n",
              "      <td>0.883874</td>\n",
              "      <td>0.953679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.595781</td>\n",
              "      <td>0.892585</td>\n",
              "      <td>0.949263</td>\n",
              "      <td>0.561182</td>\n",
              "      <td>0.890905</td>\n",
              "      <td>0.954505</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  30.000000        30.000000  ...            30.000000  30.000000\n",
              "mean    0.319737         0.889122  ...             0.836179   0.950632\n",
              "std     0.083141         0.002131  ...             0.057525   0.008181\n",
              "min     0.261288         0.880294  ...             0.670159   0.909301\n",
              "25%     0.267265         0.888019  ...             0.812820   0.951271\n",
              "50%     0.282830         0.889458  ...             0.844480   0.952792\n",
              "75%     0.334759         0.889872  ...             0.883874   0.953679\n",
              "max     0.595781         0.892585  ...             0.890905   0.954505\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXPxX4ui56Zv",
        "outputId": "8e172fb6-d592-4b14-feb0-bcf5efbb13d2"
      },
      "source": [
        "train_acc = table3_nn.evaluate(train_X, train_y)\n",
        "test_acc = table3_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 1ms/step - loss: 0.3878 - binary_accuracy: 0.8913 - auc: 0.9503\n",
            "693/693 [==============================] - 1s 2ms/step - loss: 0.3870 - binary_accuracy: 0.8932 - auc: 0.9502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIB5wNzR56Zv",
        "outputId": "9371ae12-226d-465e-81a3-ed64154dba50"
      },
      "source": [
        "dict(zip(table3_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.9501541256904602,\n",
              " 'binary_accuracy': 0.8931504487991333,\n",
              " 'loss': 0.3869595229625702}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "tAl7tiUR56Zv",
        "outputId": "dde0e391-62ce-447b-c240-6905fbafebac"
      },
      "source": [
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.32587987184524536\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "vhgrRVYg56Zw",
        "outputId": "79af52fb-5ffc-4207-ae6d-c0f67f7dee85"
      },
      "source": [
        "history_df.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.9545053839683533\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "74G3niam56Zw",
        "outputId": "c6675bb9-f71d-4bb2-ab8b-1d5030d157f9"
      },
      "source": [
        "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.8909054398536682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuRlOL3l56Zw",
        "outputId": "4fba5eab-209c-4e00-ddae-bffbf2e063a0"
      },
      "source": [
        "preds1 = table3_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8392706 ],\n",
              "       [0.24369487],\n",
              "       [0.37649712],\n",
              "       ...,\n",
              "       [0.24369487],\n",
              "       [0.24369487],\n",
              "       [0.71344495]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqamag3K56Zx",
        "outputId": "0b84bc94-77b9-4180-86b3-c79b9e3d8218"
      },
      "source": [
        "len(preds1[preds1 < 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14571"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67m649wi56Zy",
        "outputId": "717365ed-09a8-44c6-e352-c794b7ba2e46"
      },
      "source": [
        "len(preds1[preds1 >= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7591"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_1Tg6LoW56Zy",
        "outputId": "2fb9cdc4-0810-4027-ceac-15397cbd2875"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG58ZxBDo6Vy",
        "outputId": "49d7e094-feb3-4acf-e6f1-c95ece9e5759"
      },
      "source": [
        "pred_classes = np.argmax(preds1, axis=1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(val_y, pred_classes)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14519     0]\n",
            " [ 7643     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14HsrzL2oQPK"
      },
      "source": [
        "# NN on URL directory attributes (table 3) [model as function]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "D8h5pPg1oQPK",
        "outputId": "d1925c3a-b276-4137-85a3-5b075bb992cf"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table3 = ['qty_dot_directory', 'qty_hyphen_directory', 'qty_underline_directory', 'qty_slash_directory', 'qty_questionmark_directory', 'qty_equal_directory', 'qty_at_directory', 'qty_and_directory',\n",
        "                   'qty_exclamation_directory', 'qty_space_directory', 'qty_tilde_directory', 'qty_comma_directory', 'qty_plus_directory', 'qty_asterisk_directory', 'qty_hashtag_directory', 'qty_dollar_directory',\n",
        "                   'qty_percent_directory', 'directory_length'] \n",
        "\n",
        "X = full_df[features_table3]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_directory</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>qty_questionmark_directory</th>\n",
              "      <th>qty_equal_directory</th>\n",
              "      <th>qty_at_directory</th>\n",
              "      <th>qty_and_directory</th>\n",
              "      <th>qty_exclamation_directory</th>\n",
              "      <th>qty_space_directory</th>\n",
              "      <th>qty_tilde_directory</th>\n",
              "      <th>qty_comma_directory</th>\n",
              "      <th>qty_plus_directory</th>\n",
              "      <th>qty_asterisk_directory</th>\n",
              "      <th>qty_hashtag_directory</th>\n",
              "      <th>qty_dollar_directory</th>\n",
              "      <th>qty_percent_directory</th>\n",
              "      <th>directory_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.707107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_directory  ...  directory_length\n",
              "5676                 0.0  ...          0.707107\n",
              "39002                0.0  ...          0.000000\n",
              "1732                 0.0  ...          0.000000\n",
              "39668                0.0  ...          0.000000\n",
              "82035                0.0  ...          0.000000\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMEPNKYEoQPK"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_3():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[18]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_VzNxi_oQPK"
      },
      "source": [
        "mod3 = KerasClassifier(build_fn=phish_nn_3,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHkRxPanoQPK",
        "outputId": "e73defe4-440b-458f-8afc-cfb215275921"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_3=cross_val_score(mod3,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.5791 - accuracy: 0.8374 - val_loss: 0.3196 - val_accuracy: 0.8923\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3178 - accuracy: 0.8788 - val_loss: 0.2957 - val_accuracy: 0.8920\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.8867 - val_loss: 0.3184 - val_accuracy: 0.8921\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2904 - accuracy: 0.8858 - val_loss: 0.2983 - val_accuracy: 0.8920\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2897 - accuracy: 0.8853 - val_loss: 0.3036 - val_accuracy: 0.8925\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2808 - accuracy: 0.8896 - val_loss: 0.3107 - val_accuracy: 0.8922\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2773 - accuracy: 0.8919 - val_loss: 0.2899 - val_accuracy: 0.8927\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2744 - accuracy: 0.8926 - val_loss: 0.2857 - val_accuracy: 0.8925\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2746 - accuracy: 0.8890 - val_loss: 0.2701 - val_accuracy: 0.8930\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2783 - accuracy: 0.8883 - val_loss: 0.2748 - val_accuracy: 0.8938\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2743 - accuracy: 0.8903 - val_loss: 0.2792 - val_accuracy: 0.8937\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2704 - accuracy: 0.8902 - val_loss: 0.2780 - val_accuracy: 0.8928\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2704 - accuracy: 0.8925 - val_loss: 0.2746 - val_accuracy: 0.8946\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2690 - accuracy: 0.8894 - val_loss: 0.2655 - val_accuracy: 0.8946\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2645 - accuracy: 0.8926 - val_loss: 0.2676 - val_accuracy: 0.8947\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2669 - accuracy: 0.8941 - val_loss: 0.2667 - val_accuracy: 0.8954\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2664 - accuracy: 0.8917 - val_loss: 0.2675 - val_accuracy: 0.8952\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2695 - accuracy: 0.8909 - val_loss: 0.2671 - val_accuracy: 0.8951\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2684 - accuracy: 0.8932 - val_loss: 0.2634 - val_accuracy: 0.8967\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2644 - accuracy: 0.8918 - val_loss: 0.2755 - val_accuracy: 0.8967\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2613 - accuracy: 0.8946 - val_loss: 0.2646 - val_accuracy: 0.8960\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2692 - accuracy: 0.8928 - val_loss: 0.2638 - val_accuracy: 0.8961\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2672 - accuracy: 0.8934 - val_loss: 0.2626 - val_accuracy: 0.8965\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8944 - val_loss: 0.2664 - val_accuracy: 0.8972\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8916 - val_loss: 0.2644 - val_accuracy: 0.8967\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8976 - val_loss: 0.2689 - val_accuracy: 0.8961\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2664 - accuracy: 0.8910 - val_loss: 0.2573 - val_accuracy: 0.8958\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2569 - accuracy: 0.8966 - val_loss: 0.2679 - val_accuracy: 0.8973\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2658 - accuracy: 0.8942 - val_loss: 0.2642 - val_accuracy: 0.8964\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2623 - accuracy: 0.8955 - val_loss: 0.2572 - val_accuracy: 0.8972\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2616 - accuracy: 0.8954 - val_loss: 0.2597 - val_accuracy: 0.8964\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2671 - accuracy: 0.8922 - val_loss: 0.2614 - val_accuracy: 0.8970\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2661 - accuracy: 0.8941 - val_loss: 0.2676 - val_accuracy: 0.8960\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2617 - accuracy: 0.8957 - val_loss: 0.2591 - val_accuracy: 0.8968\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2631 - accuracy: 0.8955 - val_loss: 0.2626 - val_accuracy: 0.8968\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2631 - accuracy: 0.8936 - val_loss: 0.2623 - val_accuracy: 0.8968\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2638 - accuracy: 0.8947 - val_loss: 0.2630 - val_accuracy: 0.8935\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2592 - accuracy: 0.8967 - val_loss: 0.2611 - val_accuracy: 0.8975\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8954 - val_loss: 0.2634 - val_accuracy: 0.8975\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8954 - val_loss: 0.2632 - val_accuracy: 0.8954\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2625 - accuracy: 0.8960 - val_loss: 0.2590 - val_accuracy: 0.8965\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8949 - val_loss: 0.2573 - val_accuracy: 0.8966\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2622 - accuracy: 0.8941 - val_loss: 0.2578 - val_accuracy: 0.8960\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2610 - accuracy: 0.8951 - val_loss: 0.2646 - val_accuracy: 0.8955\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2640 - accuracy: 0.8925 - val_loss: 0.2603 - val_accuracy: 0.8974\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2604 - accuracy: 0.8958 - val_loss: 0.2583 - val_accuracy: 0.8966\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2556 - accuracy: 0.8988 - val_loss: 0.2649 - val_accuracy: 0.8967\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.8927 - val_loss: 0.2606 - val_accuracy: 0.8965\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2576 - accuracy: 0.8959 - val_loss: 0.2606 - val_accuracy: 0.8971\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2629 - accuracy: 0.8934 - val_loss: 0.2654 - val_accuracy: 0.8971\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2692 - accuracy: 0.8973\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 6ms/step - loss: 0.2618 - accuracy: 0.8948\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2601 - accuracy: 0.8954\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2601 - accuracy: 0.8952\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2605 - accuracy: 0.8955\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 6ms/step - loss: 0.2608 - accuracy: 0.8946\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 3s 7ms/step - loss: 0.2618 - accuracy: 0.8944\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 6ms/step - loss: 0.2611 - accuracy: 0.8949\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2603 - accuracy: 0.8953\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2590 - accuracy: 0.8962\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2600 - accuracy: 0.8952\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2650 - accuracy: 0.8980\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6049 - accuracy: 0.8481 - val_loss: 0.4357 - val_accuracy: 0.6596\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3428 - accuracy: 0.8748 - val_loss: 0.3684 - val_accuracy: 0.6968\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3034 - accuracy: 0.8851 - val_loss: 0.3595 - val_accuracy: 0.7802\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2888 - accuracy: 0.8878 - val_loss: 0.3281 - val_accuracy: 0.8783\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2837 - accuracy: 0.8889 - val_loss: 0.3125 - val_accuracy: 0.8861\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2827 - accuracy: 0.8880 - val_loss: 0.3027 - val_accuracy: 0.8887\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2755 - accuracy: 0.8914 - val_loss: 0.3126 - val_accuracy: 0.8826\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.8883 - val_loss: 0.2821 - val_accuracy: 0.8930\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8898 - val_loss: 0.2969 - val_accuracy: 0.8929\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2702 - accuracy: 0.8890 - val_loss: 0.2882 - val_accuracy: 0.8941\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2713 - accuracy: 0.8913 - val_loss: 0.2787 - val_accuracy: 0.8933\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8908 - val_loss: 0.2843 - val_accuracy: 0.8937\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8901 - val_loss: 0.2713 - val_accuracy: 0.8929\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2749 - accuracy: 0.8874 - val_loss: 0.2765 - val_accuracy: 0.8933\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2706 - accuracy: 0.8905 - val_loss: 0.2752 - val_accuracy: 0.8927\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8931 - val_loss: 0.2767 - val_accuracy: 0.8952\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8950 - val_loss: 0.2656 - val_accuracy: 0.8942\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2681 - accuracy: 0.8926 - val_loss: 0.2724 - val_accuracy: 0.8950\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2669 - accuracy: 0.8918 - val_loss: 0.2633 - val_accuracy: 0.8936\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2646 - accuracy: 0.8929 - val_loss: 0.2728 - val_accuracy: 0.8950\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2703 - accuracy: 0.8901 - val_loss: 0.2715 - val_accuracy: 0.8934\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2625 - accuracy: 0.8934 - val_loss: 0.2656 - val_accuracy: 0.8956\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8932 - val_loss: 0.2663 - val_accuracy: 0.8942\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8954 - val_loss: 0.2644 - val_accuracy: 0.8936\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2623 - accuracy: 0.8945 - val_loss: 0.2752 - val_accuracy: 0.8921\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.8912 - val_loss: 0.2618 - val_accuracy: 0.8953\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2616 - accuracy: 0.8952 - val_loss: 0.2618 - val_accuracy: 0.8947\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.8958 - val_loss: 0.2783 - val_accuracy: 0.8944\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2623 - accuracy: 0.8949 - val_loss: 0.2697 - val_accuracy: 0.8928\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2619 - accuracy: 0.8953 - val_loss: 0.2660 - val_accuracy: 0.8957\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8914 - val_loss: 0.2574 - val_accuracy: 0.8951\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2650 - accuracy: 0.8913 - val_loss: 0.2672 - val_accuracy: 0.8951\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8949 - val_loss: 0.2596 - val_accuracy: 0.8957\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2652 - accuracy: 0.8944 - val_loss: 0.2614 - val_accuracy: 0.8940\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8923 - val_loss: 0.2620 - val_accuracy: 0.8974\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8947 - val_loss: 0.2630 - val_accuracy: 0.8948\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8961 - val_loss: 0.2578 - val_accuracy: 0.8946\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2577 - accuracy: 0.8977 - val_loss: 0.2618 - val_accuracy: 0.8956\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2594 - accuracy: 0.8948 - val_loss: 0.2648 - val_accuracy: 0.8960\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8947 - val_loss: 0.2658 - val_accuracy: 0.8939\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.8953 - val_loss: 0.2570 - val_accuracy: 0.8954\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8947 - val_loss: 0.2579 - val_accuracy: 0.8962\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8957 - val_loss: 0.2639 - val_accuracy: 0.8954\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8938 - val_loss: 0.2569 - val_accuracy: 0.8951\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8948 - val_loss: 0.2608 - val_accuracy: 0.8960\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.8948 - val_loss: 0.2622 - val_accuracy: 0.8951\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8968 - val_loss: 0.2606 - val_accuracy: 0.8972\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2633 - accuracy: 0.8937 - val_loss: 0.2681 - val_accuracy: 0.8946\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2621 - accuracy: 0.8930 - val_loss: 0.2687 - val_accuracy: 0.8972\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2616 - accuracy: 0.8947 - val_loss: 0.2598 - val_accuracy: 0.8964\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2651 - accuracy: 0.8983\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2611 - accuracy: 0.8957\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2600 - accuracy: 0.8961\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2604 - accuracy: 0.8953\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2593 - accuracy: 0.8962\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2594 - accuracy: 0.8959\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2597 - accuracy: 0.8958\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2591 - accuracy: 0.8967\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2585 - accuracy: 0.8964\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2587 - accuracy: 0.8968\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2576 - accuracy: 0.8964\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2626 - accuracy: 0.8938\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.5462 - accuracy: 0.8487 - val_loss: 0.3099 - val_accuracy: 0.8925\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3076 - accuracy: 0.8822 - val_loss: 0.3224 - val_accuracy: 0.8919\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2934 - accuracy: 0.8855 - val_loss: 0.3045 - val_accuracy: 0.8918\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2877 - accuracy: 0.8861 - val_loss: 0.2906 - val_accuracy: 0.8925\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8910 - val_loss: 0.2916 - val_accuracy: 0.8927\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2806 - accuracy: 0.8883 - val_loss: 0.3047 - val_accuracy: 0.8923\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2767 - accuracy: 0.8898 - val_loss: 0.2866 - val_accuracy: 0.8931\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2731 - accuracy: 0.8916 - val_loss: 0.2720 - val_accuracy: 0.8942\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2717 - accuracy: 0.8907 - val_loss: 0.2774 - val_accuracy: 0.8951\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2702 - accuracy: 0.8909 - val_loss: 0.2774 - val_accuracy: 0.8937\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2744 - accuracy: 0.8890 - val_loss: 0.2682 - val_accuracy: 0.8956\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2649 - accuracy: 0.8945 - val_loss: 0.2635 - val_accuracy: 0.8951\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2646 - accuracy: 0.8952 - val_loss: 0.2687 - val_accuracy: 0.8950\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8954 - val_loss: 0.2649 - val_accuracy: 0.8955\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.8951 - val_loss: 0.2771 - val_accuracy: 0.8953\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2662 - accuracy: 0.8929 - val_loss: 0.2645 - val_accuracy: 0.8959\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8911 - val_loss: 0.2651 - val_accuracy: 0.8954\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2697 - accuracy: 0.8897 - val_loss: 0.2608 - val_accuracy: 0.8952\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8929 - val_loss: 0.2634 - val_accuracy: 0.8963\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2586 - accuracy: 0.8953 - val_loss: 0.2678 - val_accuracy: 0.8957\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8922 - val_loss: 0.2567 - val_accuracy: 0.8970\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2578 - accuracy: 0.8944 - val_loss: 0.2645 - val_accuracy: 0.8964\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8954 - val_loss: 0.2593 - val_accuracy: 0.8962\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2658 - accuracy: 0.8926 - val_loss: 0.2636 - val_accuracy: 0.8948\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2609 - accuracy: 0.8940 - val_loss: 0.2755 - val_accuracy: 0.8945\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2640 - accuracy: 0.8945 - val_loss: 0.2656 - val_accuracy: 0.8940\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2621 - accuracy: 0.8927 - val_loss: 0.2632 - val_accuracy: 0.8944\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2540 - accuracy: 0.8973 - val_loss: 0.2626 - val_accuracy: 0.8951\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.8918 - val_loss: 0.2659 - val_accuracy: 0.8953\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2623 - accuracy: 0.8931 - val_loss: 0.2625 - val_accuracy: 0.8957\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2642 - accuracy: 0.8939 - val_loss: 0.2634 - val_accuracy: 0.8950\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8933 - val_loss: 0.2588 - val_accuracy: 0.8961\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8960 - val_loss: 0.2634 - val_accuracy: 0.8966\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2603 - accuracy: 0.8951 - val_loss: 0.2698 - val_accuracy: 0.8947\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2615 - accuracy: 0.8948 - val_loss: 0.2621 - val_accuracy: 0.8962\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8928 - val_loss: 0.2609 - val_accuracy: 0.8966\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2590 - accuracy: 0.8973\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2646 - accuracy: 0.8940\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2638 - accuracy: 0.8935\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2635 - accuracy: 0.8937\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2630 - accuracy: 0.8937\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2632 - accuracy: 0.8934\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2624 - accuracy: 0.8940\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2632 - accuracy: 0.8943\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2622 - accuracy: 0.8942\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2622 - accuracy: 0.8938\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2615 - accuracy: 0.8948\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2626 - accuracy: 0.8997\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.4933 - accuracy: 0.8198 - val_loss: 0.2835 - val_accuracy: 0.8934\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3107 - accuracy: 0.8770 - val_loss: 0.2897 - val_accuracy: 0.8926\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2939 - accuracy: 0.8871 - val_loss: 0.2996 - val_accuracy: 0.8930\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2818 - accuracy: 0.8903 - val_loss: 0.2927 - val_accuracy: 0.8915\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2830 - accuracy: 0.8901 - val_loss: 0.2744 - val_accuracy: 0.8925\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8853 - val_loss: 0.2744 - val_accuracy: 0.8930\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2762 - accuracy: 0.8933 - val_loss: 0.2734 - val_accuracy: 0.8929\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2797 - accuracy: 0.8909 - val_loss: 0.2667 - val_accuracy: 0.8931\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8914 - val_loss: 0.2698 - val_accuracy: 0.8935\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8928 - val_loss: 0.2708 - val_accuracy: 0.8922\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2727 - accuracy: 0.8913 - val_loss: 0.2679 - val_accuracy: 0.8944\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8891 - val_loss: 0.2679 - val_accuracy: 0.8936\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2658 - accuracy: 0.8927 - val_loss: 0.2612 - val_accuracy: 0.8939\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2659 - accuracy: 0.8930 - val_loss: 0.2664 - val_accuracy: 0.8936\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8889 - val_loss: 0.2641 - val_accuracy: 0.8909\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2660 - accuracy: 0.8939 - val_loss: 0.2609 - val_accuracy: 0.8935\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2708 - accuracy: 0.8916 - val_loss: 0.2595 - val_accuracy: 0.8939\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2716 - accuracy: 0.8912 - val_loss: 0.2598 - val_accuracy: 0.8943\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8915 - val_loss: 0.2670 - val_accuracy: 0.8913\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2585 - accuracy: 0.8955 - val_loss: 0.2637 - val_accuracy: 0.8926\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8926 - val_loss: 0.2585 - val_accuracy: 0.8944\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8922 - val_loss: 0.2598 - val_accuracy: 0.8940\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.8944 - val_loss: 0.2611 - val_accuracy: 0.8902\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8912 - val_loss: 0.2638 - val_accuracy: 0.8914\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2661 - accuracy: 0.8910 - val_loss: 0.2577 - val_accuracy: 0.8943\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8939 - val_loss: 0.2593 - val_accuracy: 0.8929\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2717 - accuracy: 0.8957\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2716 - accuracy: 0.8920\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2675 - accuracy: 0.8934\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2672 - accuracy: 0.8926\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2667 - accuracy: 0.8936\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2656 - accuracy: 0.8941\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2641 - accuracy: 0.8943\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2664 - accuracy: 0.8936\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2644 - accuracy: 0.8941\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2635 - accuracy: 0.8946\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2631 - accuracy: 0.8943\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2646 - accuracy: 0.8931\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6301 - accuracy: 0.8314 - val_loss: 0.5269 - val_accuracy: 0.8926\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4670 - accuracy: 0.8898 - val_loss: 0.3157 - val_accuracy: 0.8931\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3097 - accuracy: 0.8775 - val_loss: 0.2915 - val_accuracy: 0.8922\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.8826 - val_loss: 0.3109 - val_accuracy: 0.8917\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.8870 - val_loss: 0.2843 - val_accuracy: 0.8932\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8864 - val_loss: 0.2819 - val_accuracy: 0.8925\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2692 - accuracy: 0.8931 - val_loss: 0.2772 - val_accuracy: 0.8934\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8855 - val_loss: 0.2821 - val_accuracy: 0.8937\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.8895 - val_loss: 0.2988 - val_accuracy: 0.8937\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2702 - accuracy: 0.8907 - val_loss: 0.2793 - val_accuracy: 0.8904\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2745 - accuracy: 0.8882 - val_loss: 0.2744 - val_accuracy: 0.8943\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2669 - accuracy: 0.8917 - val_loss: 0.2596 - val_accuracy: 0.8941\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2696 - accuracy: 0.8908 - val_loss: 0.2811 - val_accuracy: 0.8937\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2660 - accuracy: 0.8906 - val_loss: 0.2904 - val_accuracy: 0.8942\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8903 - val_loss: 0.2654 - val_accuracy: 0.8920\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8902 - val_loss: 0.2762 - val_accuracy: 0.8934\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.8913 - val_loss: 0.2644 - val_accuracy: 0.8953\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2639 - accuracy: 0.8917 - val_loss: 0.2839 - val_accuracy: 0.8949\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2677 - accuracy: 0.8927 - val_loss: 0.2646 - val_accuracy: 0.8938\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8940 - val_loss: 0.2663 - val_accuracy: 0.8945\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2656 - accuracy: 0.8927 - val_loss: 0.2815 - val_accuracy: 0.8927\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8929 - val_loss: 0.2580 - val_accuracy: 0.8950\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8918 - val_loss: 0.2637 - val_accuracy: 0.8954\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8943 - val_loss: 0.2837 - val_accuracy: 0.8964\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8923 - val_loss: 0.2662 - val_accuracy: 0.8962\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8945 - val_loss: 0.2639 - val_accuracy: 0.8949\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8938 - val_loss: 0.2681 - val_accuracy: 0.8954\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2678 - accuracy: 0.8921 - val_loss: 0.2627 - val_accuracy: 0.8947\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2681 - accuracy: 0.8922 - val_loss: 0.2739 - val_accuracy: 0.8956\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8941 - val_loss: 0.2736 - val_accuracy: 0.8945\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2636 - accuracy: 0.8943 - val_loss: 0.2644 - val_accuracy: 0.8960\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2617 - accuracy: 0.8936 - val_loss: 0.2754 - val_accuracy: 0.8955\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2596 - accuracy: 0.8955 - val_loss: 0.2560 - val_accuracy: 0.8963\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2631 - accuracy: 0.8936 - val_loss: 0.2572 - val_accuracy: 0.8967\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8944 - val_loss: 0.2703 - val_accuracy: 0.8934\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2593 - accuracy: 0.8944 - val_loss: 0.2615 - val_accuracy: 0.8957\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8924 - val_loss: 0.2588 - val_accuracy: 0.8962\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8937 - val_loss: 0.2615 - val_accuracy: 0.8961\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2627 - accuracy: 0.8932 - val_loss: 0.2675 - val_accuracy: 0.8954\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2625 - accuracy: 0.8937 - val_loss: 0.2600 - val_accuracy: 0.8963\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2602 - accuracy: 0.8944 - val_loss: 0.2624 - val_accuracy: 0.8944\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2590 - accuracy: 0.8969 - val_loss: 0.2647 - val_accuracy: 0.8958\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2602 - accuracy: 0.8946 - val_loss: 0.2642 - val_accuracy: 0.8966\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8935 - val_loss: 0.2592 - val_accuracy: 0.8958\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8933 - val_loss: 0.2589 - val_accuracy: 0.8960\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8960 - val_loss: 0.2634 - val_accuracy: 0.8959\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2591 - accuracy: 0.8950 - val_loss: 0.2551 - val_accuracy: 0.8962\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.8922 - val_loss: 0.2606 - val_accuracy: 0.8957\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8951 - val_loss: 0.2671 - val_accuracy: 0.8961\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2592 - accuracy: 0.8978\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2628 - accuracy: 0.8942\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2625 - accuracy: 0.8943\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2615 - accuracy: 0.8945\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2610 - accuracy: 0.8948\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2631 - accuracy: 0.8940\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2620 - accuracy: 0.8942\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2602 - accuracy: 0.8949\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2614 - accuracy: 0.8946\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2600 - accuracy: 0.8957\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2599 - accuracy: 0.8951\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2576 - accuracy: 0.8984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xvMdwXwoQPL",
        "outputId": "3f22f4bf-ce98-46ff-c598-b4d65c803435"
      },
      "source": [
        "print(round(cv_results_3.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhGNLfVHoQPL",
        "outputId": "923c0d73-9ee5-420c-9a74-af378d6118d0"
      },
      "source": [
        "print(round(cv_results_3.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0026\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxc8w57EnoX3",
        "outputId": "45e17b66-cbfe-4db4-a1bc-d2b9cbe02094"
      },
      "source": [
        "cv_results_3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.89802593, 0.89379585, 0.89965594, 0.89311296, 0.89835864])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "td_oV0TeoQPL",
        "outputId": "877de64d-1c2a-4e60-af4f-b8801d843fac"
      },
      "source": [
        "cv3_preds = cross_val_predict(mod3, X, y, cv=5, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6360 - accuracy: 0.8271 - val_loss: 0.5616 - val_accuracy: 0.8925\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5312 - accuracy: 0.8903 - val_loss: 0.5318 - val_accuracy: 0.8920\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4822 - accuracy: 0.8899 - val_loss: 0.4744 - val_accuracy: 0.8925\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4411 - accuracy: 0.8901 - val_loss: 0.4334 - val_accuracy: 0.8933\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4122 - accuracy: 0.8892 - val_loss: 0.4081 - val_accuracy: 0.8930\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3808 - accuracy: 0.8919 - val_loss: 0.3862 - val_accuracy: 0.8925\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3606 - accuracy: 0.8941 - val_loss: 0.3732 - val_accuracy: 0.8936\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3513 - accuracy: 0.8890 - val_loss: 0.3386 - val_accuracy: 0.8944\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3341 - accuracy: 0.8910 - val_loss: 0.3318 - val_accuracy: 0.8943\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3197 - accuracy: 0.8937 - val_loss: 0.3234 - val_accuracy: 0.8940\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8864 - val_loss: 0.3140 - val_accuracy: 0.8947\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3107 - accuracy: 0.8861 - val_loss: 0.3091 - val_accuracy: 0.8941\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3021 - accuracy: 0.8879 - val_loss: 0.3248 - val_accuracy: 0.8943\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2972 - accuracy: 0.8893 - val_loss: 0.3143 - val_accuracy: 0.8946\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2936 - accuracy: 0.8901 - val_loss: 0.2989 - val_accuracy: 0.8948\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2877 - accuracy: 0.8904 - val_loss: 0.2898 - val_accuracy: 0.8950\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2849 - accuracy: 0.8911 - val_loss: 0.3165 - val_accuracy: 0.8936\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2816 - accuracy: 0.8913 - val_loss: 0.2922 - val_accuracy: 0.8949\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2765 - accuracy: 0.8922 - val_loss: 0.3099 - val_accuracy: 0.8948\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8895 - val_loss: 0.2804 - val_accuracy: 0.8953\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2783 - accuracy: 0.8875 - val_loss: 0.2899 - val_accuracy: 0.8936\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8876 - val_loss: 0.2907 - val_accuracy: 0.8942\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2759 - accuracy: 0.8915 - val_loss: 0.2908 - val_accuracy: 0.8946\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2706 - accuracy: 0.8921 - val_loss: 0.3032 - val_accuracy: 0.8942\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2733 - accuracy: 0.8900 - val_loss: 0.3158 - val_accuracy: 0.8938\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2790 - accuracy: 0.8891 - val_loss: 0.2727 - val_accuracy: 0.8952\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2733 - accuracy: 0.8900 - val_loss: 0.2790 - val_accuracy: 0.8935\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2717 - accuracy: 0.8899 - val_loss: 0.2891 - val_accuracy: 0.8937\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2683 - accuracy: 0.8933 - val_loss: 0.2907 - val_accuracy: 0.8938\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8913 - val_loss: 0.2882 - val_accuracy: 0.8938\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2712 - accuracy: 0.8919 - val_loss: 0.2814 - val_accuracy: 0.8946\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2723 - accuracy: 0.8887 - val_loss: 0.2879 - val_accuracy: 0.8939\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2719 - accuracy: 0.8908 - val_loss: 0.2701 - val_accuracy: 0.8950\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2728 - accuracy: 0.8877 - val_loss: 0.2809 - val_accuracy: 0.8940\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8905 - val_loss: 0.2656 - val_accuracy: 0.8945\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2815 - accuracy: 0.8969\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2797 - accuracy: 0.8899\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2750 - accuracy: 0.8909\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2751 - accuracy: 0.8906\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2739 - accuracy: 0.8905\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2726 - accuracy: 0.8908\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2706 - accuracy: 0.8906\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2714 - accuracy: 0.8911\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2708 - accuracy: 0.8919\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2692 - accuracy: 0.8920\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2692 - accuracy: 0.8921\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6219 - accuracy: 0.8393 - val_loss: 0.4847 - val_accuracy: 0.8926\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4092 - accuracy: 0.8857 - val_loss: 0.3276 - val_accuracy: 0.8925\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3149 - accuracy: 0.8799 - val_loss: 0.3131 - val_accuracy: 0.8926\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8774 - val_loss: 0.2884 - val_accuracy: 0.8926\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8839 - val_loss: 0.2998 - val_accuracy: 0.8912\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2901 - accuracy: 0.8814 - val_loss: 0.2886 - val_accuracy: 0.8925\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2836 - accuracy: 0.8842 - val_loss: 0.2842 - val_accuracy: 0.8924\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2795 - accuracy: 0.8867 - val_loss: 0.2852 - val_accuracy: 0.8924\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2753 - accuracy: 0.8889 - val_loss: 0.2789 - val_accuracy: 0.8937\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2696 - accuracy: 0.8902 - val_loss: 0.2707 - val_accuracy: 0.8945\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8893 - val_loss: 0.2697 - val_accuracy: 0.8935\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2735 - accuracy: 0.8888 - val_loss: 0.2676 - val_accuracy: 0.8938\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2650 - accuracy: 0.8936 - val_loss: 0.2737 - val_accuracy: 0.8920\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8900 - val_loss: 0.2739 - val_accuracy: 0.8948\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2685 - accuracy: 0.8928 - val_loss: 0.2771 - val_accuracy: 0.8947\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.8913 - val_loss: 0.2690 - val_accuracy: 0.8940\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2667 - accuracy: 0.8915 - val_loss: 0.2699 - val_accuracy: 0.8955\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2654 - accuracy: 0.8926 - val_loss: 0.2715 - val_accuracy: 0.8912\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2642 - accuracy: 0.8923 - val_loss: 0.2634 - val_accuracy: 0.8968\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2622 - accuracy: 0.8942 - val_loss: 0.2621 - val_accuracy: 0.8972\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2641 - accuracy: 0.8923 - val_loss: 0.2725 - val_accuracy: 0.8936\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2628 - accuracy: 0.8940 - val_loss: 0.2664 - val_accuracy: 0.8955\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2646 - accuracy: 0.8932 - val_loss: 0.2722 - val_accuracy: 0.8961\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2654 - accuracy: 0.8927 - val_loss: 0.2637 - val_accuracy: 0.8949\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2660 - accuracy: 0.8916 - val_loss: 0.2604 - val_accuracy: 0.8969\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2626 - accuracy: 0.8962 - val_loss: 0.2679 - val_accuracy: 0.8976\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8954 - val_loss: 0.2625 - val_accuracy: 0.8958\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2622 - accuracy: 0.8933 - val_loss: 0.2682 - val_accuracy: 0.8959\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2656 - accuracy: 0.8932 - val_loss: 0.2686 - val_accuracy: 0.8963\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2654 - accuracy: 0.8936 - val_loss: 0.2652 - val_accuracy: 0.8959\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8952 - val_loss: 0.2656 - val_accuracy: 0.8979\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8931 - val_loss: 0.2732 - val_accuracy: 0.8946\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8960 - val_loss: 0.2674 - val_accuracy: 0.8973\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8957 - val_loss: 0.2541 - val_accuracy: 0.8971\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2600 - accuracy: 0.8964 - val_loss: 0.2721 - val_accuracy: 0.8960\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8955 - val_loss: 0.2595 - val_accuracy: 0.8964\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8920 - val_loss: 0.2667 - val_accuracy: 0.8965\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2601 - accuracy: 0.8938 - val_loss: 0.2651 - val_accuracy: 0.8959\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.8946 - val_loss: 0.2581 - val_accuracy: 0.8953\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8958 - val_loss: 0.2581 - val_accuracy: 0.8970\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8941 - val_loss: 0.2579 - val_accuracy: 0.8962\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2627 - accuracy: 0.8941 - val_loss: 0.2681 - val_accuracy: 0.8967\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8909 - val_loss: 0.2572 - val_accuracy: 0.8969\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2623 - accuracy: 0.8935 - val_loss: 0.2607 - val_accuracy: 0.8960\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8966 - val_loss: 0.2669 - val_accuracy: 0.8943\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2595 - accuracy: 0.8959 - val_loss: 0.2692 - val_accuracy: 0.8953\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2688 - accuracy: 0.8988\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2603 - accuracy: 0.8955\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2602 - accuracy: 0.8954\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2586 - accuracy: 0.8961\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2602 - accuracy: 0.8950\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2593 - accuracy: 0.8952\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2599 - accuracy: 0.8948\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2584 - accuracy: 0.8954\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2577 - accuracy: 0.8958\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2594 - accuracy: 0.8949\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2579 - accuracy: 0.8958\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5899 - accuracy: 0.8498 - val_loss: 0.3506 - val_accuracy: 0.8924\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3117 - accuracy: 0.8855 - val_loss: 0.3085 - val_accuracy: 0.8925\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2993 - accuracy: 0.8844 - val_loss: 0.3050 - val_accuracy: 0.8928\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2918 - accuracy: 0.8852 - val_loss: 0.2915 - val_accuracy: 0.8924\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8901 - val_loss: 0.2913 - val_accuracy: 0.8913\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2849 - accuracy: 0.8887 - val_loss: 0.2843 - val_accuracy: 0.8925\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8906 - val_loss: 0.2880 - val_accuracy: 0.8917\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2762 - accuracy: 0.8898 - val_loss: 0.2694 - val_accuracy: 0.8928\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8916 - val_loss: 0.2821 - val_accuracy: 0.8923\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2765 - accuracy: 0.8899 - val_loss: 0.2723 - val_accuracy: 0.8934\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2755 - accuracy: 0.8879 - val_loss: 0.2711 - val_accuracy: 0.8930\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2720 - accuracy: 0.8910 - val_loss: 0.2655 - val_accuracy: 0.8937\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2706 - accuracy: 0.8923 - val_loss: 0.2649 - val_accuracy: 0.8939\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.8910 - val_loss: 0.2702 - val_accuracy: 0.8927\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8911 - val_loss: 0.2796 - val_accuracy: 0.8934\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2701 - accuracy: 0.8921 - val_loss: 0.2684 - val_accuracy: 0.8933\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2703 - accuracy: 0.8915 - val_loss: 0.2670 - val_accuracy: 0.8936\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8913 - val_loss: 0.2707 - val_accuracy: 0.8945\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8914 - val_loss: 0.2650 - val_accuracy: 0.8942\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2622 - accuracy: 0.8952 - val_loss: 0.2613 - val_accuracy: 0.8941\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2632 - accuracy: 0.8946 - val_loss: 0.2740 - val_accuracy: 0.8936\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8955 - val_loss: 0.2679 - val_accuracy: 0.8941\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2690 - accuracy: 0.8920 - val_loss: 0.2648 - val_accuracy: 0.8945\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2709 - accuracy: 0.8888 - val_loss: 0.2589 - val_accuracy: 0.8944\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8938 - val_loss: 0.2609 - val_accuracy: 0.8944\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2662 - accuracy: 0.8908 - val_loss: 0.2645 - val_accuracy: 0.8945\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2629 - accuracy: 0.8966 - val_loss: 0.2647 - val_accuracy: 0.8945\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2601 - accuracy: 0.8966 - val_loss: 0.2596 - val_accuracy: 0.8955\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.8936 - val_loss: 0.2645 - val_accuracy: 0.8933\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8928 - val_loss: 0.2592 - val_accuracy: 0.8956\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2620 - accuracy: 0.8937 - val_loss: 0.2654 - val_accuracy: 0.8949\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2607 - accuracy: 0.8962 - val_loss: 0.2678 - val_accuracy: 0.8942\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2662 - accuracy: 0.8927 - val_loss: 0.2632 - val_accuracy: 0.8956\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2660 - accuracy: 0.8928 - val_loss: 0.2663 - val_accuracy: 0.8947\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2661 - accuracy: 0.8927 - val_loss: 0.2603 - val_accuracy: 0.8952\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8945 - val_loss: 0.2621 - val_accuracy: 0.8942\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2667 - accuracy: 0.8929 - val_loss: 0.2646 - val_accuracy: 0.8952\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8907 - val_loss: 0.2609 - val_accuracy: 0.8954\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2622 - accuracy: 0.8941 - val_loss: 0.2598 - val_accuracy: 0.8959\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2592 - accuracy: 0.8960 - val_loss: 0.2596 - val_accuracy: 0.8960\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2620 - accuracy: 0.8959 - val_loss: 0.2615 - val_accuracy: 0.8967\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2610 - accuracy: 0.8965 - val_loss: 0.2586 - val_accuracy: 0.8963\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8956 - val_loss: 0.2645 - val_accuracy: 0.8958\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.8967 - val_loss: 0.2611 - val_accuracy: 0.8959\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2631 - accuracy: 0.8954 - val_loss: 0.2571 - val_accuracy: 0.8961\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2595 - accuracy: 0.8959 - val_loss: 0.2637 - val_accuracy: 0.8926\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2623 - accuracy: 0.8932 - val_loss: 0.2635 - val_accuracy: 0.8961\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8937 - val_loss: 0.2597 - val_accuracy: 0.8952\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8951 - val_loss: 0.2571 - val_accuracy: 0.8964\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8937 - val_loss: 0.2569 - val_accuracy: 0.8959\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2583 - accuracy: 0.8972\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2604 - accuracy: 0.8955\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2600 - accuracy: 0.8961\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2593 - accuracy: 0.8958\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2599 - accuracy: 0.8962\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2596 - accuracy: 0.8959\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2590 - accuracy: 0.8959\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2581 - accuracy: 0.8968\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2592 - accuracy: 0.8959\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2593 - accuracy: 0.8956\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2603 - accuracy: 0.8953\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.5950 - accuracy: 0.8495 - val_loss: 0.3765 - val_accuracy: 0.8565\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3171 - accuracy: 0.8802 - val_loss: 0.3216 - val_accuracy: 0.8924\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2971 - accuracy: 0.8827 - val_loss: 0.2959 - val_accuracy: 0.8925\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2917 - accuracy: 0.8813 - val_loss: 0.2864 - val_accuracy: 0.8926\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2859 - accuracy: 0.8878 - val_loss: 0.2961 - val_accuracy: 0.8910\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2842 - accuracy: 0.8875 - val_loss: 0.2940 - val_accuracy: 0.8918\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8879 - val_loss: 0.2726 - val_accuracy: 0.8921\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8861 - val_loss: 0.2807 - val_accuracy: 0.8917\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2752 - accuracy: 0.8860 - val_loss: 0.2708 - val_accuracy: 0.8925\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2740 - accuracy: 0.8896 - val_loss: 0.2676 - val_accuracy: 0.8932\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2793 - accuracy: 0.8872 - val_loss: 0.2666 - val_accuracy: 0.8932\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8936 - val_loss: 0.2778 - val_accuracy: 0.8910\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8914 - val_loss: 0.2697 - val_accuracy: 0.8940\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2686 - accuracy: 0.8925 - val_loss: 0.2743 - val_accuracy: 0.8890\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2695 - accuracy: 0.8913 - val_loss: 0.2717 - val_accuracy: 0.8930\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2699 - accuracy: 0.8901 - val_loss: 0.2634 - val_accuracy: 0.8940\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.8939 - val_loss: 0.2673 - val_accuracy: 0.8927\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2703 - accuracy: 0.8907 - val_loss: 0.2726 - val_accuracy: 0.8943\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2680 - accuracy: 0.8899 - val_loss: 0.2678 - val_accuracy: 0.8946\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2659 - accuracy: 0.8919 - val_loss: 0.2652 - val_accuracy: 0.8946\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2697 - accuracy: 0.8900 - val_loss: 0.2662 - val_accuracy: 0.8937\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2638 - accuracy: 0.8938 - val_loss: 0.2642 - val_accuracy: 0.8939\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8923 - val_loss: 0.2647 - val_accuracy: 0.8949\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2703 - accuracy: 0.8918 - val_loss: 0.2575 - val_accuracy: 0.8957\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2653 - accuracy: 0.8929 - val_loss: 0.2660 - val_accuracy: 0.8954\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8919 - val_loss: 0.2687 - val_accuracy: 0.8951\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2598 - accuracy: 0.8966 - val_loss: 0.2619 - val_accuracy: 0.8954\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2630 - accuracy: 0.8931 - val_loss: 0.2602 - val_accuracy: 0.8946\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8946 - val_loss: 0.2619 - val_accuracy: 0.8957\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2664 - accuracy: 0.8923 - val_loss: 0.2652 - val_accuracy: 0.8948\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2634 - accuracy: 0.8940 - val_loss: 0.2609 - val_accuracy: 0.8959\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8952 - val_loss: 0.2622 - val_accuracy: 0.8946\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2701 - accuracy: 0.8914 - val_loss: 0.2614 - val_accuracy: 0.8961\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2604 - accuracy: 0.8957 - val_loss: 0.2604 - val_accuracy: 0.8962\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8927 - val_loss: 0.2600 - val_accuracy: 0.8968\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2680 - accuracy: 0.8918 - val_loss: 0.2657 - val_accuracy: 0.8937\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2657 - accuracy: 0.8915 - val_loss: 0.2610 - val_accuracy: 0.8960\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2594 - accuracy: 0.8957 - val_loss: 0.2636 - val_accuracy: 0.8968\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2654 - accuracy: 0.8921 - val_loss: 0.2649 - val_accuracy: 0.8954\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2569 - accuracy: 0.8974 - val_loss: 0.2602 - val_accuracy: 0.8956\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2620 - accuracy: 0.8945 - val_loss: 0.2607 - val_accuracy: 0.8952\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8944 - val_loss: 0.2578 - val_accuracy: 0.8951\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2664 - accuracy: 0.8936 - val_loss: 0.2639 - val_accuracy: 0.8961\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2601 - accuracy: 0.8957 - val_loss: 0.2618 - val_accuracy: 0.8967\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2626 - accuracy: 0.8950 - val_loss: 0.2563 - val_accuracy: 0.8972\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2631 - accuracy: 0.8949 - val_loss: 0.2644 - val_accuracy: 0.8947\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.8949 - val_loss: 0.2599 - val_accuracy: 0.8959\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2612 - accuracy: 0.8947 - val_loss: 0.2638 - val_accuracy: 0.8958\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2672 - accuracy: 0.8910 - val_loss: 0.2611 - val_accuracy: 0.8951\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2632 - accuracy: 0.8928 - val_loss: 0.2572 - val_accuracy: 0.8955\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2592 - accuracy: 0.8965\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2634 - accuracy: 0.8950\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2626 - accuracy: 0.8950\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2630 - accuracy: 0.8944\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2621 - accuracy: 0.8954\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2625 - accuracy: 0.8953\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2614 - accuracy: 0.8950\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2618 - accuracy: 0.8958\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2613 - accuracy: 0.8960\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2664 - accuracy: 0.8940\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2622 - accuracy: 0.8954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.5939 - accuracy: 0.8325 - val_loss: 0.3090 - val_accuracy: 0.8925\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3125 - accuracy: 0.8833 - val_loss: 0.3300 - val_accuracy: 0.8883\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2945 - accuracy: 0.8851 - val_loss: 0.2884 - val_accuracy: 0.8928\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2820 - accuracy: 0.8903 - val_loss: 0.2969 - val_accuracy: 0.8923\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2881 - accuracy: 0.8872 - val_loss: 0.3059 - val_accuracy: 0.8922\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2823 - accuracy: 0.8887 - val_loss: 0.3153 - val_accuracy: 0.8924\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2758 - accuracy: 0.8905 - val_loss: 0.2873 - val_accuracy: 0.8938\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2743 - accuracy: 0.8916 - val_loss: 0.2766 - val_accuracy: 0.8946\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.8918 - val_loss: 0.2659 - val_accuracy: 0.8942\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2664 - accuracy: 0.8939 - val_loss: 0.2854 - val_accuracy: 0.8939\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2714 - accuracy: 0.8915 - val_loss: 0.2668 - val_accuracy: 0.8951\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2696 - accuracy: 0.8916 - val_loss: 0.2721 - val_accuracy: 0.8961\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2683 - accuracy: 0.8929 - val_loss: 0.2678 - val_accuracy: 0.8962\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.8943 - val_loss: 0.2608 - val_accuracy: 0.8961\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2637 - accuracy: 0.8944 - val_loss: 0.2610 - val_accuracy: 0.8967\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2626 - accuracy: 0.8953 - val_loss: 0.2689 - val_accuracy: 0.8947\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8931 - val_loss: 0.2627 - val_accuracy: 0.8949\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2671 - accuracy: 0.8930 - val_loss: 0.2635 - val_accuracy: 0.8971\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8930 - val_loss: 0.2647 - val_accuracy: 0.8959\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2676 - accuracy: 0.8925 - val_loss: 0.2668 - val_accuracy: 0.8953\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8933 - val_loss: 0.2605 - val_accuracy: 0.8965\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2649 - accuracy: 0.8927 - val_loss: 0.2782 - val_accuracy: 0.8920\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2675 - accuracy: 0.8917 - val_loss: 0.2685 - val_accuracy: 0.8959\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2632 - accuracy: 0.8943 - val_loss: 0.2638 - val_accuracy: 0.8942\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.8939 - val_loss: 0.2627 - val_accuracy: 0.8950\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8953 - val_loss: 0.2678 - val_accuracy: 0.8956\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2593 - accuracy: 0.8957 - val_loss: 0.2643 - val_accuracy: 0.8961\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8933 - val_loss: 0.2571 - val_accuracy: 0.8961\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8956 - val_loss: 0.2660 - val_accuracy: 0.8945\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2622 - accuracy: 0.8943 - val_loss: 0.2607 - val_accuracy: 0.8966\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8956 - val_loss: 0.2635 - val_accuracy: 0.8961\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2615 - accuracy: 0.8956 - val_loss: 0.2574 - val_accuracy: 0.8969\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2616 - accuracy: 0.8961 - val_loss: 0.2607 - val_accuracy: 0.8935\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2662 - accuracy: 0.8985\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2649 - accuracy: 0.8940\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2639 - accuracy: 0.8937\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2640 - accuracy: 0.8940\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2626 - accuracy: 0.8948\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2623 - accuracy: 0.8944\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2620 - accuracy: 0.8943\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2616 - accuracy: 0.8950\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2608 - accuracy: 0.8951\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2612 - accuracy: 0.8950\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2612 - accuracy: 0.8954\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
            "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:  6.3min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKvpjui1JjlJ",
        "outputId": "34a7d731-e9ff-494b-cc94-4598d1caa3ec"
      },
      "source": [
        "cm3 = confusion_matrix(y, cv3_preds)\n",
        "print(cm3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[53239  4761]\n",
            " [ 4368 26279]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "id": "6P3z_04WoQPL",
        "outputId": "dd5640c8-5668-48a2-a6e8-774e92c6fefe"
      },
      "source": [
        "'''\n",
        "\n",
        "dropout_rate = [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4]\n",
        "param_grid = dict(dropout_rate=dropout_rate)\n",
        "grid3 = GridSearchCV(estimator=mod3, param_grid=param_grid, cv=10)\n",
        "grid_result3 = grid3.fit(X, y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result3.best_score_, grid_result3.best_params_))\n",
        "for params, mean_score, scores in grid_result3.grid_scores_:\n",
        "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\ndropout_rate = [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4]\\nparam_grid = dict(dropout_rate=dropout_rate)\\ngrid3 = GridSearchCV(estimator=mod3, param_grid=param_grid, cv=10)\\ngrid_result3 = grid3.fit(X, y)\\n# summarize results\\nprint(\"Best: %f using %s\" % (grid_result3.best_score_, grid_result3.best_params_))\\nfor params, mean_score, scores in grid_result3.grid_scores_:\\n    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 124
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVrPsUyf5_lf"
      },
      "source": [
        "# neural network on dataset attributes based on URL file name (table 4)\n",
        "\n",
        "### (18 features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Fz80e4Uo5_lg",
        "outputId": "f5268e5a-b9eb-4a7c-e7c4-9cefbc6183b2"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table1 = ['qty_dot_file', 'qty_hyphen_file', 'qty_underline_file', 'qty_slash_file', 'qty_questionmark_file', 'qty_equal_file', 'qty_at_file', 'qty_and_file',\n",
        "                   'qty_exclamation_file', 'qty_space_file', 'qty_tilde_file', 'qty_comma_file', 'qty_plus_file', 'qty_asterisk_file', 'qty_hashtag_file', 'qty_dollar_file',\n",
        "                   'qty_percent_file', 'file_length'] \n",
        "\n",
        "X = full_df[features_table1]\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_file</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>qty_underline_file</th>\n",
              "      <th>qty_slash_file</th>\n",
              "      <th>qty_questionmark_file</th>\n",
              "      <th>qty_equal_file</th>\n",
              "      <th>qty_at_file</th>\n",
              "      <th>qty_and_file</th>\n",
              "      <th>qty_exclamation_file</th>\n",
              "      <th>qty_space_file</th>\n",
              "      <th>qty_tilde_file</th>\n",
              "      <th>qty_comma_file</th>\n",
              "      <th>qty_plus_file</th>\n",
              "      <th>qty_asterisk_file</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_file  qty_hyphen_file  ...  qty_percent_file  file_length\n",
              "5676              0                0  ...                 0            0\n",
              "39002             0                0  ...                 0            0\n",
              "1732              0                0  ...                 0            0\n",
              "39668             0                0  ...                 0            0\n",
              "82035             0                0  ...                 0            0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 125
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fhFsakMB5_lg",
        "outputId": "ea0ae0a6-15fa-4fab-fde1-f8de05342a1f"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88647, 18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc1R2ttj5_lh",
        "outputId": "63026506-50a5-4de7-ab63-b5b0deb5ae38"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "table4_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[18]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "table4_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "table4_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                1216      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 14,113\n",
            "Trainable params: 14,113\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g3ZPWdH5_lh",
        "outputId": "1fd1f94d-8500-4753-dbd4-8d37af97801f"
      },
      "source": [
        "history = table4_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6279 - binary_accuracy: 0.7818 - auc: 0.7406 - val_loss: 0.5078 - val_binary_accuracy: 0.7895 - val_auc: 0.7632\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4985 - binary_accuracy: 0.8008 - auc: 0.7582 - val_loss: 0.5015 - val_binary_accuracy: 0.8121 - val_auc: 0.7638\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4890 - binary_accuracy: 0.8059 - auc: 0.7565 - val_loss: 0.4806 - val_binary_accuracy: 0.8131 - val_auc: 0.7640\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4803 - binary_accuracy: 0.8095 - auc: 0.7613 - val_loss: 0.4852 - val_binary_accuracy: 0.8127 - val_auc: 0.7643\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4806 - binary_accuracy: 0.8102 - auc: 0.7586 - val_loss: 0.4753 - val_binary_accuracy: 0.8120 - val_auc: 0.7641\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4772 - binary_accuracy: 0.8104 - auc: 0.7645 - val_loss: 0.4822 - val_binary_accuracy: 0.8122 - val_auc: 0.7648\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4854 - binary_accuracy: 0.8050 - auc: 0.7557 - val_loss: 0.4785 - val_binary_accuracy: 0.8130 - val_auc: 0.7654\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4794 - binary_accuracy: 0.8090 - auc: 0.7618 - val_loss: 0.4840 - val_binary_accuracy: 0.8116 - val_auc: 0.7656\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4811 - binary_accuracy: 0.8088 - auc: 0.7599 - val_loss: 0.4801 - val_binary_accuracy: 0.8132 - val_auc: 0.7656\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4791 - binary_accuracy: 0.8097 - auc: 0.7598 - val_loss: 0.4752 - val_binary_accuracy: 0.8134 - val_auc: 0.7657\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4829 - binary_accuracy: 0.8062 - auc: 0.7575 - val_loss: 0.4752 - val_binary_accuracy: 0.8135 - val_auc: 0.7658\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4770 - binary_accuracy: 0.8099 - auc: 0.7570 - val_loss: 0.4748 - val_binary_accuracy: 0.8135 - val_auc: 0.7661\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4749 - binary_accuracy: 0.8121 - auc: 0.7637 - val_loss: 0.4767 - val_binary_accuracy: 0.8131 - val_auc: 0.7657\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4814 - binary_accuracy: 0.8072 - auc: 0.7565 - val_loss: 0.4719 - val_binary_accuracy: 0.8149 - val_auc: 0.7662\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4834 - binary_accuracy: 0.8068 - auc: 0.7535 - val_loss: 0.4737 - val_binary_accuracy: 0.8149 - val_auc: 0.7665\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4789 - binary_accuracy: 0.8090 - auc: 0.7596 - val_loss: 0.4748 - val_binary_accuracy: 0.8149 - val_auc: 0.7660\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4774 - binary_accuracy: 0.8096 - auc: 0.7575 - val_loss: 0.4749 - val_binary_accuracy: 0.8145 - val_auc: 0.7665\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4737 - binary_accuracy: 0.8122 - auc: 0.7641 - val_loss: 0.4753 - val_binary_accuracy: 0.8142 - val_auc: 0.7662\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4775 - binary_accuracy: 0.8105 - auc: 0.7614 - val_loss: 0.4742 - val_binary_accuracy: 0.8151 - val_auc: 0.7664\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4790 - binary_accuracy: 0.8084 - auc: 0.7563 - val_loss: 0.4710 - val_binary_accuracy: 0.8151 - val_auc: 0.7666\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4750 - binary_accuracy: 0.8114 - auc: 0.7603 - val_loss: 0.4758 - val_binary_accuracy: 0.8149 - val_auc: 0.7667\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4794 - binary_accuracy: 0.8087 - auc: 0.7590 - val_loss: 0.4742 - val_binary_accuracy: 0.8146 - val_auc: 0.7664\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4799 - binary_accuracy: 0.8086 - auc: 0.7576 - val_loss: 0.4729 - val_binary_accuracy: 0.8140 - val_auc: 0.7650\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4796 - binary_accuracy: 0.8084 - auc: 0.7592 - val_loss: 0.4746 - val_binary_accuracy: 0.8148 - val_auc: 0.7668\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4762 - binary_accuracy: 0.8099 - auc: 0.7599 - val_loss: 0.4755 - val_binary_accuracy: 0.8145 - val_auc: 0.7665\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4740 - binary_accuracy: 0.8120 - auc: 0.7639 - val_loss: 0.4703 - val_binary_accuracy: 0.8151 - val_auc: 0.7665\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4751 - binary_accuracy: 0.8121 - auc: 0.7618 - val_loss: 0.4728 - val_binary_accuracy: 0.8146 - val_auc: 0.7663\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4754 - binary_accuracy: 0.8126 - auc: 0.7589 - val_loss: 0.4717 - val_binary_accuracy: 0.8150 - val_auc: 0.7663\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4739 - binary_accuracy: 0.8120 - auc: 0.7572 - val_loss: 0.4738 - val_binary_accuracy: 0.8148 - val_auc: 0.7652\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4799 - binary_accuracy: 0.8081 - auc: 0.7596 - val_loss: 0.4721 - val_binary_accuracy: 0.8151 - val_auc: 0.7666\n",
            "Epoch 31/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4724 - binary_accuracy: 0.8131 - auc: 0.7631 - val_loss: 0.4769 - val_binary_accuracy: 0.8151 - val_auc: 0.7664\n",
            "Epoch 32/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4769 - binary_accuracy: 0.8108 - auc: 0.7599 - val_loss: 0.4743 - val_binary_accuracy: 0.8151 - val_auc: 0.7667\n",
            "Epoch 33/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4762 - binary_accuracy: 0.8099 - auc: 0.7612 - val_loss: 0.4727 - val_binary_accuracy: 0.8149 - val_auc: 0.7661\n",
            "Epoch 34/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4707 - binary_accuracy: 0.8129 - auc: 0.7673 - val_loss: 0.4724 - val_binary_accuracy: 0.8153 - val_auc: 0.7666\n",
            "Epoch 35/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4707 - binary_accuracy: 0.8141 - auc: 0.7609 - val_loss: 0.4746 - val_binary_accuracy: 0.8151 - val_auc: 0.7665\n",
            "Epoch 36/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4776 - binary_accuracy: 0.8099 - auc: 0.7608 - val_loss: 0.4714 - val_binary_accuracy: 0.8145 - val_auc: 0.7662\n",
            "Epoch 37/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4766 - binary_accuracy: 0.8110 - auc: 0.7573 - val_loss: 0.4719 - val_binary_accuracy: 0.8151 - val_auc: 0.7667\n",
            "Epoch 38/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4737 - binary_accuracy: 0.8120 - auc: 0.7620 - val_loss: 0.4728 - val_binary_accuracy: 0.8153 - val_auc: 0.7661\n",
            "Epoch 39/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4697 - binary_accuracy: 0.8150 - auc: 0.7635 - val_loss: 0.4705 - val_binary_accuracy: 0.8151 - val_auc: 0.7665\n",
            "Epoch 40/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4738 - binary_accuracy: 0.8122 - auc: 0.7591 - val_loss: 0.4743 - val_binary_accuracy: 0.8152 - val_auc: 0.7651\n",
            "Epoch 41/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4750 - binary_accuracy: 0.8115 - auc: 0.7608 - val_loss: 0.4716 - val_binary_accuracy: 0.8145 - val_auc: 0.7664\n",
            "Epoch 42/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4759 - binary_accuracy: 0.8112 - auc: 0.7564 - val_loss: 0.4709 - val_binary_accuracy: 0.8151 - val_auc: 0.7668\n",
            "Epoch 43/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4748 - binary_accuracy: 0.8111 - auc: 0.7620 - val_loss: 0.4711 - val_binary_accuracy: 0.8147 - val_auc: 0.7669\n",
            "Epoch 44/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4713 - binary_accuracy: 0.8128 - auc: 0.7653 - val_loss: 0.4714 - val_binary_accuracy: 0.8150 - val_auc: 0.7667\n",
            "Epoch 45/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4739 - binary_accuracy: 0.8118 - auc: 0.7620 - val_loss: 0.4717 - val_binary_accuracy: 0.8146 - val_auc: 0.7665\n",
            "Epoch 46/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4724 - binary_accuracy: 0.8120 - auc: 0.7643 - val_loss: 0.4719 - val_binary_accuracy: 0.8150 - val_auc: 0.7669\n",
            "Epoch 47/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4759 - binary_accuracy: 0.8100 - auc: 0.7634 - val_loss: 0.4706 - val_binary_accuracy: 0.8152 - val_auc: 0.7667\n",
            "Epoch 48/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4722 - binary_accuracy: 0.8133 - auc: 0.7611 - val_loss: 0.4715 - val_binary_accuracy: 0.8146 - val_auc: 0.7663\n",
            "Epoch 49/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4737 - binary_accuracy: 0.8122 - auc: 0.7614 - val_loss: 0.4717 - val_binary_accuracy: 0.8147 - val_auc: 0.7661\n",
            "Epoch 50/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4724 - binary_accuracy: 0.8134 - auc: 0.7625 - val_loss: 0.4737 - val_binary_accuracy: 0.8139 - val_auc: 0.7661\n",
            "Epoch 51/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4771 - binary_accuracy: 0.8093 - auc: 0.7602 - val_loss: 0.4710 - val_binary_accuracy: 0.8151 - val_auc: 0.7666\n",
            "Epoch 52/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4743 - binary_accuracy: 0.8118 - auc: 0.7608 - val_loss: 0.4709 - val_binary_accuracy: 0.8151 - val_auc: 0.7667\n",
            "Epoch 53/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4700 - binary_accuracy: 0.8140 - auc: 0.7625 - val_loss: 0.4712 - val_binary_accuracy: 0.8154 - val_auc: 0.7668\n",
            "Epoch 54/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4721 - binary_accuracy: 0.8121 - auc: 0.7644 - val_loss: 0.4708 - val_binary_accuracy: 0.8147 - val_auc: 0.7667\n",
            "Epoch 55/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4697 - binary_accuracy: 0.8148 - auc: 0.7629 - val_loss: 0.4719 - val_binary_accuracy: 0.8149 - val_auc: 0.7655\n",
            "Epoch 56/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4764 - binary_accuracy: 0.8103 - auc: 0.7648 - val_loss: 0.4712 - val_binary_accuracy: 0.8149 - val_auc: 0.7664\n",
            "Epoch 57/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4739 - binary_accuracy: 0.8128 - auc: 0.7600 - val_loss: 0.4719 - val_binary_accuracy: 0.8124 - val_auc: 0.7662\n",
            "Epoch 58/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4693 - binary_accuracy: 0.8155 - auc: 0.7649 - val_loss: 0.4713 - val_binary_accuracy: 0.8152 - val_auc: 0.7664\n",
            "Epoch 59/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4731 - binary_accuracy: 0.8130 - auc: 0.7565 - val_loss: 0.4720 - val_binary_accuracy: 0.8141 - val_auc: 0.7653\n",
            "Epoch 60/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4734 - binary_accuracy: 0.8122 - auc: 0.7625 - val_loss: 0.4714 - val_binary_accuracy: 0.8145 - val_auc: 0.7665\n",
            "Epoch 61/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4721 - binary_accuracy: 0.8130 - auc: 0.7628 - val_loss: 0.4711 - val_binary_accuracy: 0.8149 - val_auc: 0.7664\n",
            "Epoch 62/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4742 - binary_accuracy: 0.8110 - auc: 0.7601 - val_loss: 0.4726 - val_binary_accuracy: 0.8147 - val_auc: 0.7661\n",
            "Epoch 63/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4707 - binary_accuracy: 0.8136 - auc: 0.7631 - val_loss: 0.4719 - val_binary_accuracy: 0.8150 - val_auc: 0.7665\n",
            "Epoch 64/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4733 - binary_accuracy: 0.8125 - auc: 0.7579 - val_loss: 0.4712 - val_binary_accuracy: 0.8150 - val_auc: 0.7662\n",
            "Epoch 65/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4733 - binary_accuracy: 0.8117 - auc: 0.7578 - val_loss: 0.4709 - val_binary_accuracy: 0.8150 - val_auc: 0.7663\n",
            "Epoch 66/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4716 - binary_accuracy: 0.8127 - auc: 0.7616 - val_loss: 0.4715 - val_binary_accuracy: 0.8146 - val_auc: 0.7666\n",
            "Epoch 67/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4724 - binary_accuracy: 0.8143 - auc: 0.7633 - val_loss: 0.4769 - val_binary_accuracy: 0.8121 - val_auc: 0.7663\n",
            "Epoch 68/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4722 - binary_accuracy: 0.8126 - auc: 0.7664 - val_loss: 0.4724 - val_binary_accuracy: 0.8147 - val_auc: 0.7665\n",
            "Epoch 69/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4671 - binary_accuracy: 0.8162 - auc: 0.7636 - val_loss: 0.4721 - val_binary_accuracy: 0.8149 - val_auc: 0.7656\n",
            "Epoch 70/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4709 - binary_accuracy: 0.8146 - auc: 0.7646 - val_loss: 0.4723 - val_binary_accuracy: 0.8145 - val_auc: 0.7661\n",
            "Epoch 71/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4747 - binary_accuracy: 0.8118 - auc: 0.7589 - val_loss: 0.4708 - val_binary_accuracy: 0.8150 - val_auc: 0.7667\n",
            "Epoch 72/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4744 - binary_accuracy: 0.8117 - auc: 0.7584 - val_loss: 0.4722 - val_binary_accuracy: 0.8145 - val_auc: 0.7651\n",
            "Epoch 73/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4713 - binary_accuracy: 0.8135 - auc: 0.7662 - val_loss: 0.4711 - val_binary_accuracy: 0.8151 - val_auc: 0.7669\n",
            "Epoch 74/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4733 - binary_accuracy: 0.8123 - auc: 0.7615 - val_loss: 0.4710 - val_binary_accuracy: 0.8150 - val_auc: 0.7669\n",
            "Epoch 75/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4748 - binary_accuracy: 0.8115 - auc: 0.7586 - val_loss: 0.4724 - val_binary_accuracy: 0.8150 - val_auc: 0.7670\n",
            "Epoch 76/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4727 - binary_accuracy: 0.8129 - auc: 0.7634 - val_loss: 0.4728 - val_binary_accuracy: 0.8147 - val_auc: 0.7667\n",
            "Epoch 77/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4723 - binary_accuracy: 0.8142 - auc: 0.7618 - val_loss: 0.4722 - val_binary_accuracy: 0.8151 - val_auc: 0.7672\n",
            "Epoch 78/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4750 - binary_accuracy: 0.8105 - auc: 0.7608 - val_loss: 0.4725 - val_binary_accuracy: 0.8144 - val_auc: 0.7661\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "CeqLdowL5_lh",
        "outputId": "2a559135-6a8d-4f83-c6e9-2797abac6499"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "history_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>78.000000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>78.000000</td>\n",
              "      <td>78.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.476857</td>\n",
              "      <td>0.810889</td>\n",
              "      <td>0.761227</td>\n",
              "      <td>0.474175</td>\n",
              "      <td>0.814132</td>\n",
              "      <td>0.766123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.012312</td>\n",
              "      <td>0.002779</td>\n",
              "      <td>0.002623</td>\n",
              "      <td>0.005839</td>\n",
              "      <td>0.002961</td>\n",
              "      <td>0.000776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.472327</td>\n",
              "      <td>0.791852</td>\n",
              "      <td>0.744801</td>\n",
              "      <td>0.470258</td>\n",
              "      <td>0.789532</td>\n",
              "      <td>0.763244</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.473347</td>\n",
              "      <td>0.810632</td>\n",
              "      <td>0.760276</td>\n",
              "      <td>0.471381</td>\n",
              "      <td>0.814424</td>\n",
              "      <td>0.766020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.474394</td>\n",
              "      <td>0.811803</td>\n",
              "      <td>0.761579</td>\n",
              "      <td>0.472325</td>\n",
              "      <td>0.814800</td>\n",
              "      <td>0.766362</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.476338</td>\n",
              "      <td>0.812136</td>\n",
              "      <td>0.762697</td>\n",
              "      <td>0.474716</td>\n",
              "      <td>0.815038</td>\n",
              "      <td>0.766623</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.579457</td>\n",
              "      <td>0.812759</td>\n",
              "      <td>0.766824</td>\n",
              "      <td>0.507849</td>\n",
              "      <td>0.815351</td>\n",
              "      <td>0.767213</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  78.000000        78.000000  ...            78.000000  78.000000\n",
              "mean    0.476857         0.810889  ...             0.814132   0.766123\n",
              "std     0.012312         0.002779  ...             0.002961   0.000776\n",
              "min     0.472327         0.791852  ...             0.789532   0.763244\n",
              "25%     0.473347         0.810632  ...             0.814424   0.766020\n",
              "50%     0.474394         0.811803  ...             0.814800   0.766362\n",
              "75%     0.476338         0.812136  ...             0.815038   0.766623\n",
              "max     0.579457         0.812759  ...             0.815351   0.767213\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NMuuD2v95_li",
        "outputId": "d277dae4-79cd-4bd6-d13b-d4e507c97042"
      },
      "source": [
        "train_acc = table4_nn.evaluate(train_X, train_y)\n",
        "test_acc = table4_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 1ms/step - loss: 0.4710 - binary_accuracy: 0.8140 - auc: 0.7646\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4730 - binary_accuracy: 0.8123 - auc: 0.7616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5J-Hq0M5_li",
        "outputId": "03b41aa3-41dd-4f39-9699-a79c06452e47"
      },
      "source": [
        "dict(zip(table4_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.7615656852722168,\n",
              " 'binary_accuracy': 0.8123364448547363,\n",
              " 'loss': 0.47299692034721375}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "tueLGNPE5_li",
        "outputId": "bfc70afd-586b-4fca-992a-aba6c1fc1200"
      },
      "source": [
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.47025832533836365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "sl0UetwG5_li",
        "outputId": "4745fd26-6ca0-42f2-d90d-2fed47ed6094"
      },
      "source": [
        "history_df.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.7672128677368164\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "9ScDedYC5_lj",
        "outputId": "23e05a5f-c728-4be5-e7c5-676cc8168822"
      },
      "source": [
        "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.8153514266014099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtqeVqaV5_lj",
        "outputId": "510c2bd4-be7c-43be-dead-3fde36e940ed"
      },
      "source": [
        "preds1 = table4_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.87461555],\n",
              "       [0.19973338],\n",
              "       [0.6438374 ],\n",
              "       ...,\n",
              "       [0.19973338],\n",
              "       [0.19973338],\n",
              "       [0.8664142 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w77GDCJy5_lj",
        "outputId": "fa988e34-f6a7-4f85-e27a-97bb714b6995"
      },
      "source": [
        "len(preds1[preds1 < 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ypvCcW_5_lj",
        "outputId": "2287446c-13d9-48f0-c856-6147dc039343"
      },
      "source": [
        "len(preds1[preds1 >= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4962"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOU9X9SA5_lk",
        "outputId": "c546b3d5-7ff8-4b61-ade6-a9a9c7e78e6e"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUiMHraYpCcT",
        "outputId": "38cdfee0-4c4d-4a5f-8fef-d3dd8ac1cd5e"
      },
      "source": [
        "pred_classes = np.argmax(preds1, axis=1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(val_y, pred_classes)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14519     0]\n",
            " [ 7643     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYKWx22aoQPL"
      },
      "source": [
        "# NN on URL file name attributes (table 4) [model as function]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "V3O2FqWFoQPL",
        "outputId": "db9c85a2-4ec7-4a9a-c119-954a39ec525e"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table4 = ['qty_dot_file', 'qty_hyphen_file', 'qty_underline_file', 'qty_slash_file', 'qty_questionmark_file', 'qty_equal_file', 'qty_at_file', 'qty_and_file',\n",
        "                   'qty_exclamation_file', 'qty_space_file', 'qty_tilde_file', 'qty_comma_file', 'qty_plus_file', 'qty_asterisk_file', 'qty_hashtag_file', 'qty_dollar_file',\n",
        "                   'qty_percent_file', 'file_length'] \n",
        "\n",
        "X = full_df[features_table4]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_file</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>qty_underline_file</th>\n",
              "      <th>qty_slash_file</th>\n",
              "      <th>qty_questionmark_file</th>\n",
              "      <th>qty_equal_file</th>\n",
              "      <th>qty_at_file</th>\n",
              "      <th>qty_and_file</th>\n",
              "      <th>qty_exclamation_file</th>\n",
              "      <th>qty_space_file</th>\n",
              "      <th>qty_tilde_file</th>\n",
              "      <th>qty_comma_file</th>\n",
              "      <th>qty_plus_file</th>\n",
              "      <th>qty_asterisk_file</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_file  qty_hyphen_file  ...  qty_percent_file  file_length\n",
              "5676            0.0              0.0  ...               0.0          0.0\n",
              "39002           0.0              0.0  ...               0.0          0.0\n",
              "1732            0.0              0.0  ...               0.0          0.0\n",
              "39668           0.0              0.0  ...               0.0          0.0\n",
              "82035           0.0              0.0  ...               0.0          0.0\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wArXwQcFoQPM"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_4():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[18]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbG3ttQQoQPM"
      },
      "source": [
        "mod4 = KerasClassifier(build_fn=phish_nn_4,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qgj7sDZioQPM",
        "outputId": "ddec488c-4002-4287-fc3f-74109b88d1b1"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_4=cross_val_score(mod4,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6491 - accuracy: 0.7736 - val_loss: 0.5922 - val_accuracy: 0.8124\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5764 - accuracy: 0.8066 - val_loss: 0.5652 - val_accuracy: 0.8119\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5400 - accuracy: 0.8076 - val_loss: 0.5373 - val_accuracy: 0.8130\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5159 - accuracy: 0.8081 - val_loss: 0.5279 - val_accuracy: 0.8116\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5021 - accuracy: 0.8082 - val_loss: 0.5150 - val_accuracy: 0.8121\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4983 - accuracy: 0.8053 - val_loss: 0.5149 - val_accuracy: 0.8103\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4897 - accuracy: 0.8079 - val_loss: 0.5003 - val_accuracy: 0.8130\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4794 - accuracy: 0.8126 - val_loss: 0.4923 - val_accuracy: 0.8138\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4855 - accuracy: 0.8068 - val_loss: 0.4942 - val_accuracy: 0.8133\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4849 - accuracy: 0.8062 - val_loss: 0.4786 - val_accuracy: 0.8141\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4806 - accuracy: 0.8102 - val_loss: 0.4905 - val_accuracy: 0.8143\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4820 - accuracy: 0.8069 - val_loss: 0.4770 - val_accuracy: 0.8140\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4796 - accuracy: 0.8096 - val_loss: 0.4897 - val_accuracy: 0.8138\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4856 - accuracy: 0.8048 - val_loss: 0.4968 - val_accuracy: 0.8130\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4802 - accuracy: 0.8090 - val_loss: 0.4795 - val_accuracy: 0.8140\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4784 - accuracy: 0.8110 - val_loss: 0.4894 - val_accuracy: 0.8113\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4812 - accuracy: 0.8095 - val_loss: 0.4848 - val_accuracy: 0.8131\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4837 - accuracy: 0.8070 - val_loss: 0.4880 - val_accuracy: 0.8139\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4832 - accuracy: 0.8074 - val_loss: 0.4823 - val_accuracy: 0.8124\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4778 - accuracy: 0.8118 - val_loss: 0.4800 - val_accuracy: 0.8139\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4831 - accuracy: 0.8078 - val_loss: 0.4809 - val_accuracy: 0.8137\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4838 - accuracy: 0.8061 - val_loss: 0.4779 - val_accuracy: 0.8138\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4794 - accuracy: 0.8103 - val_loss: 0.4858 - val_accuracy: 0.8142\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4783 - accuracy: 0.8113 - val_loss: 0.4830 - val_accuracy: 0.8132\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4789 - accuracy: 0.8100 - val_loss: 0.4758 - val_accuracy: 0.8140\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4823 - accuracy: 0.8073 - val_loss: 0.4812 - val_accuracy: 0.8136\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4967 - accuracy: 0.8100\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4806 - accuracy: 0.8093\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4807 - accuracy: 0.8093\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4802 - accuracy: 0.8095\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4797 - accuracy: 0.8095\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4807 - accuracy: 0.8094\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4796 - accuracy: 0.8097\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4800 - accuracy: 0.8094\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4794 - accuracy: 0.8091\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4795 - accuracy: 0.8099\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4795 - accuracy: 0.8101\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.4888 - accuracy: 0.8123\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6016 - accuracy: 0.7825 - val_loss: 0.5139 - val_accuracy: 0.8113\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5020 - accuracy: 0.7992 - val_loss: 0.5038 - val_accuracy: 0.8103\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4892 - accuracy: 0.8059 - val_loss: 0.4946 - val_accuracy: 0.8130\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4839 - accuracy: 0.8090 - val_loss: 0.4897 - val_accuracy: 0.8133\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4806 - accuracy: 0.8092 - val_loss: 0.4893 - val_accuracy: 0.8132\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4800 - accuracy: 0.8103 - val_loss: 0.4859 - val_accuracy: 0.8133\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4827 - accuracy: 0.8085 - val_loss: 0.4781 - val_accuracy: 0.8144\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4828 - accuracy: 0.8072 - val_loss: 0.4736 - val_accuracy: 0.8139\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4816 - accuracy: 0.8090 - val_loss: 0.4782 - val_accuracy: 0.8145\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4747 - accuracy: 0.8129 - val_loss: 0.4804 - val_accuracy: 0.8142\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4795 - accuracy: 0.8095 - val_loss: 0.4764 - val_accuracy: 0.8143\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4793 - accuracy: 0.8110 - val_loss: 0.4784 - val_accuracy: 0.8133\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4823 - accuracy: 0.8081 - val_loss: 0.4770 - val_accuracy: 0.8145\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4842 - accuracy: 0.8063 - val_loss: 0.4748 - val_accuracy: 0.8140\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4805 - accuracy: 0.8091 - val_loss: 0.4779 - val_accuracy: 0.8144\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4806 - accuracy: 0.8087 - val_loss: 0.4831 - val_accuracy: 0.8144\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4811 - accuracy: 0.8085 - val_loss: 0.4748 - val_accuracy: 0.8145\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4850 - accuracy: 0.8066 - val_loss: 0.4737 - val_accuracy: 0.8145\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4813 - accuracy: 0.8079 - val_loss: 0.4766 - val_accuracy: 0.8145\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4810 - accuracy: 0.8087 - val_loss: 0.4772 - val_accuracy: 0.8144\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4836 - accuracy: 0.8072 - val_loss: 0.4741 - val_accuracy: 0.8143\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4788 - accuracy: 0.8101 - val_loss: 0.4741 - val_accuracy: 0.8144\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4815 - accuracy: 0.8081 - val_loss: 0.4813 - val_accuracy: 0.8137\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4812 - accuracy: 0.8084 - val_loss: 0.4748 - val_accuracy: 0.8144\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4842 - accuracy: 0.8100\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4813 - accuracy: 0.8091\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4803 - accuracy: 0.8097\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4805 - accuracy: 0.8094\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4803 - accuracy: 0.8095\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4802 - accuracy: 0.8090\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4798 - accuracy: 0.8095\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4794 - accuracy: 0.8097\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4800 - accuracy: 0.8097\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4793 - accuracy: 0.8098\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4797 - accuracy: 0.8097\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.8144\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6219 - accuracy: 0.7828 - val_loss: 0.5050 - val_accuracy: 0.8133\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4948 - accuracy: 0.8029 - val_loss: 0.5353 - val_accuracy: 0.7579\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4849 - accuracy: 0.8079 - val_loss: 0.5380 - val_accuracy: 0.7552\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4860 - accuracy: 0.8072 - val_loss: 0.5092 - val_accuracy: 0.8116\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4831 - accuracy: 0.8092 - val_loss: 0.4865 - val_accuracy: 0.8138\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4838 - accuracy: 0.8084 - val_loss: 0.4906 - val_accuracy: 0.8137\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4848 - accuracy: 0.8063 - val_loss: 0.4969 - val_accuracy: 0.8142\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4788 - accuracy: 0.8110 - val_loss: 0.4886 - val_accuracy: 0.8143\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4836 - accuracy: 0.8075 - val_loss: 0.4732 - val_accuracy: 0.8143\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4817 - accuracy: 0.8087 - val_loss: 0.4809 - val_accuracy: 0.8139\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4777 - accuracy: 0.8108 - val_loss: 0.4819 - val_accuracy: 0.8128\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4778 - accuracy: 0.8104 - val_loss: 0.4843 - val_accuracy: 0.8145\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4834 - accuracy: 0.8063 - val_loss: 0.4766 - val_accuracy: 0.8137\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4794 - accuracy: 0.8100 - val_loss: 0.4799 - val_accuracy: 0.8146\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4812 - accuracy: 0.8092 - val_loss: 0.4778 - val_accuracy: 0.8145\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4808 - accuracy: 0.8084 - val_loss: 0.4774 - val_accuracy: 0.8143\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4801 - accuracy: 0.8089 - val_loss: 0.4802 - val_accuracy: 0.8145\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4841 - accuracy: 0.8061 - val_loss: 0.4753 - val_accuracy: 0.8144\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4822 - accuracy: 0.8074 - val_loss: 0.4829 - val_accuracy: 0.8131\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4811 - accuracy: 0.8085 - val_loss: 0.4777 - val_accuracy: 0.8142\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4794 - accuracy: 0.8101 - val_loss: 0.4757 - val_accuracy: 0.8138\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4797 - accuracy: 0.8098 - val_loss: 0.4736 - val_accuracy: 0.8140\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4806 - accuracy: 0.8085 - val_loss: 0.4877 - val_accuracy: 0.8146\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4789 - accuracy: 0.8108 - val_loss: 0.4753 - val_accuracy: 0.8146\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4800 - accuracy: 0.8092 - val_loss: 0.4766 - val_accuracy: 0.8144\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4801 - accuracy: 0.8103 - val_loss: 0.4770 - val_accuracy: 0.8141\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4760 - accuracy: 0.8119 - val_loss: 0.4772 - val_accuracy: 0.8138\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4801 - accuracy: 0.8084 - val_loss: 0.4785 - val_accuracy: 0.8143\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4784 - accuracy: 0.8103 - val_loss: 0.4738 - val_accuracy: 0.8145\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4858 - accuracy: 0.8096\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4778 - accuracy: 0.8116\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4765 - accuracy: 0.8116\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4774 - accuracy: 0.8117\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4768 - accuracy: 0.8116\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4764 - accuracy: 0.8119\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4764 - accuracy: 0.8115\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4762 - accuracy: 0.8120\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4757 - accuracy: 0.8121\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4759 - accuracy: 0.8121\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4759 - accuracy: 0.8119\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.4889 - accuracy: 0.8057\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5985 - accuracy: 0.7214 - val_loss: 0.4842 - val_accuracy: 0.8124\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4915 - accuracy: 0.8070 - val_loss: 0.4782 - val_accuracy: 0.8129\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4863 - accuracy: 0.8069 - val_loss: 0.4801 - val_accuracy: 0.8132\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4834 - accuracy: 0.8073 - val_loss: 0.4851 - val_accuracy: 0.8133\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4796 - accuracy: 0.8111 - val_loss: 0.4829 - val_accuracy: 0.8140\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4830 - accuracy: 0.8080 - val_loss: 0.4762 - val_accuracy: 0.8137\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4792 - accuracy: 0.8107 - val_loss: 0.4839 - val_accuracy: 0.8117\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4838 - accuracy: 0.8069 - val_loss: 0.4746 - val_accuracy: 0.8135\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4843 - accuracy: 0.8068 - val_loss: 0.4738 - val_accuracy: 0.8137\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4806 - accuracy: 0.8090 - val_loss: 0.4760 - val_accuracy: 0.8144\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4771 - accuracy: 0.8117 - val_loss: 0.4804 - val_accuracy: 0.8143\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4849 - accuracy: 0.8058 - val_loss: 0.4775 - val_accuracy: 0.8123\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4816 - accuracy: 0.8081 - val_loss: 0.4789 - val_accuracy: 0.8142\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4778 - accuracy: 0.8101 - val_loss: 0.4781 - val_accuracy: 0.8141\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4784 - accuracy: 0.8116 - val_loss: 0.4748 - val_accuracy: 0.8144\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4787 - accuracy: 0.8097 - val_loss: 0.4759 - val_accuracy: 0.8144\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4755 - accuracy: 0.8116 - val_loss: 0.4742 - val_accuracy: 0.8145\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4804 - accuracy: 0.8096 - val_loss: 0.4742 - val_accuracy: 0.8145\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4815 - accuracy: 0.8084 - val_loss: 0.4736 - val_accuracy: 0.8142\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4807 - accuracy: 0.8088 - val_loss: 0.4736 - val_accuracy: 0.8143\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4795 - accuracy: 0.8094 - val_loss: 0.4759 - val_accuracy: 0.8139\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4810 - accuracy: 0.8084 - val_loss: 0.4751 - val_accuracy: 0.8143\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4778 - accuracy: 0.8106 - val_loss: 0.4737 - val_accuracy: 0.8143\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4792 - accuracy: 0.8096 - val_loss: 0.4738 - val_accuracy: 0.8143\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4810 - accuracy: 0.8084 - val_loss: 0.4726 - val_accuracy: 0.8144\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4782 - accuracy: 0.8102 - val_loss: 0.4743 - val_accuracy: 0.8146\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4809 - accuracy: 0.8081 - val_loss: 0.4735 - val_accuracy: 0.8142\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4789 - accuracy: 0.8093 - val_loss: 0.4740 - val_accuracy: 0.8145\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4837 - accuracy: 0.8065 - val_loss: 0.4766 - val_accuracy: 0.8140\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4791 - accuracy: 0.8087 - val_loss: 0.4751 - val_accuracy: 0.8136\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4766 - accuracy: 0.8111 - val_loss: 0.4725 - val_accuracy: 0.8146\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4800 - accuracy: 0.8089 - val_loss: 0.4743 - val_accuracy: 0.8143\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4830 - accuracy: 0.8074 - val_loss: 0.4724 - val_accuracy: 0.8141\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4769 - accuracy: 0.8105 - val_loss: 0.4720 - val_accuracy: 0.8146\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4787 - accuracy: 0.8101 - val_loss: 0.4735 - val_accuracy: 0.8143\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4804 - accuracy: 0.8071 - val_loss: 0.4748 - val_accuracy: 0.8144\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4767 - accuracy: 0.8111 - val_loss: 0.4746 - val_accuracy: 0.8146\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4760 - accuracy: 0.8110 - val_loss: 0.4728 - val_accuracy: 0.8143\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4756 - accuracy: 0.8118 - val_loss: 0.4728 - val_accuracy: 0.8147\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4807 - accuracy: 0.8075 - val_loss: 0.4743 - val_accuracy: 0.8145\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4750 - accuracy: 0.8124 - val_loss: 0.4724 - val_accuracy: 0.8139\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4790 - accuracy: 0.8094 - val_loss: 0.4728 - val_accuracy: 0.8146\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4806 - accuracy: 0.8080 - val_loss: 0.4722 - val_accuracy: 0.8145\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4778 - accuracy: 0.8097 - val_loss: 0.4722 - val_accuracy: 0.8145\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4755 - accuracy: 0.8122 - val_loss: 0.4721 - val_accuracy: 0.8150\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4765 - accuracy: 0.8109 - val_loss: 0.4728 - val_accuracy: 0.8140\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4760 - accuracy: 0.8109 - val_loss: 0.4748 - val_accuracy: 0.8146\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4815 - accuracy: 0.8074 - val_loss: 0.4719 - val_accuracy: 0.8147\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4814 - accuracy: 0.8083 - val_loss: 0.4724 - val_accuracy: 0.8145\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4832 - accuracy: 0.8067 - val_loss: 0.4731 - val_accuracy: 0.8141\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4788 - accuracy: 0.8101\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4778 - accuracy: 0.8108\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4773 - accuracy: 0.8110\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4774 - accuracy: 0.8110\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4772 - accuracy: 0.8104\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4771 - accuracy: 0.8108\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4773 - accuracy: 0.8105\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4770 - accuracy: 0.8108\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4774 - accuracy: 0.8108\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4771 - accuracy: 0.8108\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4772 - accuracy: 0.8107\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.4783 - accuracy: 0.8106\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6316 - accuracy: 0.7855 - val_loss: 0.5328 - val_accuracy: 0.8110\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4968 - accuracy: 0.8068 - val_loss: 0.5085 - val_accuracy: 0.8086\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4945 - accuracy: 0.8024 - val_loss: 0.5148 - val_accuracy: 0.7775\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4845 - accuracy: 0.8083 - val_loss: 0.5120 - val_accuracy: 0.8105\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4841 - accuracy: 0.8084 - val_loss: 0.4865 - val_accuracy: 0.8134\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4792 - accuracy: 0.8088 - val_loss: 0.4841 - val_accuracy: 0.8130\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4867 - accuracy: 0.8066 - val_loss: 0.4823 - val_accuracy: 0.8134\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4812 - accuracy: 0.8093 - val_loss: 0.4804 - val_accuracy: 0.8138\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4807 - accuracy: 0.8099 - val_loss: 0.4821 - val_accuracy: 0.8119\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4874 - accuracy: 0.8048 - val_loss: 0.4758 - val_accuracy: 0.8138\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4791 - accuracy: 0.8094 - val_loss: 0.4766 - val_accuracy: 0.8137\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4843 - accuracy: 0.8068 - val_loss: 0.4787 - val_accuracy: 0.8137\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4844 - accuracy: 0.8068 - val_loss: 0.4764 - val_accuracy: 0.8144\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4775 - accuracy: 0.8111 - val_loss: 0.4840 - val_accuracy: 0.8139\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4750 - accuracy: 0.8133 - val_loss: 0.4764 - val_accuracy: 0.8143\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4751 - accuracy: 0.8124 - val_loss: 0.4760 - val_accuracy: 0.8138\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4810 - accuracy: 0.8095 - val_loss: 0.4763 - val_accuracy: 0.8142\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4777 - accuracy: 0.8100 - val_loss: 0.4762 - val_accuracy: 0.8144\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4813 - accuracy: 0.8087 - val_loss: 0.4842 - val_accuracy: 0.8145\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4814 - accuracy: 0.8087 - val_loss: 0.4784 - val_accuracy: 0.8147\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4750 - accuracy: 0.8129 - val_loss: 0.4741 - val_accuracy: 0.8137\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4736 - accuracy: 0.8138 - val_loss: 0.4752 - val_accuracy: 0.8141\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4750 - accuracy: 0.8126 - val_loss: 0.4786 - val_accuracy: 0.8149\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4771 - accuracy: 0.8108 - val_loss: 0.4768 - val_accuracy: 0.8137\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4790 - accuracy: 0.8101 - val_loss: 0.4772 - val_accuracy: 0.8139\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4787 - accuracy: 0.8094 - val_loss: 0.4818 - val_accuracy: 0.8144\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4792 - accuracy: 0.8091 - val_loss: 0.4773 - val_accuracy: 0.8145\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4796 - accuracy: 0.8090 - val_loss: 0.4742 - val_accuracy: 0.8148\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4766 - accuracy: 0.8114 - val_loss: 0.4764 - val_accuracy: 0.8146\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4798 - accuracy: 0.8088 - val_loss: 0.4749 - val_accuracy: 0.8145\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4824 - accuracy: 0.8074 - val_loss: 0.4744 - val_accuracy: 0.8145\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4815 - accuracy: 0.8074 - val_loss: 0.4750 - val_accuracy: 0.8130\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4799 - accuracy: 0.8092 - val_loss: 0.4813 - val_accuracy: 0.8140\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4751 - accuracy: 0.8121 - val_loss: 0.4759 - val_accuracy: 0.8140\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4772 - accuracy: 0.8108 - val_loss: 0.4753 - val_accuracy: 0.8143\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4788 - accuracy: 0.8093 - val_loss: 0.4753 - val_accuracy: 0.8145\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4756 - accuracy: 0.8120 - val_loss: 0.4783 - val_accuracy: 0.8136\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4800 - accuracy: 0.8095 - val_loss: 0.4739 - val_accuracy: 0.8140\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4846 - accuracy: 0.8100\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4803 - accuracy: 0.8097\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4796 - accuracy: 0.8099\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4792 - accuracy: 0.8100\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4796 - accuracy: 0.8096\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4795 - accuracy: 0.8100\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4793 - accuracy: 0.8101\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4790 - accuracy: 0.8099\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4790 - accuracy: 0.8099\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4791 - accuracy: 0.8103\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4794 - accuracy: 0.8100\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.4750 - accuracy: 0.8141\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MunXmQAioQPM",
        "outputId": "07994d4d-3f36-4c71-eff1-ab66c90d4376"
      },
      "source": [
        "print(round(cv_results_4.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak0UQucQoQPM",
        "outputId": "e5f4af5f-1b52-4995-9bf2-8822ee640d81"
      },
      "source": [
        "print(round(cv_results_4.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0032\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrKzBGgoqUJO",
        "outputId": "702a708d-60e9-4f5e-e1b1-7f5d76317a51"
      },
      "source": [
        "cv_results_4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.81229556, 0.81443882, 0.80574203, 0.81064922, 0.81408989])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a37kd9roQPM",
        "outputId": "7dd3aebf-3b62-4c94-ae8c-7d5daa012e69"
      },
      "source": [
        "cv4_preds = cross_val_predict(mod4, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6165 - accuracy: 0.7695 - val_loss: 0.4912 - val_accuracy: 0.8125\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4901 - accuracy: 0.8075 - val_loss: 0.4988 - val_accuracy: 0.8122\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4903 - accuracy: 0.8046 - val_loss: 0.4907 - val_accuracy: 0.8130\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4805 - accuracy: 0.8101 - val_loss: 0.4792 - val_accuracy: 0.8130\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4812 - accuracy: 0.8093 - val_loss: 0.4763 - val_accuracy: 0.8131\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4845 - accuracy: 0.8072 - val_loss: 0.4768 - val_accuracy: 0.8137\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4872 - accuracy: 0.8045 - val_loss: 0.4806 - val_accuracy: 0.8136\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4842 - accuracy: 0.8068 - val_loss: 0.4821 - val_accuracy: 0.8132\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4832 - accuracy: 0.8071 - val_loss: 0.4798 - val_accuracy: 0.8134\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4784 - accuracy: 0.8104 - val_loss: 0.4733 - val_accuracy: 0.8142\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4805 - accuracy: 0.8087 - val_loss: 0.4767 - val_accuracy: 0.8134\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4839 - accuracy: 0.8069 - val_loss: 0.4751 - val_accuracy: 0.8137\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4802 - accuracy: 0.8097 - val_loss: 0.4791 - val_accuracy: 0.8141\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4831 - accuracy: 0.8078 - val_loss: 0.4767 - val_accuracy: 0.8137\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4790 - accuracy: 0.8101 - val_loss: 0.4723 - val_accuracy: 0.8144\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4816 - accuracy: 0.8075 - val_loss: 0.4748 - val_accuracy: 0.8145\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4858 - accuracy: 0.8053 - val_loss: 0.4726 - val_accuracy: 0.8145\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4777 - accuracy: 0.8108 - val_loss: 0.4819 - val_accuracy: 0.8136\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4767 - accuracy: 0.8107 - val_loss: 0.4741 - val_accuracy: 0.8145\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4815 - accuracy: 0.8080 - val_loss: 0.4742 - val_accuracy: 0.8143\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4824 - accuracy: 0.8077 - val_loss: 0.4741 - val_accuracy: 0.8144\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4763 - accuracy: 0.8119 - val_loss: 0.4756 - val_accuracy: 0.8143\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4842 - accuracy: 0.8059 - val_loss: 0.4742 - val_accuracy: 0.8135\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4778 - accuracy: 0.8100 - val_loss: 0.4767 - val_accuracy: 0.8140\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4798 - accuracy: 0.8088 - val_loss: 0.4733 - val_accuracy: 0.8135\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4801 - accuracy: 0.8092 - val_loss: 0.4727 - val_accuracy: 0.8142\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4794 - accuracy: 0.8097 - val_loss: 0.4744 - val_accuracy: 0.8146\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4783 - accuracy: 0.8098 - val_loss: 0.4744 - val_accuracy: 0.8145\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4774 - accuracy: 0.8107 - val_loss: 0.4738 - val_accuracy: 0.8144\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4754 - accuracy: 0.8121 - val_loss: 0.4741 - val_accuracy: 0.8145\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4798 - accuracy: 0.8084 - val_loss: 0.4736 - val_accuracy: 0.8145\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4744 - accuracy: 0.8125 - val_loss: 0.4736 - val_accuracy: 0.8146\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4812 - accuracy: 0.8081 - val_loss: 0.4722 - val_accuracy: 0.8146\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4783 - accuracy: 0.8093 - val_loss: 0.4731 - val_accuracy: 0.8144\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4758 - accuracy: 0.8122 - val_loss: 0.4729 - val_accuracy: 0.8145\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4808 - accuracy: 0.8081 - val_loss: 0.4726 - val_accuracy: 0.8145\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4767 - accuracy: 0.8113 - val_loss: 0.4738 - val_accuracy: 0.8144\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4776 - accuracy: 0.8101 - val_loss: 0.4739 - val_accuracy: 0.8145\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4781 - accuracy: 0.8101 - val_loss: 0.4738 - val_accuracy: 0.8146\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4782 - accuracy: 0.8089 - val_loss: 0.4779 - val_accuracy: 0.8123\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4784 - accuracy: 0.8091 - val_loss: 0.4734 - val_accuracy: 0.8143\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4753 - accuracy: 0.8122 - val_loss: 0.4765 - val_accuracy: 0.8139\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4815 - accuracy: 0.8075 - val_loss: 0.4747 - val_accuracy: 0.8143\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4804 - accuracy: 0.8077 - val_loss: 0.4721 - val_accuracy: 0.8143\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4744 - accuracy: 0.8120 - val_loss: 0.4738 - val_accuracy: 0.8139\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4762 - accuracy: 0.8107 - val_loss: 0.4748 - val_accuracy: 0.8141\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4813 - accuracy: 0.8080 - val_loss: 0.4722 - val_accuracy: 0.8146\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4772 - accuracy: 0.8106 - val_loss: 0.4758 - val_accuracy: 0.8118\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4775 - accuracy: 0.8101\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4794 - accuracy: 0.8097\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4795 - accuracy: 0.8095\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4788 - accuracy: 0.8098\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4795 - accuracy: 0.8096\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4792 - accuracy: 0.8095\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4788 - accuracy: 0.8095\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4787 - accuracy: 0.8098\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4788 - accuracy: 0.8100\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4794 - accuracy: 0.8093\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4785 - accuracy: 0.8097\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6424 - accuracy: 0.7853 - val_loss: 0.5554 - val_accuracy: 0.8130\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5294 - accuracy: 0.8012 - val_loss: 0.5321 - val_accuracy: 0.7674\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4939 - accuracy: 0.8031 - val_loss: 0.5434 - val_accuracy: 0.7437\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4894 - accuracy: 0.8061 - val_loss: 0.5076 - val_accuracy: 0.8088\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4849 - accuracy: 0.8064 - val_loss: 0.4894 - val_accuracy: 0.8140\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4844 - accuracy: 0.8078 - val_loss: 0.4899 - val_accuracy: 0.8130\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4824 - accuracy: 0.8095 - val_loss: 0.4835 - val_accuracy: 0.8132\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4805 - accuracy: 0.8093 - val_loss: 0.4772 - val_accuracy: 0.8123\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4811 - accuracy: 0.8095 - val_loss: 0.4757 - val_accuracy: 0.8135\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4850 - accuracy: 0.8062 - val_loss: 0.4738 - val_accuracy: 0.8143\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4831 - accuracy: 0.8070 - val_loss: 0.4774 - val_accuracy: 0.8135\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4823 - accuracy: 0.8072 - val_loss: 0.4805 - val_accuracy: 0.8134\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4831 - accuracy: 0.8080 - val_loss: 0.4772 - val_accuracy: 0.8140\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4817 - accuracy: 0.8092 - val_loss: 0.4761 - val_accuracy: 0.8144\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4845 - accuracy: 0.8071 - val_loss: 0.4783 - val_accuracy: 0.8147\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4815 - accuracy: 0.8085 - val_loss: 0.4780 - val_accuracy: 0.8138\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4815 - accuracy: 0.8079 - val_loss: 0.4735 - val_accuracy: 0.8144\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4850 - accuracy: 0.8054 - val_loss: 0.4752 - val_accuracy: 0.8144\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4794 - accuracy: 0.8100 - val_loss: 0.4766 - val_accuracy: 0.8146\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4803 - accuracy: 0.8084 - val_loss: 0.4749 - val_accuracy: 0.8144\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4785 - accuracy: 0.8100 - val_loss: 0.4759 - val_accuracy: 0.8140\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4804 - accuracy: 0.8089 - val_loss: 0.4749 - val_accuracy: 0.8142\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4799 - accuracy: 0.8081 - val_loss: 0.4732 - val_accuracy: 0.8145\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4855 - accuracy: 0.8055 - val_loss: 0.4743 - val_accuracy: 0.8136\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4748 - accuracy: 0.8130 - val_loss: 0.4781 - val_accuracy: 0.8145\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4793 - accuracy: 0.8099 - val_loss: 0.4743 - val_accuracy: 0.8142\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4752 - accuracy: 0.8120 - val_loss: 0.4787 - val_accuracy: 0.8138\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4868 - accuracy: 0.8050 - val_loss: 0.4777 - val_accuracy: 0.8145\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4784 - accuracy: 0.8104 - val_loss: 0.4837 - val_accuracy: 0.8112\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4780 - accuracy: 0.8109 - val_loss: 0.4797 - val_accuracy: 0.8149\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4795 - accuracy: 0.8083 - val_loss: 0.4780 - val_accuracy: 0.8134\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4791 - accuracy: 0.8099 - val_loss: 0.4754 - val_accuracy: 0.8147\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4781 - accuracy: 0.8099 - val_loss: 0.4790 - val_accuracy: 0.8138\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4799 - accuracy: 0.8095 - val_loss: 0.4729 - val_accuracy: 0.8146\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4809 - accuracy: 0.8089 - val_loss: 0.4732 - val_accuracy: 0.8144\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4744 - accuracy: 0.8122 - val_loss: 0.4735 - val_accuracy: 0.8150\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4747 - accuracy: 0.8121 - val_loss: 0.4749 - val_accuracy: 0.8147\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4807 - accuracy: 0.8085 - val_loss: 0.4742 - val_accuracy: 0.8144\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4738 - accuracy: 0.8139 - val_loss: 0.4734 - val_accuracy: 0.8140\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4839 - accuracy: 0.8059 - val_loss: 0.4733 - val_accuracy: 0.8137\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4824 - accuracy: 0.8070 - val_loss: 0.4769 - val_accuracy: 0.8134\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4785 - accuracy: 0.8104 - val_loss: 0.4747 - val_accuracy: 0.8149\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4792 - accuracy: 0.8103 - val_loss: 0.4762 - val_accuracy: 0.8136\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4781 - accuracy: 0.8093 - val_loss: 0.4794 - val_accuracy: 0.8142\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4787 - accuracy: 0.8098 - val_loss: 0.4747 - val_accuracy: 0.8147\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4753 - accuracy: 0.8113 - val_loss: 0.4772 - val_accuracy: 0.8144\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4776 - accuracy: 0.8104 - val_loss: 0.4712 - val_accuracy: 0.8149\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4818 - accuracy: 0.8078 - val_loss: 0.4773 - val_accuracy: 0.8125\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4765 - accuracy: 0.8111 - val_loss: 0.4748 - val_accuracy: 0.8141\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4772 - accuracy: 0.8109 - val_loss: 0.4777 - val_accuracy: 0.8145\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4835 - accuracy: 0.8101\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4762 - accuracy: 0.8117\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4765 - accuracy: 0.8116\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4760 - accuracy: 0.8118\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4761 - accuracy: 0.8119\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4766 - accuracy: 0.8115\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4767 - accuracy: 0.8112\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4766 - accuracy: 0.8114\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4761 - accuracy: 0.8114\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4763 - accuracy: 0.8118\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4763 - accuracy: 0.8119\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.5903 - accuracy: 0.7756 - val_loss: 0.4973 - val_accuracy: 0.8134\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4934 - accuracy: 0.8057 - val_loss: 0.4942 - val_accuracy: 0.8130\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4861 - accuracy: 0.8081 - val_loss: 0.4956 - val_accuracy: 0.8129\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4857 - accuracy: 0.8084 - val_loss: 0.4854 - val_accuracy: 0.8135\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4831 - accuracy: 0.8087 - val_loss: 0.4784 - val_accuracy: 0.8136\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4812 - accuracy: 0.8102 - val_loss: 0.4773 - val_accuracy: 0.8138\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4821 - accuracy: 0.8089 - val_loss: 0.4781 - val_accuracy: 0.8145\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4824 - accuracy: 0.8082 - val_loss: 0.4784 - val_accuracy: 0.8143\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4851 - accuracy: 0.8057 - val_loss: 0.4738 - val_accuracy: 0.8147\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4837 - accuracy: 0.8071 - val_loss: 0.4792 - val_accuracy: 0.8142\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4787 - accuracy: 0.8101 - val_loss: 0.4741 - val_accuracy: 0.8135\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4784 - accuracy: 0.8106 - val_loss: 0.4766 - val_accuracy: 0.8148\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4790 - accuracy: 0.8094 - val_loss: 0.4784 - val_accuracy: 0.8136\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4823 - accuracy: 0.8079 - val_loss: 0.4782 - val_accuracy: 0.8142\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4816 - accuracy: 0.8072 - val_loss: 0.4740 - val_accuracy: 0.8144\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4762 - accuracy: 0.8116 - val_loss: 0.4784 - val_accuracy: 0.8145\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4807 - accuracy: 0.8080 - val_loss: 0.4722 - val_accuracy: 0.8145\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4787 - accuracy: 0.8096 - val_loss: 0.4768 - val_accuracy: 0.8145\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4767 - accuracy: 0.8112 - val_loss: 0.4773 - val_accuracy: 0.8135\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4813 - accuracy: 0.8082 - val_loss: 0.4720 - val_accuracy: 0.8143\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4778 - accuracy: 0.8108 - val_loss: 0.4743 - val_accuracy: 0.8145\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4783 - accuracy: 0.8093 - val_loss: 0.4789 - val_accuracy: 0.8143\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4743 - accuracy: 0.8139 - val_loss: 0.4736 - val_accuracy: 0.8144\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4802 - accuracy: 0.8081 - val_loss: 0.4728 - val_accuracy: 0.8142\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4798 - accuracy: 0.8095 - val_loss: 0.4741 - val_accuracy: 0.8141\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4772 - accuracy: 0.8109 - val_loss: 0.4739 - val_accuracy: 0.8144\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4791 - accuracy: 0.8096 - val_loss: 0.4721 - val_accuracy: 0.8143\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4823 - accuracy: 0.8097\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4778 - accuracy: 0.8111\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4782 - accuracy: 0.8107\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4783 - accuracy: 0.8109\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4780 - accuracy: 0.8106\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4778 - accuracy: 0.8109\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4782 - accuracy: 0.8108\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4773 - accuracy: 0.8109\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4772 - accuracy: 0.8111\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.4769 - accuracy: 0.8110\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.4771 - accuracy: 0.8115\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6346 - accuracy: 0.7837 - val_loss: 0.5182 - val_accuracy: 0.8131\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5009 - accuracy: 0.8028 - val_loss: 0.5105 - val_accuracy: 0.8120\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4861 - accuracy: 0.8075 - val_loss: 0.4819 - val_accuracy: 0.8129\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4883 - accuracy: 0.8058 - val_loss: 0.4905 - val_accuracy: 0.8127\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4865 - accuracy: 0.8048 - val_loss: 0.4974 - val_accuracy: 0.8135\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4847 - accuracy: 0.8069 - val_loss: 0.4795 - val_accuracy: 0.8141\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4834 - accuracy: 0.8064 - val_loss: 0.4909 - val_accuracy: 0.8143\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4853 - accuracy: 0.8058 - val_loss: 0.4901 - val_accuracy: 0.8137\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4835 - accuracy: 0.8076 - val_loss: 0.4739 - val_accuracy: 0.8137\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4771 - accuracy: 0.8126 - val_loss: 0.4804 - val_accuracy: 0.8145\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4762 - accuracy: 0.8130 - val_loss: 0.4754 - val_accuracy: 0.8135\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4782 - accuracy: 0.8106 - val_loss: 0.4771 - val_accuracy: 0.8136\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4803 - accuracy: 0.8097 - val_loss: 0.4742 - val_accuracy: 0.8142\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4751 - accuracy: 0.8122 - val_loss: 0.4752 - val_accuracy: 0.8144\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4760 - accuracy: 0.8122 - val_loss: 0.4799 - val_accuracy: 0.8145\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4813 - accuracy: 0.8072 - val_loss: 0.4734 - val_accuracy: 0.8150\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4829 - accuracy: 0.8072 - val_loss: 0.4741 - val_accuracy: 0.8144\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4789 - accuracy: 0.8104 - val_loss: 0.4784 - val_accuracy: 0.8141\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4839 - accuracy: 0.8060 - val_loss: 0.4777 - val_accuracy: 0.8140\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4836 - accuracy: 0.8066 - val_loss: 0.4726 - val_accuracy: 0.8149\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4783 - accuracy: 0.8112 - val_loss: 0.4728 - val_accuracy: 0.8145\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4799 - accuracy: 0.8086 - val_loss: 0.4740 - val_accuracy: 0.8143\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4763 - accuracy: 0.8120 - val_loss: 0.4782 - val_accuracy: 0.8147\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4771 - accuracy: 0.8119 - val_loss: 0.4752 - val_accuracy: 0.8144\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4824 - accuracy: 0.8077 - val_loss: 0.4748 - val_accuracy: 0.8148\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4720 - accuracy: 0.8146 - val_loss: 0.4767 - val_accuracy: 0.8147\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4798 - accuracy: 0.8092 - val_loss: 0.4754 - val_accuracy: 0.8148\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4831 - accuracy: 0.8064 - val_loss: 0.4733 - val_accuracy: 0.8140\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4804 - accuracy: 0.8090 - val_loss: 0.4738 - val_accuracy: 0.8140\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4775 - accuracy: 0.8100 - val_loss: 0.4741 - val_accuracy: 0.8144\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4771 - accuracy: 0.8111 - val_loss: 0.4761 - val_accuracy: 0.8138\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4788 - accuracy: 0.8099\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4801 - accuracy: 0.8094\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4806 - accuracy: 0.8089\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4802 - accuracy: 0.8093\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4794 - accuracy: 0.8091\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4799 - accuracy: 0.8095\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4793 - accuracy: 0.8095\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4797 - accuracy: 0.8094\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4798 - accuracy: 0.8098\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4793 - accuracy: 0.8098\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4794 - accuracy: 0.8099\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6161 - accuracy: 0.7822 - val_loss: 0.5024 - val_accuracy: 0.8133\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4951 - accuracy: 0.8029 - val_loss: 0.4853 - val_accuracy: 0.8125\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4894 - accuracy: 0.8056 - val_loss: 0.5060 - val_accuracy: 0.8120\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4816 - accuracy: 0.8108 - val_loss: 0.4882 - val_accuracy: 0.8127\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4843 - accuracy: 0.8070 - val_loss: 0.4980 - val_accuracy: 0.8133\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4877 - accuracy: 0.8050 - val_loss: 0.4814 - val_accuracy: 0.8144\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4806 - accuracy: 0.8096 - val_loss: 0.4798 - val_accuracy: 0.8138\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4813 - accuracy: 0.8087 - val_loss: 0.4897 - val_accuracy: 0.8143\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4759 - accuracy: 0.8123 - val_loss: 0.4818 - val_accuracy: 0.8140\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4850 - accuracy: 0.8055 - val_loss: 0.4832 - val_accuracy: 0.8135\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4804 - accuracy: 0.8090 - val_loss: 0.4805 - val_accuracy: 0.8140\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4830 - accuracy: 0.8078 - val_loss: 0.4800 - val_accuracy: 0.8135\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4817 - accuracy: 0.8087 - val_loss: 0.4794 - val_accuracy: 0.8137\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4813 - accuracy: 0.8077 - val_loss: 0.4766 - val_accuracy: 0.8151\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4828 - accuracy: 0.8079 - val_loss: 0.4777 - val_accuracy: 0.8146\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4818 - accuracy: 0.8076 - val_loss: 0.4787 - val_accuracy: 0.8146\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4749 - accuracy: 0.8133 - val_loss: 0.4763 - val_accuracy: 0.8146\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4781 - accuracy: 0.8103 - val_loss: 0.4746 - val_accuracy: 0.8145\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4799 - accuracy: 0.8096 - val_loss: 0.4736 - val_accuracy: 0.8146\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4789 - accuracy: 0.8097 - val_loss: 0.4728 - val_accuracy: 0.8146\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4784 - accuracy: 0.8108 - val_loss: 0.4743 - val_accuracy: 0.8143\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4774 - accuracy: 0.8105 - val_loss: 0.4742 - val_accuracy: 0.8146\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4801 - accuracy: 0.8097 - val_loss: 0.4759 - val_accuracy: 0.8141\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4771 - accuracy: 0.8116 - val_loss: 0.4744 - val_accuracy: 0.8148\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4741 - accuracy: 0.8127 - val_loss: 0.4767 - val_accuracy: 0.8142\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4808 - accuracy: 0.8081 - val_loss: 0.4732 - val_accuracy: 0.8141\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4797 - accuracy: 0.8096 - val_loss: 0.4744 - val_accuracy: 0.8141\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4786 - accuracy: 0.8097 - val_loss: 0.4730 - val_accuracy: 0.8146\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4778 - accuracy: 0.8105 - val_loss: 0.4756 - val_accuracy: 0.8141\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4830 - accuracy: 0.8099\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.4786 - accuracy: 0.8103\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4791 - accuracy: 0.8105\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4783 - accuracy: 0.8108\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4781 - accuracy: 0.8107\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4781 - accuracy: 0.8105\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4780 - accuracy: 0.8108\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.4779 - accuracy: 0.8106\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4773 - accuracy: 0.8108\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4779 - accuracy: 0.8105\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.4775 - accuracy: 0.8108\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDkJ-v5CJqp6",
        "outputId": "39237778-0a9d-4db8-c5d5-ce8c0eed1e77"
      },
      "source": [
        "cm4 = confusion_matrix(y, cv4_preds)\n",
        "print(cm4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[54478  3522]\n",
            " [13204 17443]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSlEQdinoQPM"
      },
      "source": [
        "'''\n",
        "dropout_rate = [0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.4]\n",
        "param_grid = dict(dropout_rate=dropout_rate)\n",
        "grid4 = GridSearchCV(estimator=mod4, param_grid=param_grid, cv=10)\n",
        "grid_result4 = grid4.fit(X, y)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result4.best_score_, grid_result4.best_params_))\n",
        "for params, mean_score, scores in grid_result4.grid_scores_:\n",
        "    print(\"%f (%f) with: %r\" % (scores.mean(), scores.std(), params))'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zpUqUyS6EGW"
      },
      "source": [
        "# neural network on dataset attributes based on URL parameters (table 5)\n",
        "\n",
        "### (20 features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "YYsHtOZn6EGa",
        "outputId": "5797cdf0-d48c-47cc-b714-e357b2bcee90"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table1 = ['qty_dot_params', 'qty_hyphen_params', 'qty_underline_params', 'qty_slash_params', 'qty_questionmark_params', 'qty_equal_params', 'qty_at_params', 'qty_and_params',\n",
        "                   'qty_exclamation_params', 'qty_space_params', 'qty_tilde_params', 'qty_comma_params', 'qty_plus_params', 'qty_asterisk_params', 'qty_hashtag_params', 'qty_dollar_params',\n",
        "                   'qty_percent_params', 'params_length', 'tld_present_params', 'qty_params'] \n",
        "\n",
        "X = full_df[features_table1]\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_params</th>\n",
              "      <th>qty_hyphen_params</th>\n",
              "      <th>qty_underline_params</th>\n",
              "      <th>qty_slash_params</th>\n",
              "      <th>qty_questionmark_params</th>\n",
              "      <th>qty_equal_params</th>\n",
              "      <th>qty_at_params</th>\n",
              "      <th>qty_and_params</th>\n",
              "      <th>qty_exclamation_params</th>\n",
              "      <th>qty_space_params</th>\n",
              "      <th>qty_tilde_params</th>\n",
              "      <th>qty_comma_params</th>\n",
              "      <th>qty_plus_params</th>\n",
              "      <th>qty_asterisk_params</th>\n",
              "      <th>qty_hashtag_params</th>\n",
              "      <th>qty_dollar_params</th>\n",
              "      <th>qty_percent_params</th>\n",
              "      <th>params_length</th>\n",
              "      <th>tld_present_params</th>\n",
              "      <th>qty_params</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_params  qty_hyphen_params  ...  tld_present_params  qty_params\n",
              "5676                0                  0  ...                   0           0\n",
              "39002               0                  0  ...                   0           0\n",
              "1732                0                  0  ...                   0           0\n",
              "39668               0                  0  ...                   0           0\n",
              "82035               0                  0  ...                   0           0\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY0cEI756EGc",
        "outputId": "f6e97ddc-582f-4ce8-9da6-5f3b273a15e8"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88647, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZgL8sqN6EGc",
        "outputId": "946a17f4-ae3c-4d04-937b-fa61fe13ea49"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "table5_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[20]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "table5_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "table5_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                1344      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 14,241\n",
            "Trainable params: 14,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hBJSOJv6EGd",
        "outputId": "c0ce4de9-1051-4e65-b4f7-559724c4df71"
      },
      "source": [
        "history = table5_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6327 - binary_accuracy: 0.7040 - auc: 0.5997 - val_loss: 0.5874 - val_binary_accuracy: 0.7177 - val_auc: 0.6026\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5889 - binary_accuracy: 0.7154 - auc: 0.6075 - val_loss: 0.5767 - val_binary_accuracy: 0.7223 - val_auc: 0.6029\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5799 - binary_accuracy: 0.7191 - auc: 0.5991 - val_loss: 0.5778 - val_binary_accuracy: 0.7234 - val_auc: 0.6030\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5777 - binary_accuracy: 0.7193 - auc: 0.6019 - val_loss: 0.5770 - val_binary_accuracy: 0.7251 - val_auc: 0.6031\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5760 - binary_accuracy: 0.7217 - auc: 0.6049 - val_loss: 0.5767 - val_binary_accuracy: 0.7250 - val_auc: 0.6030\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - binary_accuracy: 0.7235 - auc: 0.6049 - val_loss: 0.5831 - val_binary_accuracy: 0.7194 - val_auc: 0.6015\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - binary_accuracy: 0.7218 - auc: 0.6018 - val_loss: 0.5720 - val_binary_accuracy: 0.7260 - val_auc: 0.6039\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5746 - binary_accuracy: 0.7235 - auc: 0.6067 - val_loss: 0.5745 - val_binary_accuracy: 0.7256 - val_auc: 0.6038\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - binary_accuracy: 0.7223 - auc: 0.6093 - val_loss: 0.5716 - val_binary_accuracy: 0.7262 - val_auc: 0.6039\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - binary_accuracy: 0.7200 - auc: 0.6017 - val_loss: 0.5751 - val_binary_accuracy: 0.7263 - val_auc: 0.6039\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5727 - binary_accuracy: 0.7230 - auc: 0.6017 - val_loss: 0.5722 - val_binary_accuracy: 0.7259 - val_auc: 0.6041\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5757 - binary_accuracy: 0.7206 - auc: 0.6010 - val_loss: 0.5705 - val_binary_accuracy: 0.7262 - val_auc: 0.6039\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5748 - binary_accuracy: 0.7206 - auc: 0.6055 - val_loss: 0.5740 - val_binary_accuracy: 0.7263 - val_auc: 0.6038\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5722 - binary_accuracy: 0.7230 - auc: 0.6067 - val_loss: 0.5737 - val_binary_accuracy: 0.7263 - val_auc: 0.6039\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5751 - binary_accuracy: 0.7209 - auc: 0.6001 - val_loss: 0.5721 - val_binary_accuracy: 0.7261 - val_auc: 0.6041\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5708 - binary_accuracy: 0.7251 - auc: 0.6035 - val_loss: 0.5733 - val_binary_accuracy: 0.7259 - val_auc: 0.6041\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - binary_accuracy: 0.7185 - auc: 0.6037 - val_loss: 0.5803 - val_binary_accuracy: 0.7263 - val_auc: 0.6039\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5754 - binary_accuracy: 0.7209 - auc: 0.6010 - val_loss: 0.5708 - val_binary_accuracy: 0.7262 - val_auc: 0.6040\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5725 - binary_accuracy: 0.7229 - auc: 0.6019 - val_loss: 0.5707 - val_binary_accuracy: 0.7263 - val_auc: 0.6040\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5701 - binary_accuracy: 0.7247 - auc: 0.6033 - val_loss: 0.5722 - val_binary_accuracy: 0.7259 - val_auc: 0.6040\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5722 - binary_accuracy: 0.7237 - auc: 0.6015 - val_loss: 0.5703 - val_binary_accuracy: 0.7262 - val_auc: 0.6040\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5751 - binary_accuracy: 0.7197 - auc: 0.5996 - val_loss: 0.5719 - val_binary_accuracy: 0.7263 - val_auc: 0.6041\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5721 - binary_accuracy: 0.7223 - auc: 0.6074 - val_loss: 0.5706 - val_binary_accuracy: 0.7261 - val_auc: 0.6041\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5764 - binary_accuracy: 0.7178 - auc: 0.6008 - val_loss: 0.5706 - val_binary_accuracy: 0.7262 - val_auc: 0.6040\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5736 - binary_accuracy: 0.7206 - auc: 0.6076 - val_loss: 0.5688 - val_binary_accuracy: 0.7260 - val_auc: 0.6040\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5720 - binary_accuracy: 0.7232 - auc: 0.6020 - val_loss: 0.5695 - val_binary_accuracy: 0.7263 - val_auc: 0.6039\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5741 - binary_accuracy: 0.7204 - auc: 0.6063 - val_loss: 0.5706 - val_binary_accuracy: 0.7262 - val_auc: 0.6040\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5716 - binary_accuracy: 0.7219 - auc: 0.6082 - val_loss: 0.5706 - val_binary_accuracy: 0.7261 - val_auc: 0.6041\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5719 - binary_accuracy: 0.7223 - auc: 0.6042 - val_loss: 0.5708 - val_binary_accuracy: 0.7262 - val_auc: 0.6041\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5713 - binary_accuracy: 0.7226 - auc: 0.6041 - val_loss: 0.5730 - val_binary_accuracy: 0.7260 - val_auc: 0.6039\n",
            "Epoch 31/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5737 - binary_accuracy: 0.7209 - auc: 0.6032 - val_loss: 0.5743 - val_binary_accuracy: 0.7263 - val_auc: 0.6039\n",
            "Epoch 32/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5717 - binary_accuracy: 0.7222 - auc: 0.6028 - val_loss: 0.5705 - val_binary_accuracy: 0.7262 - val_auc: 0.6040\n",
            "Epoch 33/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5717 - binary_accuracy: 0.7225 - auc: 0.6021 - val_loss: 0.5701 - val_binary_accuracy: 0.7259 - val_auc: 0.6038\n",
            "Epoch 34/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5710 - binary_accuracy: 0.7237 - auc: 0.6055 - val_loss: 0.5701 - val_binary_accuracy: 0.7261 - val_auc: 0.6041\n",
            "Epoch 35/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5696 - binary_accuracy: 0.7265 - auc: 0.5940 - val_loss: 0.5697 - val_binary_accuracy: 0.7262 - val_auc: 0.6040\n",
            "Epoch 36/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5703 - binary_accuracy: 0.7232 - auc: 0.6050 - val_loss: 0.5710 - val_binary_accuracy: 0.7260 - val_auc: 0.6039\n",
            "Epoch 37/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5774 - binary_accuracy: 0.7169 - auc: 0.5999 - val_loss: 0.5690 - val_binary_accuracy: 0.7260 - val_auc: 0.6039\n",
            "Epoch 38/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5744 - binary_accuracy: 0.7193 - auc: 0.6027 - val_loss: 0.5721 - val_binary_accuracy: 0.7257 - val_auc: 0.6035\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "yxRi-IIE6EGd",
        "outputId": "110640e2-8aed-48f1-dcd9-e7a3e66232a9"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "history_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "      <td>38.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.575082</td>\n",
              "      <td>0.721122</td>\n",
              "      <td>0.604629</td>\n",
              "      <td>0.573038</td>\n",
              "      <td>0.725475</td>\n",
              "      <td>0.603759</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.006245</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.002145</td>\n",
              "      <td>0.003972</td>\n",
              "      <td>0.001842</td>\n",
              "      <td>0.000534</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.571837</td>\n",
              "      <td>0.713101</td>\n",
              "      <td>0.598813</td>\n",
              "      <td>0.568824</td>\n",
              "      <td>0.717688</td>\n",
              "      <td>0.601519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.572771</td>\n",
              "      <td>0.721363</td>\n",
              "      <td>0.603742</td>\n",
              "      <td>0.570575</td>\n",
              "      <td>0.725910</td>\n",
              "      <td>0.603837</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.573426</td>\n",
              "      <td>0.721642</td>\n",
              "      <td>0.604710</td>\n",
              "      <td>0.571990</td>\n",
              "      <td>0.726085</td>\n",
              "      <td>0.603943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.574563</td>\n",
              "      <td>0.721803</td>\n",
              "      <td>0.606124</td>\n",
              "      <td>0.574230</td>\n",
              "      <td>0.726211</td>\n",
              "      <td>0.604031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.609140</td>\n",
              "      <td>0.722018</td>\n",
              "      <td>0.608472</td>\n",
              "      <td>0.587438</td>\n",
              "      <td>0.726311</td>\n",
              "      <td>0.604143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  38.000000        38.000000  ...            38.000000  38.000000\n",
              "mean    0.575082         0.721122  ...             0.725475   0.603759\n",
              "std     0.006245         0.001653  ...             0.001842   0.000534\n",
              "min     0.571837         0.713101  ...             0.717688   0.601519\n",
              "25%     0.572771         0.721363  ...             0.725910   0.603837\n",
              "50%     0.573426         0.721642  ...             0.726085   0.603943\n",
              "75%     0.574563         0.721803  ...             0.726211   0.604031\n",
              "max     0.609140         0.722018  ...             0.726311   0.604143\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df-oGTRP6EGd",
        "outputId": "624b0745-be5c-4b15-cc3c-b9d32fcdd44c"
      },
      "source": [
        "train_acc = table5_nn.evaluate(train_X, train_y)\n",
        "test_acc = table5_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 1ms/step - loss: 0.5760 - binary_accuracy: 0.7233 - auc: 0.6050\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5749 - binary_accuracy: 0.7248 - auc: 0.6062\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwJewG2m6EGe",
        "outputId": "642a949b-9e96-46be-e93e-76c642f028d1"
      },
      "source": [
        "dict(zip(table5_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.6062233448028564,\n",
              " 'binary_accuracy': 0.7247992157936096,\n",
              " 'loss': 0.5748729109764099}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ghI14dWw6EGe",
        "outputId": "c3a22945-3700-416c-8d14-60144102b303"
      },
      "source": [
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.5688237547874451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "JR6JiX286EGe",
        "outputId": "f0839f90-2357-48e3-c8e0-2deba79d30e3"
      },
      "source": [
        "history_df.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.6041432023048401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ToYIHbz36EGf",
        "outputId": "30510597-3c46-4ca4-8956-691ce6ba0c12"
      },
      "source": [
        "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.7263110280036926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8s1Pc2A6EGf",
        "outputId": "6eb95753-559b-4eae-f95b-a1a1d48649c7"
      },
      "source": [
        "preds1 = table5_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.30349493],\n",
              "       [0.30349493],\n",
              "       [0.30349493],\n",
              "       ...,\n",
              "       [0.30349493],\n",
              "       [0.30349493],\n",
              "       [0.30349493]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmaWOyYo6EGf",
        "outputId": "a7d73313-891e-4478-93d6-e872412a5e18"
      },
      "source": [
        "len(preds1[preds1 < 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20332"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxiEu28n6EGf",
        "outputId": "974d0b43-8671-42bd-f8b9-d4c726d35b7c"
      },
      "source": [
        "len(preds1[preds1 >= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1830"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij8N26hO6EGg",
        "outputId": "72bbd962-5053-4d8c-e9ec-bd5f935b68ff"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld35dvSMoQPN"
      },
      "source": [
        "# NN on URL parameter attributes (table 5) [model as function]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "HN9A8PrxoQPN",
        "outputId": "f659e407-393f-4097-d1bd-c66bf1c7e872"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table5 = ['qty_dot_params', 'qty_hyphen_params', 'qty_underline_params', 'qty_slash_params', 'qty_questionmark_params', 'qty_equal_params', 'qty_at_params', 'qty_and_params',\n",
        "                   'qty_exclamation_params', 'qty_space_params', 'qty_tilde_params', 'qty_comma_params', 'qty_plus_params', 'qty_asterisk_params', 'qty_hashtag_params', 'qty_dollar_params',\n",
        "                   'qty_percent_params', 'params_length', 'tld_present_params', 'qty_params'] \n",
        "\n",
        "X = full_df[features_table5]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_params</th>\n",
              "      <th>qty_hyphen_params</th>\n",
              "      <th>qty_underline_params</th>\n",
              "      <th>qty_slash_params</th>\n",
              "      <th>qty_questionmark_params</th>\n",
              "      <th>qty_equal_params</th>\n",
              "      <th>qty_at_params</th>\n",
              "      <th>qty_and_params</th>\n",
              "      <th>qty_exclamation_params</th>\n",
              "      <th>qty_space_params</th>\n",
              "      <th>qty_tilde_params</th>\n",
              "      <th>qty_comma_params</th>\n",
              "      <th>qty_plus_params</th>\n",
              "      <th>qty_asterisk_params</th>\n",
              "      <th>qty_hashtag_params</th>\n",
              "      <th>qty_dollar_params</th>\n",
              "      <th>qty_percent_params</th>\n",
              "      <th>params_length</th>\n",
              "      <th>tld_present_params</th>\n",
              "      <th>qty_params</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_params  qty_hyphen_params  ...  tld_present_params  qty_params\n",
              "5676              0.0                0.0  ...                 0.0         0.0\n",
              "39002             0.0                0.0  ...                 0.0         0.0\n",
              "1732              0.0                0.0  ...                 0.0         0.0\n",
              "39668             0.0                0.0  ...                 0.0         0.0\n",
              "82035             0.0                0.0  ...                 0.0         0.0\n",
              "\n",
              "[5 rows x 20 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWwaYgzioQPN"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_5():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[20]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMPbuZoXoQPN"
      },
      "source": [
        "mod5 = KerasClassifier(build_fn=phish_nn_5,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4AxgXRo8oQPN",
        "outputId": "48589abc-e439-40d6-976c-1b53067f1ed9"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_5=cross_val_score(mod5,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6305 - accuracy: 0.7027 - val_loss: 0.5767 - val_accuracy: 0.7232\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5802 - accuracy: 0.7207 - val_loss: 0.5755 - val_accuracy: 0.7260\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5747 - accuracy: 0.7230 - val_loss: 0.5795 - val_accuracy: 0.7234\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5773 - accuracy: 0.7199 - val_loss: 0.5733 - val_accuracy: 0.7255\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5765 - accuracy: 0.7197 - val_loss: 0.5732 - val_accuracy: 0.7234\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.7239 - val_loss: 0.5747 - val_accuracy: 0.7259\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5755 - accuracy: 0.7210 - val_loss: 0.5779 - val_accuracy: 0.7261\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5735 - accuracy: 0.7225 - val_loss: 0.5786 - val_accuracy: 0.7256\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5781 - accuracy: 0.7190 - val_loss: 0.5696 - val_accuracy: 0.7259\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5777 - accuracy: 0.7212 - val_loss: 0.5708 - val_accuracy: 0.7240\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5765 - accuracy: 0.7188 - val_loss: 0.5755 - val_accuracy: 0.7261\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5738 - accuracy: 0.7216 - val_loss: 0.5696 - val_accuracy: 0.7259\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5737 - accuracy: 0.7224 - val_loss: 0.5737 - val_accuracy: 0.7260\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.7198 - val_loss: 0.5787 - val_accuracy: 0.7260\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5738 - accuracy: 0.7226 - val_loss: 0.5727 - val_accuracy: 0.7261\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5731 - accuracy: 0.7217 - val_loss: 0.5748 - val_accuracy: 0.7257\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5738 - accuracy: 0.7215 - val_loss: 0.5728 - val_accuracy: 0.7260\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.7219 - val_loss: 0.5733 - val_accuracy: 0.7259\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.7208 - val_loss: 0.5700 - val_accuracy: 0.7259\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5774 - accuracy: 0.7200 - val_loss: 0.5724 - val_accuracy: 0.7259\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5730 - accuracy: 0.7220 - val_loss: 0.5728 - val_accuracy: 0.7259\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5752 - accuracy: 0.7212 - val_loss: 0.5717 - val_accuracy: 0.7260\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5785 - accuracy: 0.7247\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5725 - accuracy: 0.7236\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5718 - accuracy: 0.7241\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5713 - accuracy: 0.7239\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5714 - accuracy: 0.7240\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5717 - accuracy: 0.7242\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5714 - accuracy: 0.7241\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5712 - accuracy: 0.7241\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5704 - accuracy: 0.7240\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5708 - accuracy: 0.7242\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5713 - accuracy: 0.7240\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5763 - accuracy: 0.7207\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6547 - accuracy: 0.6990 - val_loss: 0.5842 - val_accuracy: 0.7243\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5784 - accuracy: 0.7251 - val_loss: 0.5734 - val_accuracy: 0.7263\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5779 - accuracy: 0.7204 - val_loss: 0.5763 - val_accuracy: 0.7263\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5773 - accuracy: 0.7213 - val_loss: 0.5711 - val_accuracy: 0.7259\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7223 - val_loss: 0.5745 - val_accuracy: 0.7262\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5779 - accuracy: 0.7200 - val_loss: 0.5748 - val_accuracy: 0.7260\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5771 - accuracy: 0.7186 - val_loss: 0.5714 - val_accuracy: 0.7260\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5746 - accuracy: 0.7216 - val_loss: 0.5702 - val_accuracy: 0.7259\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5728 - accuracy: 0.7230 - val_loss: 0.5715 - val_accuracy: 0.7259\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5711 - accuracy: 0.7246 - val_loss: 0.5808 - val_accuracy: 0.7259\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5773 - accuracy: 0.7193 - val_loss: 0.5720 - val_accuracy: 0.7260\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5738 - accuracy: 0.7214 - val_loss: 0.5743 - val_accuracy: 0.7259\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5701 - accuracy: 0.7246 - val_loss: 0.5721 - val_accuracy: 0.7259\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5762 - accuracy: 0.7187 - val_loss: 0.5730 - val_accuracy: 0.7260\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5692 - accuracy: 0.7267 - val_loss: 0.5710 - val_accuracy: 0.7260\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5696 - accuracy: 0.7254 - val_loss: 0.5767 - val_accuracy: 0.7259\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5750 - accuracy: 0.7197 - val_loss: 0.5784 - val_accuracy: 0.7259\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5736 - accuracy: 0.7250\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5762 - accuracy: 0.7223\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5749 - accuracy: 0.7227\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5743 - accuracy: 0.7224\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5735 - accuracy: 0.7227\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5736 - accuracy: 0.7225\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5730 - accuracy: 0.7227\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5732 - accuracy: 0.7228\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5734 - accuracy: 0.7225\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5730 - accuracy: 0.7226\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5728 - accuracy: 0.7229\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5701 - accuracy: 0.7263\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6379 - accuracy: 0.7001 - val_loss: 0.5774 - val_accuracy: 0.7243\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5887 - accuracy: 0.7143 - val_loss: 0.5743 - val_accuracy: 0.7246\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5780 - accuracy: 0.7222 - val_loss: 0.5719 - val_accuracy: 0.7262\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5775 - accuracy: 0.7199 - val_loss: 0.5766 - val_accuracy: 0.7259\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5775 - accuracy: 0.7197 - val_loss: 0.5725 - val_accuracy: 0.7262\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5769 - accuracy: 0.7201 - val_loss: 0.5756 - val_accuracy: 0.7259\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5755 - accuracy: 0.7202 - val_loss: 0.5730 - val_accuracy: 0.7259\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5722 - accuracy: 0.7237 - val_loss: 0.5727 - val_accuracy: 0.7258\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.7240 - val_loss: 0.5785 - val_accuracy: 0.7259\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.7187 - val_loss: 0.5768 - val_accuracy: 0.7261\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5705 - accuracy: 0.7248 - val_loss: 0.5742 - val_accuracy: 0.7261\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5756 - accuracy: 0.7200 - val_loss: 0.5820 - val_accuracy: 0.7263\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5783 - accuracy: 0.7168 - val_loss: 0.5700 - val_accuracy: 0.7261\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5728 - accuracy: 0.7211 - val_loss: 0.5784 - val_accuracy: 0.7258\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5737 - accuracy: 0.7218 - val_loss: 0.5712 - val_accuracy: 0.7261\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5749 - accuracy: 0.7203 - val_loss: 0.5772 - val_accuracy: 0.7253\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5710 - accuracy: 0.7242 - val_loss: 0.5789 - val_accuracy: 0.7260\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5717 - accuracy: 0.7244 - val_loss: 0.5741 - val_accuracy: 0.7261\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7205 - val_loss: 0.5708 - val_accuracy: 0.7259\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5745 - accuracy: 0.7211 - val_loss: 0.5714 - val_accuracy: 0.7259\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.7194 - val_loss: 0.5692 - val_accuracy: 0.7260\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5730 - accuracy: 0.7223 - val_loss: 0.5701 - val_accuracy: 0.7254\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5729 - accuracy: 0.7208 - val_loss: 0.5717 - val_accuracy: 0.7261\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5740 - accuracy: 0.7197 - val_loss: 0.5726 - val_accuracy: 0.7261\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5747 - accuracy: 0.7205 - val_loss: 0.5703 - val_accuracy: 0.7261\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5736 - accuracy: 0.7218 - val_loss: 0.5702 - val_accuracy: 0.7263\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5732 - accuracy: 0.7211 - val_loss: 0.5688 - val_accuracy: 0.7261\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5830 - accuracy: 0.7249\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5731 - accuracy: 0.7227\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5721 - accuracy: 0.7228\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7229\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5721 - accuracy: 0.7229\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7230\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7228\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5725 - accuracy: 0.7231\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5722 - accuracy: 0.7231\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5719 - accuracy: 0.7228\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5717 - accuracy: 0.7232\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.5694 - accuracy: 0.7256\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6326 - accuracy: 0.6975 - val_loss: 0.5774 - val_accuracy: 0.7243\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5859 - accuracy: 0.7175 - val_loss: 0.5722 - val_accuracy: 0.7259\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5808 - accuracy: 0.7166 - val_loss: 0.5712 - val_accuracy: 0.7259\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5770 - accuracy: 0.7195 - val_loss: 0.5738 - val_accuracy: 0.7260\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5746 - accuracy: 0.7223 - val_loss: 0.5778 - val_accuracy: 0.7240\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5747 - accuracy: 0.7226 - val_loss: 0.5780 - val_accuracy: 0.7255\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5764 - accuracy: 0.7199 - val_loss: 0.5714 - val_accuracy: 0.7259\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5764 - accuracy: 0.7205 - val_loss: 0.5706 - val_accuracy: 0.7259\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5752 - accuracy: 0.7206 - val_loss: 0.5768 - val_accuracy: 0.7259\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5741 - accuracy: 0.7227 - val_loss: 0.5719 - val_accuracy: 0.7259\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5765 - accuracy: 0.7186 - val_loss: 0.5704 - val_accuracy: 0.7259\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5738 - accuracy: 0.7231 - val_loss: 0.5719 - val_accuracy: 0.7259\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5749 - accuracy: 0.7205 - val_loss: 0.5845 - val_accuracy: 0.7260\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5742 - accuracy: 0.7205 - val_loss: 0.5784 - val_accuracy: 0.7262\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5767 - accuracy: 0.7195 - val_loss: 0.5811 - val_accuracy: 0.7262\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5772 - accuracy: 0.7191 - val_loss: 0.5729 - val_accuracy: 0.7261\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5724 - accuracy: 0.7228 - val_loss: 0.5742 - val_accuracy: 0.7259\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5741 - accuracy: 0.7212 - val_loss: 0.5710 - val_accuracy: 0.7261\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5754 - accuracy: 0.7194 - val_loss: 0.5734 - val_accuracy: 0.7259\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5758 - accuracy: 0.7191 - val_loss: 0.5755 - val_accuracy: 0.7260\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5697 - accuracy: 0.7259 - val_loss: 0.5749 - val_accuracy: 0.7260\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5737 - accuracy: 0.7224 - val_loss: 0.5704 - val_accuracy: 0.7259\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5763 - accuracy: 0.7186 - val_loss: 0.5701 - val_accuracy: 0.7259\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7218 - val_loss: 0.5711 - val_accuracy: 0.7259\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5770 - accuracy: 0.7180 - val_loss: 0.5777 - val_accuracy: 0.7259\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7206 - val_loss: 0.5707 - val_accuracy: 0.7261\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5761 - accuracy: 0.7193 - val_loss: 0.5721 - val_accuracy: 0.7260\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5723 - accuracy: 0.7220 - val_loss: 0.5734 - val_accuracy: 0.7261\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5740 - accuracy: 0.7212 - val_loss: 0.5766 - val_accuracy: 0.7261\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5790 - accuracy: 0.7249\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7234\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5720 - accuracy: 0.7237\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7235\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5721 - accuracy: 0.7237\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5713 - accuracy: 0.7236\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5722 - accuracy: 0.7234\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5716 - accuracy: 0.7236\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5718 - accuracy: 0.7235\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5721 - accuracy: 0.7237\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5711 - accuracy: 0.7236\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5736 - accuracy: 0.7231\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6400 - accuracy: 0.6985 - val_loss: 0.5868 - val_accuracy: 0.7235\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5843 - accuracy: 0.7191 - val_loss: 0.5720 - val_accuracy: 0.7247\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5793 - accuracy: 0.7209 - val_loss: 0.5768 - val_accuracy: 0.7266\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7234 - val_loss: 0.5730 - val_accuracy: 0.7259\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5766 - accuracy: 0.7207 - val_loss: 0.5720 - val_accuracy: 0.7260\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5732 - accuracy: 0.7233 - val_loss: 0.5711 - val_accuracy: 0.7259\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5758 - accuracy: 0.7205 - val_loss: 0.5769 - val_accuracy: 0.7259\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5732 - accuracy: 0.7209 - val_loss: 0.5928 - val_accuracy: 0.7262\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5742 - accuracy: 0.7231 - val_loss: 0.5736 - val_accuracy: 0.7262\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5738 - accuracy: 0.7220 - val_loss: 0.5732 - val_accuracy: 0.7260\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5726 - accuracy: 0.7233 - val_loss: 0.5715 - val_accuracy: 0.7260\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7229 - val_loss: 0.5755 - val_accuracy: 0.7264\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5748 - accuracy: 0.7204 - val_loss: 0.5750 - val_accuracy: 0.7263\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5750 - accuracy: 0.7196 - val_loss: 0.5724 - val_accuracy: 0.7259\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5752 - accuracy: 0.7209 - val_loss: 0.5703 - val_accuracy: 0.7262\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5754 - accuracy: 0.7187 - val_loss: 0.5721 - val_accuracy: 0.7262\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5725 - accuracy: 0.7219 - val_loss: 0.5729 - val_accuracy: 0.7260\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5742 - accuracy: 0.7202 - val_loss: 0.5712 - val_accuracy: 0.7261\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5775 - accuracy: 0.7245\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5738 - accuracy: 0.7236\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5725 - accuracy: 0.7236\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5729 - accuracy: 0.7236\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5720 - accuracy: 0.7240\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5718 - accuracy: 0.7235\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5714 - accuracy: 0.7236\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5712 - accuracy: 0.7237\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5721 - accuracy: 0.7238\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5713 - accuracy: 0.7236\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5715 - accuracy: 0.7237\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5786 - accuracy: 0.7224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDCZSyy-oQPN",
        "outputId": "daf168f0-0d8f-46ba-9ddb-8faef8707ecd"
      },
      "source": [
        "print(round(cv_results_5.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEVgIZdNoQPN",
        "outputId": "532afad5-460f-4bdb-98d9-a5fab60fc12b"
      },
      "source": [
        "print(round(cv_results_5.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0021\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_3dV91GqP_j",
        "outputId": "ffff8a85-d92b-4cbf-bdde-12a43e476529"
      },
      "source": [
        "cv_results_5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.72069937, 0.72633952, 0.72559083, 0.72305262, 0.72237575])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 177
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riWJGbY6oQPN",
        "outputId": "8a00ca36-52b1-4640-8a44-dd71472fed1c"
      },
      "source": [
        "cv5_preds = cross_val_predict(mod5, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6342 - accuracy: 0.7043 - val_loss: 0.5783 - val_accuracy: 0.7243\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5821 - accuracy: 0.7204 - val_loss: 0.5724 - val_accuracy: 0.7259\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5793 - accuracy: 0.7196 - val_loss: 0.5727 - val_accuracy: 0.7259\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5763 - accuracy: 0.7224 - val_loss: 0.5735 - val_accuracy: 0.7261\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7226 - val_loss: 0.5835 - val_accuracy: 0.7237\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5793 - accuracy: 0.7176 - val_loss: 0.5752 - val_accuracy: 0.7246\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5751 - accuracy: 0.7205 - val_loss: 0.5769 - val_accuracy: 0.7258\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5773 - accuracy: 0.7196 - val_loss: 0.5781 - val_accuracy: 0.7239\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5758 - accuracy: 0.7214 - val_loss: 0.5786 - val_accuracy: 0.7259\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5762 - accuracy: 0.7205 - val_loss: 0.5784 - val_accuracy: 0.7258\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7205 - val_loss: 0.5788 - val_accuracy: 0.7262\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7209 - val_loss: 0.5713 - val_accuracy: 0.7262\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5770 - accuracy: 0.7186 - val_loss: 0.5752 - val_accuracy: 0.7262\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5732 - accuracy: 0.7219 - val_loss: 0.5717 - val_accuracy: 0.7260\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7211 - val_loss: 0.5741 - val_accuracy: 0.7258\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5721 - accuracy: 0.7229 - val_loss: 0.5747 - val_accuracy: 0.7260\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5726 - accuracy: 0.7227 - val_loss: 0.5717 - val_accuracy: 0.7261\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5690 - accuracy: 0.7249 - val_loss: 0.5745 - val_accuracy: 0.7262\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5729 - accuracy: 0.7233 - val_loss: 0.5710 - val_accuracy: 0.7262\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5708 - accuracy: 0.7251 - val_loss: 0.5703 - val_accuracy: 0.7263\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5791 - accuracy: 0.7177 - val_loss: 0.5698 - val_accuracy: 0.7261\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5700 - accuracy: 0.7254 - val_loss: 0.5701 - val_accuracy: 0.7260\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5763 - accuracy: 0.7186 - val_loss: 0.5760 - val_accuracy: 0.7261\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7214 - val_loss: 0.5757 - val_accuracy: 0.7258\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5716 - accuracy: 0.7235 - val_loss: 0.5734 - val_accuracy: 0.7262\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5726 - accuracy: 0.7224 - val_loss: 0.5739 - val_accuracy: 0.7260\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5717 - accuracy: 0.7232 - val_loss: 0.5719 - val_accuracy: 0.7261\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5709 - accuracy: 0.7238 - val_loss: 0.5744 - val_accuracy: 0.7261\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7223 - val_loss: 0.5712 - val_accuracy: 0.7259\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5734 - accuracy: 0.7221 - val_loss: 0.5721 - val_accuracy: 0.7261\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5709 - accuracy: 0.7243 - val_loss: 0.5732 - val_accuracy: 0.7261\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5754 - accuracy: 0.7182 - val_loss: 0.5695 - val_accuracy: 0.7261\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7216 - val_loss: 0.5705 - val_accuracy: 0.7261\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5720 - accuracy: 0.7229 - val_loss: 0.5747 - val_accuracy: 0.7262\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5732 - accuracy: 0.7217 - val_loss: 0.5690 - val_accuracy: 0.7261\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5703 - accuracy: 0.7250\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5731 - accuracy: 0.7221\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5735 - accuracy: 0.7220\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5735 - accuracy: 0.7222\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5736 - accuracy: 0.7220\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5732 - accuracy: 0.7221\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7224\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5728 - accuracy: 0.7223\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5730 - accuracy: 0.7223\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7223\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5728 - accuracy: 0.7222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6484 - accuracy: 0.6984 - val_loss: 0.5801 - val_accuracy: 0.7243\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5865 - accuracy: 0.7182 - val_loss: 0.5723 - val_accuracy: 0.7259\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5790 - accuracy: 0.7217 - val_loss: 0.5737 - val_accuracy: 0.7262\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5733 - accuracy: 0.7247 - val_loss: 0.5776 - val_accuracy: 0.7240\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5756 - accuracy: 0.7227 - val_loss: 0.5720 - val_accuracy: 0.7263\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5789 - accuracy: 0.7183 - val_loss: 0.5721 - val_accuracy: 0.7259\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5742 - accuracy: 0.7232 - val_loss: 0.5718 - val_accuracy: 0.7259\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5730 - accuracy: 0.7231 - val_loss: 0.5768 - val_accuracy: 0.7260\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5747 - accuracy: 0.7219 - val_loss: 0.5712 - val_accuracy: 0.7260\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5750 - accuracy: 0.7203 - val_loss: 0.5719 - val_accuracy: 0.7260\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5760 - accuracy: 0.7197 - val_loss: 0.5737 - val_accuracy: 0.7259\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7220 - val_loss: 0.5763 - val_accuracy: 0.7260\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5772 - accuracy: 0.7197 - val_loss: 0.5727 - val_accuracy: 0.7261\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5756 - accuracy: 0.7212 - val_loss: 0.5725 - val_accuracy: 0.7262\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5759 - accuracy: 0.7198 - val_loss: 0.5733 - val_accuracy: 0.7262\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5727 - accuracy: 0.7224 - val_loss: 0.5699 - val_accuracy: 0.7260\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5723 - accuracy: 0.7227 - val_loss: 0.5742 - val_accuracy: 0.7266\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5757 - accuracy: 0.7191 - val_loss: 0.5707 - val_accuracy: 0.7262\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5784 - accuracy: 0.7178 - val_loss: 0.5711 - val_accuracy: 0.7259\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5699 - accuracy: 0.7241 - val_loss: 0.5700 - val_accuracy: 0.7258\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7218 - val_loss: 0.5757 - val_accuracy: 0.7260\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5722 - accuracy: 0.7230 - val_loss: 0.5698 - val_accuracy: 0.7260\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5717 - accuracy: 0.7226 - val_loss: 0.5727 - val_accuracy: 0.7261\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5733 - accuracy: 0.7218 - val_loss: 0.5701 - val_accuracy: 0.7261\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5733 - accuracy: 0.7214 - val_loss: 0.5699 - val_accuracy: 0.7261\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5724 - accuracy: 0.7235 - val_loss: 0.5738 - val_accuracy: 0.7261\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.7222 - val_loss: 0.5726 - val_accuracy: 0.7260\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5758 - accuracy: 0.7185 - val_loss: 0.5711 - val_accuracy: 0.7262\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5714 - accuracy: 0.7238 - val_loss: 0.5715 - val_accuracy: 0.7259\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5706 - accuracy: 0.7245 - val_loss: 0.5724 - val_accuracy: 0.7259\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5720 - accuracy: 0.7229 - val_loss: 0.5695 - val_accuracy: 0.7258\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5708 - accuracy: 0.7244 - val_loss: 0.5694 - val_accuracy: 0.7260\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5745 - accuracy: 0.7253\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5709 - accuracy: 0.7247\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5708 - accuracy: 0.7246\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5709 - accuracy: 0.7245\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5704 - accuracy: 0.7245\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5707 - accuracy: 0.7247\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5706 - accuracy: 0.7246\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5703 - accuracy: 0.7248\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5700 - accuracy: 0.7246\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5703 - accuracy: 0.7246\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5702 - accuracy: 0.7249\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6394 - accuracy: 0.7068 - val_loss: 0.5767 - val_accuracy: 0.7243\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5776 - accuracy: 0.7231 - val_loss: 0.5745 - val_accuracy: 0.7248\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5793 - accuracy: 0.7195 - val_loss: 0.5726 - val_accuracy: 0.7265\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5753 - accuracy: 0.7231 - val_loss: 0.5719 - val_accuracy: 0.7259\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5755 - accuracy: 0.7222 - val_loss: 0.5784 - val_accuracy: 0.7262\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5788 - accuracy: 0.7194 - val_loss: 0.5708 - val_accuracy: 0.7259\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5752 - accuracy: 0.7211 - val_loss: 0.5789 - val_accuracy: 0.7257\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5742 - accuracy: 0.7217 - val_loss: 0.5706 - val_accuracy: 0.7260\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5721 - accuracy: 0.7245 - val_loss: 0.5780 - val_accuracy: 0.7269\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5729 - accuracy: 0.7219 - val_loss: 0.5751 - val_accuracy: 0.7261\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5692 - accuracy: 0.7258 - val_loss: 0.5736 - val_accuracy: 0.7267\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5687 - accuracy: 0.7275 - val_loss: 0.5724 - val_accuracy: 0.7263\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5737 - accuracy: 0.7228 - val_loss: 0.5697 - val_accuracy: 0.7259\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5740 - accuracy: 0.7223 - val_loss: 0.5700 - val_accuracy: 0.7259\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5741 - accuracy: 0.7218 - val_loss: 0.5706 - val_accuracy: 0.7259\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5748 - accuracy: 0.7192 - val_loss: 0.5734 - val_accuracy: 0.7260\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7216 - val_loss: 0.5738 - val_accuracy: 0.7259\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5719 - accuracy: 0.7240 - val_loss: 0.5718 - val_accuracy: 0.7260\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5755 - accuracy: 0.7189 - val_loss: 0.5761 - val_accuracy: 0.7260\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5721 - accuracy: 0.7241 - val_loss: 0.5849 - val_accuracy: 0.7263\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5767 - accuracy: 0.7193 - val_loss: 0.5696 - val_accuracy: 0.7259\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5710 - accuracy: 0.7238 - val_loss: 0.5705 - val_accuracy: 0.7261\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5758 - accuracy: 0.7189 - val_loss: 0.5735 - val_accuracy: 0.7261\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5737 - accuracy: 0.7221 - val_loss: 0.5730 - val_accuracy: 0.7246\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5787 - accuracy: 0.7251\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5725 - accuracy: 0.7232\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5726 - accuracy: 0.7233\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5718 - accuracy: 0.7234\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5719 - accuracy: 0.7235\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5721 - accuracy: 0.7236\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5715 - accuracy: 0.7236\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5715 - accuracy: 0.7234\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5717 - accuracy: 0.7234\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5719 - accuracy: 0.7234\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5715 - accuracy: 0.7235\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6307 - accuracy: 0.7103 - val_loss: 0.5788 - val_accuracy: 0.7243\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5857 - accuracy: 0.7169 - val_loss: 0.5748 - val_accuracy: 0.7246\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5779 - accuracy: 0.7217 - val_loss: 0.5714 - val_accuracy: 0.7259\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5775 - accuracy: 0.7201 - val_loss: 0.5721 - val_accuracy: 0.7252\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5733 - accuracy: 0.7235 - val_loss: 0.5741 - val_accuracy: 0.7263\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5766 - accuracy: 0.7190 - val_loss: 0.5715 - val_accuracy: 0.7261\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5748 - accuracy: 0.7224 - val_loss: 0.5813 - val_accuracy: 0.7234\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5729 - accuracy: 0.7238 - val_loss: 0.5794 - val_accuracy: 0.7259\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5763 - accuracy: 0.7210 - val_loss: 0.5779 - val_accuracy: 0.7258\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5734 - accuracy: 0.7215 - val_loss: 0.5748 - val_accuracy: 0.7259\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5747 - accuracy: 0.7212 - val_loss: 0.5801 - val_accuracy: 0.7237\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5737 - accuracy: 0.7211 - val_loss: 0.5708 - val_accuracy: 0.7259\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5711 - accuracy: 0.7258 - val_loss: 0.5738 - val_accuracy: 0.7259\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5714 - accuracy: 0.7241 - val_loss: 0.5758 - val_accuracy: 0.7260\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5722 - accuracy: 0.7218 - val_loss: 0.5726 - val_accuracy: 0.7262\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5737 - accuracy: 0.7211 - val_loss: 0.5732 - val_accuracy: 0.7259\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5707 - accuracy: 0.7245 - val_loss: 0.5735 - val_accuracy: 0.7259\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5739 - accuracy: 0.7202 - val_loss: 0.5741 - val_accuracy: 0.7259\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5755 - accuracy: 0.7192 - val_loss: 0.5710 - val_accuracy: 0.7259\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5743 - accuracy: 0.7197 - val_loss: 0.5704 - val_accuracy: 0.7260\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5745 - accuracy: 0.7245\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5743 - accuracy: 0.7219\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5738 - accuracy: 0.7222\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5736 - accuracy: 0.7223\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5729 - accuracy: 0.7222\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5732 - accuracy: 0.7223\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5725 - accuracy: 0.7225\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5727 - accuracy: 0.7224\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5724 - accuracy: 0.7226\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5726 - accuracy: 0.7226\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5728 - accuracy: 0.7226\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6440 - accuracy: 0.7015 - val_loss: 0.5799 - val_accuracy: 0.7242\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5808 - accuracy: 0.7207 - val_loss: 0.5719 - val_accuracy: 0.7259\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5741 - accuracy: 0.7244 - val_loss: 0.5733 - val_accuracy: 0.7257\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5741 - accuracy: 0.7230 - val_loss: 0.5726 - val_accuracy: 0.7266\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5724 - accuracy: 0.7264 - val_loss: 0.5714 - val_accuracy: 0.7258\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5717 - accuracy: 0.7249 - val_loss: 0.5739 - val_accuracy: 0.7260\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5735 - accuracy: 0.7225 - val_loss: 0.5703 - val_accuracy: 0.7259\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5750 - accuracy: 0.7210 - val_loss: 0.5715 - val_accuracy: 0.7259\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5754 - accuracy: 0.7187 - val_loss: 0.5746 - val_accuracy: 0.7259\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5737 - accuracy: 0.7218 - val_loss: 0.5713 - val_accuracy: 0.7259\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5742 - accuracy: 0.7213 - val_loss: 0.5715 - val_accuracy: 0.7259\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5731 - accuracy: 0.7223 - val_loss: 0.5723 - val_accuracy: 0.7259\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.7228 - val_loss: 0.5709 - val_accuracy: 0.7259\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5746 - accuracy: 0.7216 - val_loss: 0.5693 - val_accuracy: 0.7259\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5723 - accuracy: 0.7237 - val_loss: 0.5713 - val_accuracy: 0.7260\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5720 - accuracy: 0.7228 - val_loss: 0.5797 - val_accuracy: 0.7259\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5724 - accuracy: 0.7219 - val_loss: 0.5717 - val_accuracy: 0.7259\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.7202 - val_loss: 0.5758 - val_accuracy: 0.7259\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5725 - accuracy: 0.7229 - val_loss: 0.5735 - val_accuracy: 0.7259\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5730 - accuracy: 0.7250\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5734 - accuracy: 0.7238\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5727 - accuracy: 0.7240\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5723 - accuracy: 0.7240\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5723 - accuracy: 0.7242\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5720 - accuracy: 0.7240\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5716 - accuracy: 0.7242\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5720 - accuracy: 0.7238\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5714 - accuracy: 0.7240\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5716 - accuracy: 0.7243\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5715 - accuracy: 0.7240\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjJcBIp9JuVn",
        "outputId": "fbf707d5-c14a-49b0-999c-815305b7cb54"
      },
      "source": [
        "cm5 = confusion_matrix(y, cv5_preds)\n",
        "print(cm5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[57445   555]\n",
            " [23948  6699]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FIKr-RX6M8V"
      },
      "source": [
        "# neural network on dataset attributes based on URL resolving data and external metrics (table 6)\n",
        "\n",
        "### (14 features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "rJIJtdh36M8W",
        "outputId": "8e2d05d9-8b0a-4413-ab67-60265ba40fe7"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table1 = ['time_response', 'domain_spf', 'asn_ip', 'time_domain_activation', 'time_domain_expiration', 'qty_ip_resolved', 'qty_nameservers', 'qty_mx_servers', 'ttl_hostname', 'tls_ssl_certificate',\n",
        "                   'qty_redirects', 'url_google_index', 'domain_google_index', 'url_shortened'] \n",
        "\n",
        "X = full_df[features_table1]\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_response</th>\n",
              "      <th>domain_spf</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "      <th>url_google_index</th>\n",
              "      <th>domain_google_index</th>\n",
              "      <th>url_shortened</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>8.696835</td>\n",
              "      <td>0</td>\n",
              "      <td>262254</td>\n",
              "      <td>155</td>\n",
              "      <td>209</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>3011</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.244978</td>\n",
              "      <td>0</td>\n",
              "      <td>2818</td>\n",
              "      <td>0</td>\n",
              "      <td>2625</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>247</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.430590</td>\n",
              "      <td>0</td>\n",
              "      <td>15169</td>\n",
              "      <td>5858</td>\n",
              "      <td>350</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>14399</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.617731</td>\n",
              "      <td>0</td>\n",
              "      <td>14061</td>\n",
              "      <td>4285</td>\n",
              "      <td>97</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>7199</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.149499</td>\n",
              "      <td>0</td>\n",
              "      <td>29671</td>\n",
              "      <td>6669</td>\n",
              "      <td>269</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>21598</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       time_response  domain_spf  ...  domain_google_index  url_shortened\n",
              "5676        8.696835           0  ...                    0              0\n",
              "39002       0.244978           0  ...                    0              0\n",
              "1732        0.430590           0  ...                    0              0\n",
              "39668       0.617731           0  ...                    0              0\n",
              "82035       0.149499           0  ...                    0              0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdqTeCOB6M8X",
        "outputId": "2ff35241-63d1-4fb4-a25b-159ab60c9581"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(88647, 14)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cPGgs3S6M8X",
        "outputId": "7fc94a22-4c22-4a07-dc64-08d737ce3f75"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "table6_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[14]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "table6_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "table6_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                960       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 13,857\n",
            "Trainable params: 13,857\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6mvZLoj6M8X",
        "outputId": "3664c952-1a25-4ae0-ae11-4b33144bdf18"
      },
      "source": [
        "history = table6_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 32.2590 - binary_accuracy: 0.5457 - auc: 0.4992 - val_loss: 0.7034 - val_binary_accuracy: 0.6568 - val_auc: 0.5037\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 1.7595 - binary_accuracy: 0.5802 - auc: 0.4929 - val_loss: 0.6575 - val_binary_accuracy: 0.6576 - val_auc: 0.3860\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 1.0169 - binary_accuracy: 0.6103 - auc: 0.4973 - val_loss: 0.6427 - val_binary_accuracy: 0.6589 - val_auc: 0.4889\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.8251 - binary_accuracy: 0.6267 - auc: 0.5012 - val_loss: 0.6415 - val_binary_accuracy: 0.6592 - val_auc: 0.5237\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.7500 - binary_accuracy: 0.6300 - auc: 0.5028 - val_loss: 0.6414 - val_binary_accuracy: 0.6592 - val_auc: 0.5211\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.7063 - binary_accuracy: 0.6415 - auc: 0.5001 - val_loss: 0.6414 - val_binary_accuracy: 0.6592 - val_auc: 0.5202\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6924 - binary_accuracy: 0.6439 - auc: 0.5122 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5004\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6729 - binary_accuracy: 0.6476 - auc: 0.5019 - val_loss: 0.6420 - val_binary_accuracy: 0.6592 - val_auc: 0.4053\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6739 - binary_accuracy: 0.6443 - auc: 0.4961 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.4900\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6692 - binary_accuracy: 0.6449 - auc: 0.5044 - val_loss: 0.6417 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6635 - binary_accuracy: 0.6506 - auc: 0.5006 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6611 - binary_accuracy: 0.6455 - auc: 0.5001 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6579 - binary_accuracy: 0.6466 - auc: 0.5008 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6544 - binary_accuracy: 0.6490 - auc: 0.5009 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6512 - binary_accuracy: 0.6520 - auc: 0.5042 - val_loss: 0.6419 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6504 - binary_accuracy: 0.6532 - auc: 0.4932 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6510 - binary_accuracy: 0.6504 - auc: 0.4960 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6519 - binary_accuracy: 0.6509 - auc: 0.4982 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6492 - binary_accuracy: 0.6531 - auc: 0.4994 - val_loss: 0.6419 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.6492 - binary_accuracy: 0.6497 - auc: 0.4972 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6483 - binary_accuracy: 0.6515 - auc: 0.5032 - val_loss: 0.6416 - val_binary_accuracy: 0.6592 - val_auc: 0.5000\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6541 - binary_accuracy: 0.6533 - auc: 0.5003 - val_loss: 0.6419 - val_binary_accuracy: 0.6592 - val_auc: 0.5029\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6476 - binary_accuracy: 0.6527 - auc: 0.5018 - val_loss: 0.6280 - val_binary_accuracy: 0.6592 - val_auc: 0.6137\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6424 - binary_accuracy: 0.6519 - auc: 0.5320 - val_loss: 0.5900 - val_binary_accuracy: 0.6592 - val_auc: 0.7346\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6323 - binary_accuracy: 0.6508 - auc: 0.5728 - val_loss: 0.5967 - val_binary_accuracy: 0.6592 - val_auc: 0.6621\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6178 - binary_accuracy: 0.6525 - auc: 0.6126 - val_loss: 0.5818 - val_binary_accuracy: 0.6592 - val_auc: 0.7316\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6065 - binary_accuracy: 0.6502 - auc: 0.6452 - val_loss: 0.5895 - val_binary_accuracy: 0.6592 - val_auc: 0.7180\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.6017 - binary_accuracy: 0.6542 - auc: 0.6634 - val_loss: 0.6078 - val_binary_accuracy: 0.6592 - val_auc: 0.7143\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5968 - binary_accuracy: 0.6518 - auc: 0.6657 - val_loss: 0.5939 - val_binary_accuracy: 0.6592 - val_auc: 0.7139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "UHzvmS896M8Y",
        "outputId": "34ddd517-8241-4f6a-eb85-79e95d64190f"
      },
      "source": [
        "history_df = pd.DataFrame(history.history)\n",
        "\n",
        "history_df.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.090051</td>\n",
              "      <td>0.641658</td>\n",
              "      <td>0.526685</td>\n",
              "      <td>0.633891</td>\n",
              "      <td>0.659035</td>\n",
              "      <td>0.542427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>2.109026</td>\n",
              "      <td>0.020677</td>\n",
              "      <td>0.055095</td>\n",
              "      <td>0.024581</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>0.096086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.593613</td>\n",
              "      <td>0.557532</td>\n",
              "      <td>0.495854</td>\n",
              "      <td>0.581833</td>\n",
              "      <td>0.656823</td>\n",
              "      <td>0.386016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.648457</td>\n",
              "      <td>0.645330</td>\n",
              "      <td>0.499809</td>\n",
              "      <td>0.641391</td>\n",
              "      <td>0.659180</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.654094</td>\n",
              "      <td>0.649627</td>\n",
              "      <td>0.502311</td>\n",
              "      <td>0.641615</td>\n",
              "      <td>0.659180</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.673525</td>\n",
              "      <td>0.650938</td>\n",
              "      <td>0.506521</td>\n",
              "      <td>0.641672</td>\n",
              "      <td>0.659180</td>\n",
              "      <td>0.523662</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>12.020675</td>\n",
              "      <td>0.651647</td>\n",
              "      <td>0.670968</td>\n",
              "      <td>0.703430</td>\n",
              "      <td>0.659180</td>\n",
              "      <td>0.734637</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  29.000000        29.000000  ...            29.000000  29.000000\n",
              "mean    1.090051         0.641658  ...             0.659035   0.542427\n",
              "std     2.109026         0.020677  ...             0.000516   0.096086\n",
              "min     0.593613         0.557532  ...             0.656823   0.386016\n",
              "25%     0.648457         0.645330  ...             0.659180   0.500000\n",
              "50%     0.654094         0.649627  ...             0.659180   0.500000\n",
              "75%     0.673525         0.650938  ...             0.659180   0.523662\n",
              "max    12.020675         0.651647  ...             0.659180   0.734637\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wymbTKTP6M8Y",
        "outputId": "b5423f20-09a7-4f08-e41d-f54da555c4a8"
      },
      "source": [
        "train_acc = table6_nn.evaluate(train_X, train_y)\n",
        "test_acc = table6_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 1ms/step - loss: 0.6453 - binary_accuracy: 0.6540 - auc: 0.5197\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.6444 - binary_accuracy: 0.6551 - auc: 0.5216\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2KIvTWe6M8Z",
        "outputId": "7658f3f7-cc97-4f09-94c4-7cd94c405e83"
      },
      "source": [
        "dict(zip(table6_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.5216185450553894,\n",
              " 'binary_accuracy': 0.6551303863525391,\n",
              " 'loss': 0.6443995833396912}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "4aWoUa-g6M8Z",
        "outputId": "25fcef89-e404-4e7d-c3a8-35b4f8bc4ba6"
      },
      "source": [
        "history_df.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.5818330645561218\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "ebbI1nSF6M8Z",
        "outputId": "90a31cdc-8407-4723-8881-459e80c74da2"
      },
      "source": [
        "history_df.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.7346371412277222\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "ak7kgjhU6M8Z",
        "outputId": "f891be5c-88c9-4f9a-b198-a8dab2072b58"
      },
      "source": [
        "history_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.6591798067092896\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64eYC0BI6M8a",
        "outputId": "29f62ada-bba0-41f8-d9d8-112825ee7594"
      },
      "source": [
        "preds1 = table6_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.32830507],\n",
              "       [0.32830507],\n",
              "       [0.33570668],\n",
              "       ...,\n",
              "       [0.33615583],\n",
              "       [0.32830507],\n",
              "       [0.37132597]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 190
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHVdDXn96M8a",
        "outputId": "ee93f946-fad8-43aa-d0ae-26aeed9c8443"
      },
      "source": [
        "len(preds1[preds1 < 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 191
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVY7CVqH6M8a",
        "outputId": "117513d6-88af-405a-c6ae-969e7253615e"
      },
      "source": [
        "len(preds1[preds1 >= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAp_xQHm6M8a",
        "outputId": "e8b5b999-96e5-474b-e164-1bd0571a0f8e"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 193
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayXwQRKIpXyR",
        "outputId": "f93a84ea-5766-4e97-8a8c-1256957ea3d9"
      },
      "source": [
        "pred_classes = np.argmax(preds1, axis=1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(val_y, pred_classes)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14519     0]\n",
            " [ 7643     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_xzBJWRoQPO"
      },
      "source": [
        "# NN on URL resolving data and external metric attributes (table 6) [model as function]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "CTWsuItBoQPO",
        "outputId": "028409f1-30c1-487f-b4e4-75aecf07b203"
      },
      "source": [
        "y = full_df['phishing']\n",
        "\n",
        "features_table6 = ['time_response', 'domain_spf', 'asn_ip', 'time_domain_activation', 'time_domain_expiration', 'qty_ip_resolved', 'qty_nameservers', 'qty_mx_servers', 'ttl_hostname', 'tls_ssl_certificate',\n",
        "                   'qty_redirects', 'url_google_index', 'domain_google_index', 'url_shortened'] \n",
        "\n",
        "X = full_df[features_table6]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>time_response</th>\n",
              "      <th>domain_spf</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "      <th>url_google_index</th>\n",
              "      <th>domain_google_index</th>\n",
              "      <th>url_shortened</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999934</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.011480</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.730219</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.680207</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>0.001037</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.064004</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.698307</td>\n",
              "      <td>0.269674</td>\n",
              "      <td>0.016112</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.662860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.859060</td>\n",
              "      <td>0.261793</td>\n",
              "      <td>0.005926</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.439824</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.795441</td>\n",
              "      <td>0.178787</td>\n",
              "      <td>0.007212</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.579014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       time_response  domain_spf  ...  domain_google_index  url_shortened\n",
              "5676        0.000033         0.0  ...                  0.0            0.0\n",
              "39002       0.000063         0.0  ...                  0.0            0.0\n",
              "1732        0.000020         0.0  ...                  0.0            0.0\n",
              "39668       0.000038         0.0  ...                  0.0            0.0\n",
              "82035       0.000004         0.0  ...                  0.0            0.0\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 195
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0mwxnv7oQPO"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_6():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[14]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpJyIoPaoQPO"
      },
      "source": [
        "mod6 = KerasClassifier(build_fn=phish_nn_6,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G186mhyVoQPO",
        "outputId": "cbbda48c-c3ab-4f9d-e2b9-ed2e1513a5e4"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_6=cross_val_score(mod6,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6269 - accuracy: 0.6479 - val_loss: 0.5881 - val_accuracy: 0.7189\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5627 - accuracy: 0.7027 - val_loss: 0.5820 - val_accuracy: 0.7188\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5558 - accuracy: 0.7103 - val_loss: 0.5764 - val_accuracy: 0.7216\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5512 - accuracy: 0.7148 - val_loss: 0.5746 - val_accuracy: 0.7199\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5559 - accuracy: 0.7079 - val_loss: 0.5601 - val_accuracy: 0.7199\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5507 - accuracy: 0.7151 - val_loss: 0.5511 - val_accuracy: 0.7194\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5491 - accuracy: 0.7134 - val_loss: 0.5504 - val_accuracy: 0.7227\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5493 - accuracy: 0.7121 - val_loss: 0.5486 - val_accuracy: 0.7211\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5515 - accuracy: 0.7140 - val_loss: 0.5614 - val_accuracy: 0.7098\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5479 - accuracy: 0.7158 - val_loss: 0.5448 - val_accuracy: 0.7207\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5470 - accuracy: 0.7123 - val_loss: 0.5456 - val_accuracy: 0.7215\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5461 - accuracy: 0.7161 - val_loss: 0.5646 - val_accuracy: 0.7211\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5491 - accuracy: 0.7170 - val_loss: 0.5364 - val_accuracy: 0.7225\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5463 - accuracy: 0.7164 - val_loss: 0.5548 - val_accuracy: 0.7198\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5458 - accuracy: 0.7183 - val_loss: 0.5409 - val_accuracy: 0.7223\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5438 - accuracy: 0.7180 - val_loss: 0.5425 - val_accuracy: 0.7218\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5457 - accuracy: 0.7124 - val_loss: 0.5412 - val_accuracy: 0.7211\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5439 - accuracy: 0.7150 - val_loss: 0.5505 - val_accuracy: 0.7136\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5441 - accuracy: 0.7155 - val_loss: 0.5360 - val_accuracy: 0.7233\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5436 - accuracy: 0.7145 - val_loss: 0.5379 - val_accuracy: 0.7241\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5418 - accuracy: 0.7153 - val_loss: 0.5381 - val_accuracy: 0.7183\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5419 - accuracy: 0.7155 - val_loss: 0.5380 - val_accuracy: 0.7240\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5454 - accuracy: 0.7088 - val_loss: 0.5352 - val_accuracy: 0.7244\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5350 - accuracy: 0.7224 - val_loss: 0.5501 - val_accuracy: 0.7227\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5416 - accuracy: 0.7125 - val_loss: 0.5332 - val_accuracy: 0.7224\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5387 - accuracy: 0.7162 - val_loss: 0.5462 - val_accuracy: 0.7218\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5397 - accuracy: 0.7166 - val_loss: 0.5428 - val_accuracy: 0.7235\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5381 - accuracy: 0.7229 - val_loss: 0.5384 - val_accuracy: 0.7229\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5413 - accuracy: 0.7163 - val_loss: 0.5436 - val_accuracy: 0.7224\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5416 - accuracy: 0.7129 - val_loss: 0.5351 - val_accuracy: 0.7225\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5395 - accuracy: 0.7132 - val_loss: 0.5515 - val_accuracy: 0.7226\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5352 - accuracy: 0.7200 - val_loss: 0.5356 - val_accuracy: 0.7234\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5429 - accuracy: 0.7078 - val_loss: 0.5407 - val_accuracy: 0.7230\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5411 - accuracy: 0.7147 - val_loss: 0.5353 - val_accuracy: 0.7253\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5406 - accuracy: 0.7161 - val_loss: 0.5573 - val_accuracy: 0.7229\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5383 - accuracy: 0.7163 - val_loss: 0.5355 - val_accuracy: 0.7218\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5371 - accuracy: 0.7171 - val_loss: 0.5397 - val_accuracy: 0.7239\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5400 - accuracy: 0.7180 - val_loss: 0.5502 - val_accuracy: 0.7168\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5353 - accuracy: 0.7192 - val_loss: 0.5494 - val_accuracy: 0.7231\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5427 - accuracy: 0.7124 - val_loss: 0.5429 - val_accuracy: 0.7189\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5412 - accuracy: 0.7147 - val_loss: 0.5425 - val_accuracy: 0.7240\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5361 - accuracy: 0.7159 - val_loss: 0.5432 - val_accuracy: 0.7211\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5340 - accuracy: 0.7213 - val_loss: 0.5367 - val_accuracy: 0.7234\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5383 - accuracy: 0.7144 - val_loss: 0.5407 - val_accuracy: 0.7244\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5362 - accuracy: 0.7213 - val_loss: 0.5417 - val_accuracy: 0.7221\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5400 - accuracy: 0.7153 - val_loss: 0.5363 - val_accuracy: 0.7219\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5401 - accuracy: 0.7163 - val_loss: 0.5394 - val_accuracy: 0.7149\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5364 - accuracy: 0.7183 - val_loss: 0.5425 - val_accuracy: 0.7227\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5331 - accuracy: 0.7176 - val_loss: 0.5356 - val_accuracy: 0.7209\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5355 - accuracy: 0.7243\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5363 - accuracy: 0.7160\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5368 - accuracy: 0.7171\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5357 - accuracy: 0.7184\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5357 - accuracy: 0.7165\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5352 - accuracy: 0.7200\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5348 - accuracy: 0.7210\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5352 - accuracy: 0.7167\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5345 - accuracy: 0.7186\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5346 - accuracy: 0.7197\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5339 - accuracy: 0.7187\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5351 - accuracy: 0.7205\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6328 - accuracy: 0.6457 - val_loss: 0.5779 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5661 - accuracy: 0.6890 - val_loss: 0.6144 - val_accuracy: 0.7200\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5594 - accuracy: 0.7121 - val_loss: 0.6014 - val_accuracy: 0.7200\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5560 - accuracy: 0.7111 - val_loss: 0.5743 - val_accuracy: 0.7205\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5524 - accuracy: 0.7147 - val_loss: 0.5701 - val_accuracy: 0.7205\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5539 - accuracy: 0.7079 - val_loss: 0.5426 - val_accuracy: 0.7228\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5505 - accuracy: 0.7144 - val_loss: 0.5524 - val_accuracy: 0.7163\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5503 - accuracy: 0.7105 - val_loss: 0.5553 - val_accuracy: 0.7199\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5491 - accuracy: 0.7134 - val_loss: 0.5542 - val_accuracy: 0.7207\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5499 - accuracy: 0.7141 - val_loss: 0.5496 - val_accuracy: 0.7206\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5515 - accuracy: 0.7131 - val_loss: 0.5463 - val_accuracy: 0.7207\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5479 - accuracy: 0.7158 - val_loss: 0.5537 - val_accuracy: 0.7191\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5452 - accuracy: 0.7170 - val_loss: 0.5464 - val_accuracy: 0.7228\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5487 - accuracy: 0.7147 - val_loss: 0.5472 - val_accuracy: 0.7225\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5482 - accuracy: 0.7146 - val_loss: 0.5418 - val_accuracy: 0.7207\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5461 - accuracy: 0.7153 - val_loss: 0.5427 - val_accuracy: 0.7222\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5406 - accuracy: 0.7188 - val_loss: 0.5374 - val_accuracy: 0.7233\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5447 - accuracy: 0.7180 - val_loss: 0.5489 - val_accuracy: 0.7086\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5461 - accuracy: 0.7144 - val_loss: 0.5495 - val_accuracy: 0.7226\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5432 - accuracy: 0.7176 - val_loss: 0.5485 - val_accuracy: 0.7232\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5482 - accuracy: 0.7151 - val_loss: 0.5365 - val_accuracy: 0.7222\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5509 - accuracy: 0.7125 - val_loss: 0.5488 - val_accuracy: 0.7216\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5453 - accuracy: 0.7167 - val_loss: 0.5424 - val_accuracy: 0.7219\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5449 - accuracy: 0.7172 - val_loss: 0.5462 - val_accuracy: 0.7236\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5425 - accuracy: 0.7196 - val_loss: 0.5472 - val_accuracy: 0.7240\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5473 - accuracy: 0.7126 - val_loss: 0.5473 - val_accuracy: 0.7214\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5453 - accuracy: 0.7160 - val_loss: 0.5418 - val_accuracy: 0.7224\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5445 - accuracy: 0.7142 - val_loss: 0.5442 - val_accuracy: 0.7215\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5442 - accuracy: 0.7150 - val_loss: 0.5417 - val_accuracy: 0.7243\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5481 - accuracy: 0.7144 - val_loss: 0.5469 - val_accuracy: 0.7238\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5436 - accuracy: 0.7175 - val_loss: 0.5524 - val_accuracy: 0.7216\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5446 - accuracy: 0.7176 - val_loss: 0.5411 - val_accuracy: 0.7217\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5467 - accuracy: 0.7132 - val_loss: 0.5381 - val_accuracy: 0.7213\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5407 - accuracy: 0.7190 - val_loss: 0.5394 - val_accuracy: 0.7207\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5432 - accuracy: 0.7144 - val_loss: 0.5407 - val_accuracy: 0.7222\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5437 - accuracy: 0.7175 - val_loss: 0.5366 - val_accuracy: 0.7235\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5424 - accuracy: 0.7173 - val_loss: 0.5360 - val_accuracy: 0.7236\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5414 - accuracy: 0.7169 - val_loss: 0.5427 - val_accuracy: 0.7238\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5458 - accuracy: 0.7123 - val_loss: 0.5325 - val_accuracy: 0.7232\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5441 - accuracy: 0.7170 - val_loss: 0.5394 - val_accuracy: 0.7222\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5412 - accuracy: 0.7190 - val_loss: 0.5521 - val_accuracy: 0.7240\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5438 - accuracy: 0.7159 - val_loss: 0.5411 - val_accuracy: 0.7233\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5452 - accuracy: 0.7135 - val_loss: 0.5395 - val_accuracy: 0.7238\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5435 - accuracy: 0.7180 - val_loss: 0.5375 - val_accuracy: 0.7228\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5430 - accuracy: 0.7232\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5430 - accuracy: 0.7170\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5422 - accuracy: 0.7172\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5422 - accuracy: 0.7168\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5421 - accuracy: 0.7173\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5429 - accuracy: 0.7172\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5417 - accuracy: 0.7163\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5416 - accuracy: 0.7181\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5425 - accuracy: 0.7183\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5410 - accuracy: 0.7181\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5414 - accuracy: 0.7183\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5344 - accuracy: 0.7230\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6302 - accuracy: 0.6480 - val_loss: 0.5825 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5640 - accuracy: 0.6824 - val_loss: 0.5894 - val_accuracy: 0.7181\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5603 - accuracy: 0.7100 - val_loss: 0.5797 - val_accuracy: 0.7196\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5525 - accuracy: 0.7142 - val_loss: 0.5633 - val_accuracy: 0.7189\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5527 - accuracy: 0.7100 - val_loss: 0.5901 - val_accuracy: 0.7202\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5526 - accuracy: 0.7133 - val_loss: 0.5589 - val_accuracy: 0.7218\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5498 - accuracy: 0.7108 - val_loss: 0.5528 - val_accuracy: 0.7201\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5460 - accuracy: 0.7148 - val_loss: 0.5578 - val_accuracy: 0.7206\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5476 - accuracy: 0.7173 - val_loss: 0.5450 - val_accuracy: 0.7226\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5476 - accuracy: 0.7176 - val_loss: 0.5477 - val_accuracy: 0.7211\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5458 - accuracy: 0.7156 - val_loss: 0.5542 - val_accuracy: 0.7197\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5462 - accuracy: 0.7169 - val_loss: 0.5480 - val_accuracy: 0.7151\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5452 - accuracy: 0.7179 - val_loss: 0.5407 - val_accuracy: 0.7212\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5470 - accuracy: 0.7139 - val_loss: 0.5408 - val_accuracy: 0.7233\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5491 - accuracy: 0.7123 - val_loss: 0.5453 - val_accuracy: 0.7202\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5451 - accuracy: 0.7154 - val_loss: 0.5410 - val_accuracy: 0.7210\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5467 - accuracy: 0.7143 - val_loss: 0.5421 - val_accuracy: 0.7195\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5438 - accuracy: 0.7135 - val_loss: 0.5454 - val_accuracy: 0.7195\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5428 - accuracy: 0.7159 - val_loss: 0.5474 - val_accuracy: 0.7240\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5440 - accuracy: 0.7157 - val_loss: 0.5382 - val_accuracy: 0.7193\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5492 - accuracy: 0.7121 - val_loss: 0.5440 - val_accuracy: 0.7207\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5405 - accuracy: 0.7163 - val_loss: 0.5335 - val_accuracy: 0.7233\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5435 - accuracy: 0.7137 - val_loss: 0.5389 - val_accuracy: 0.7227\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5429 - accuracy: 0.7144 - val_loss: 0.5381 - val_accuracy: 0.7222\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5446 - accuracy: 0.7134 - val_loss: 0.5427 - val_accuracy: 0.7232\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5381 - accuracy: 0.7180 - val_loss: 0.5455 - val_accuracy: 0.7159\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5428 - accuracy: 0.7137 - val_loss: 0.5430 - val_accuracy: 0.7226\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5404 - accuracy: 0.7161 - val_loss: 0.5380 - val_accuracy: 0.7233\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5404 - accuracy: 0.7176 - val_loss: 0.5429 - val_accuracy: 0.7229\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5406 - accuracy: 0.7148 - val_loss: 0.5387 - val_accuracy: 0.7229\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5386 - accuracy: 0.7179 - val_loss: 0.5339 - val_accuracy: 0.7190\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5373 - accuracy: 0.7153 - val_loss: 0.5544 - val_accuracy: 0.7162\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5420 - accuracy: 0.7122 - val_loss: 0.5539 - val_accuracy: 0.7201\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5421 - accuracy: 0.7148 - val_loss: 0.5517 - val_accuracy: 0.7173\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5479 - accuracy: 0.7216\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5406 - accuracy: 0.7164\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5396 - accuracy: 0.7155\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5405 - accuracy: 0.7156\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5393 - accuracy: 0.7158\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5391 - accuracy: 0.7188\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5390 - accuracy: 0.7173\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5382 - accuracy: 0.7169\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5380 - accuracy: 0.7189\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5385 - accuracy: 0.7165\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5382 - accuracy: 0.7160\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5397 - accuracy: 0.7200\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 8ms/step - loss: 0.6303 - accuracy: 0.6474 - val_loss: 0.5771 - val_accuracy: 0.7172\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5675 - accuracy: 0.6870 - val_loss: 0.5803 - val_accuracy: 0.7185\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5601 - accuracy: 0.7047 - val_loss: 0.5602 - val_accuracy: 0.7176\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5583 - accuracy: 0.7093 - val_loss: 0.5697 - val_accuracy: 0.7208\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5491 - accuracy: 0.7161 - val_loss: 0.5463 - val_accuracy: 0.7176\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5480 - accuracy: 0.7146 - val_loss: 0.5553 - val_accuracy: 0.7173\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5527 - accuracy: 0.7130 - val_loss: 0.5443 - val_accuracy: 0.7230\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5510 - accuracy: 0.7133 - val_loss: 0.5424 - val_accuracy: 0.7220\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5471 - accuracy: 0.7136 - val_loss: 0.5390 - val_accuracy: 0.7214\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5508 - accuracy: 0.7131 - val_loss: 0.5388 - val_accuracy: 0.7219\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5478 - accuracy: 0.7120 - val_loss: 0.5448 - val_accuracy: 0.7193\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5452 - accuracy: 0.7148 - val_loss: 0.5403 - val_accuracy: 0.7216\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5464 - accuracy: 0.7128 - val_loss: 0.5402 - val_accuracy: 0.7126\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5453 - accuracy: 0.7116 - val_loss: 0.5427 - val_accuracy: 0.7119\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5463 - accuracy: 0.7190 - val_loss: 0.5348 - val_accuracy: 0.7204\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5445 - accuracy: 0.7152 - val_loss: 0.5423 - val_accuracy: 0.7148\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5415 - accuracy: 0.7155 - val_loss: 0.5409 - val_accuracy: 0.7228\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5425 - accuracy: 0.7173 - val_loss: 0.5358 - val_accuracy: 0.7223\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5386 - accuracy: 0.7144 - val_loss: 0.5387 - val_accuracy: 0.7226\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5384 - accuracy: 0.7171 - val_loss: 0.5382 - val_accuracy: 0.7227\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5406 - accuracy: 0.7194 - val_loss: 0.5512 - val_accuracy: 0.7221\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5398 - accuracy: 0.7199 - val_loss: 0.5503 - val_accuracy: 0.7156\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5454 - accuracy: 0.7209\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5454 - accuracy: 0.7165\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5449 - accuracy: 0.7173\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5446 - accuracy: 0.7160\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5452 - accuracy: 0.7156\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5438 - accuracy: 0.7166\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5434 - accuracy: 0.7171\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5433 - accuracy: 0.7161\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5427 - accuracy: 0.7178\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5431 - accuracy: 0.7174\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5428 - accuracy: 0.7183\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.5428 - accuracy: 0.7194\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6270 - accuracy: 0.6499 - val_loss: 0.5868 - val_accuracy: 0.7154\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5656 - accuracy: 0.6977 - val_loss: 0.5903 - val_accuracy: 0.7176\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5586 - accuracy: 0.7114 - val_loss: 0.5774 - val_accuracy: 0.7209\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5560 - accuracy: 0.7134 - val_loss: 0.5677 - val_accuracy: 0.7187\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5532 - accuracy: 0.7139 - val_loss: 0.5565 - val_accuracy: 0.7213\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5497 - accuracy: 0.7148 - val_loss: 0.5425 - val_accuracy: 0.7212\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5538 - accuracy: 0.7103 - val_loss: 0.5618 - val_accuracy: 0.7212\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5480 - accuracy: 0.7186 - val_loss: 0.5486 - val_accuracy: 0.7199\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5489 - accuracy: 0.7157 - val_loss: 0.5384 - val_accuracy: 0.7217\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5494 - accuracy: 0.7154 - val_loss: 0.5538 - val_accuracy: 0.7217\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5498 - accuracy: 0.7147 - val_loss: 0.5478 - val_accuracy: 0.7199\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5489 - accuracy: 0.7165 - val_loss: 0.5484 - val_accuracy: 0.7163\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5508 - accuracy: 0.7134 - val_loss: 0.5437 - val_accuracy: 0.7172\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5480 - accuracy: 0.7150 - val_loss: 0.5417 - val_accuracy: 0.7198\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5432 - accuracy: 0.7186 - val_loss: 0.5502 - val_accuracy: 0.7213\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5447 - accuracy: 0.7159 - val_loss: 0.5482 - val_accuracy: 0.7212\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5453 - accuracy: 0.7151 - val_loss: 0.5474 - val_accuracy: 0.7155\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5458 - accuracy: 0.7156 - val_loss: 0.5399 - val_accuracy: 0.7230\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5454 - accuracy: 0.7147 - val_loss: 0.5443 - val_accuracy: 0.7212\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5441 - accuracy: 0.7135 - val_loss: 0.5514 - val_accuracy: 0.7225\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5431 - accuracy: 0.7187 - val_loss: 0.5473 - val_accuracy: 0.7223\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5411 - accuracy: 0.7166 - val_loss: 0.5466 - val_accuracy: 0.7164\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5430 - accuracy: 0.7157 - val_loss: 0.5360 - val_accuracy: 0.7230\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5448 - accuracy: 0.7152 - val_loss: 0.5330 - val_accuracy: 0.7225\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5390 - accuracy: 0.7215 - val_loss: 0.5504 - val_accuracy: 0.7191\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5409 - accuracy: 0.7162 - val_loss: 0.5337 - val_accuracy: 0.7218\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5383 - accuracy: 0.7179 - val_loss: 0.5349 - val_accuracy: 0.7215\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5404 - accuracy: 0.7155 - val_loss: 0.5525 - val_accuracy: 0.7227\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5382 - accuracy: 0.7205 - val_loss: 0.5364 - val_accuracy: 0.7203\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5382 - accuracy: 0.7209 - val_loss: 0.5476 - val_accuracy: 0.7218\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5398 - accuracy: 0.7178 - val_loss: 0.5438 - val_accuracy: 0.7221\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5452 - accuracy: 0.7128 - val_loss: 0.5359 - val_accuracy: 0.7223\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5414 - accuracy: 0.7157 - val_loss: 0.5465 - val_accuracy: 0.7221\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5402 - accuracy: 0.7233\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5400 - accuracy: 0.7177\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5406 - accuracy: 0.7173\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5395 - accuracy: 0.7173\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5391 - accuracy: 0.7160\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5381 - accuracy: 0.7173\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5382 - accuracy: 0.7182\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5369 - accuracy: 0.7175\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5367 - accuracy: 0.7178\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5366 - accuracy: 0.7198\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5370 - accuracy: 0.7173\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.5400 - accuracy: 0.7154\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-f-gOTAoQPO",
        "outputId": "91218be4-0329-483c-b80b-cf2916bfbaa5"
      },
      "source": [
        "print(round(cv_results_6.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7197\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSFJ132soQPO",
        "outputId": "8dc49ab4-9405-404b-8e21-e1942af66a4e"
      },
      "source": [
        "print(round(cv_results_6.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpYVhu5HqaJe",
        "outputId": "809a84d9-7278-43d1-ac57-c2691bb257bc"
      },
      "source": [
        "cv_results_6"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.72047377, 0.72295547, 0.71995038, 0.71944273, 0.71543801])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_tCBSlPoQPO",
        "outputId": "613e12a3-b8e8-4160-9adc-9445807ba5a7"
      },
      "source": [
        "cv6_preds = cross_val_predict(mod6, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6334 - accuracy: 0.6398 - val_loss: 0.5792 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5663 - accuracy: 0.6852 - val_loss: 0.5644 - val_accuracy: 0.7183\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5629 - accuracy: 0.7094 - val_loss: 0.5745 - val_accuracy: 0.7199\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5558 - accuracy: 0.7166 - val_loss: 0.5650 - val_accuracy: 0.7204\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5539 - accuracy: 0.7137 - val_loss: 0.5665 - val_accuracy: 0.7098\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5491 - accuracy: 0.7156 - val_loss: 0.5466 - val_accuracy: 0.7211\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5529 - accuracy: 0.7109 - val_loss: 0.5495 - val_accuracy: 0.7198\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5475 - accuracy: 0.7157 - val_loss: 0.5486 - val_accuracy: 0.7224\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5505 - accuracy: 0.7123 - val_loss: 0.5474 - val_accuracy: 0.7217\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5439 - accuracy: 0.7128 - val_loss: 0.5532 - val_accuracy: 0.7212\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5480 - accuracy: 0.7154 - val_loss: 0.5459 - val_accuracy: 0.7186\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5483 - accuracy: 0.7136 - val_loss: 0.5502 - val_accuracy: 0.7216\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5439 - accuracy: 0.7127 - val_loss: 0.5456 - val_accuracy: 0.7210\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5420 - accuracy: 0.7167 - val_loss: 0.5539 - val_accuracy: 0.7194\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5456 - accuracy: 0.7129 - val_loss: 0.5468 - val_accuracy: 0.7169\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5423 - accuracy: 0.7148 - val_loss: 0.5493 - val_accuracy: 0.7199\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5468 - accuracy: 0.7093 - val_loss: 0.5363 - val_accuracy: 0.7205\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5417 - accuracy: 0.7122 - val_loss: 0.5547 - val_accuracy: 0.7231\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5418 - accuracy: 0.7160 - val_loss: 0.5470 - val_accuracy: 0.7231\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5447 - accuracy: 0.7139 - val_loss: 0.5436 - val_accuracy: 0.7227\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5445 - accuracy: 0.7133 - val_loss: 0.5337 - val_accuracy: 0.7236\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5452 - accuracy: 0.7105 - val_loss: 0.5380 - val_accuracy: 0.7235\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5427 - accuracy: 0.7135 - val_loss: 0.5505 - val_accuracy: 0.7158\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5420 - accuracy: 0.7131 - val_loss: 0.5395 - val_accuracy: 0.7236\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5404 - accuracy: 0.7164 - val_loss: 0.5465 - val_accuracy: 0.7222\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5425 - accuracy: 0.7138 - val_loss: 0.5555 - val_accuracy: 0.7220\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5408 - accuracy: 0.7160 - val_loss: 0.5627 - val_accuracy: 0.7078\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5409 - accuracy: 0.7152 - val_loss: 0.5458 - val_accuracy: 0.7224\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5388 - accuracy: 0.7190 - val_loss: 0.5465 - val_accuracy: 0.7189\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5442 - accuracy: 0.7144 - val_loss: 0.5628 - val_accuracy: 0.7138\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5398 - accuracy: 0.7160 - val_loss: 0.5325 - val_accuracy: 0.7235\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5434 - accuracy: 0.7140 - val_loss: 0.5465 - val_accuracy: 0.7222\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5371 - accuracy: 0.7170 - val_loss: 0.5491 - val_accuracy: 0.7212\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5367 - accuracy: 0.7159 - val_loss: 0.5338 - val_accuracy: 0.7227\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5394 - accuracy: 0.7132 - val_loss: 0.5474 - val_accuracy: 0.7206\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5431 - accuracy: 0.7104 - val_loss: 0.5450 - val_accuracy: 0.7212\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5352 - accuracy: 0.7230\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5408 - accuracy: 0.7165\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5391 - accuracy: 0.7172\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5400 - accuracy: 0.7160\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5396 - accuracy: 0.7152\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5388 - accuracy: 0.7175\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5379 - accuracy: 0.7186\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5381 - accuracy: 0.7164\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5377 - accuracy: 0.7177\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5373 - accuracy: 0.7154\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5377 - accuracy: 0.7177\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6252 - accuracy: 0.6487 - val_loss: 0.5729 - val_accuracy: 0.7151\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5719 - accuracy: 0.6921 - val_loss: 0.5897 - val_accuracy: 0.7197\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5602 - accuracy: 0.7119 - val_loss: 0.5723 - val_accuracy: 0.7163\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5534 - accuracy: 0.7102 - val_loss: 0.5866 - val_accuracy: 0.7202\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5544 - accuracy: 0.7100 - val_loss: 0.5558 - val_accuracy: 0.7216\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5543 - accuracy: 0.7108 - val_loss: 0.5528 - val_accuracy: 0.7199\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5514 - accuracy: 0.7129 - val_loss: 0.5580 - val_accuracy: 0.7153\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5464 - accuracy: 0.7170 - val_loss: 0.5484 - val_accuracy: 0.7228\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5498 - accuracy: 0.7147 - val_loss: 0.5446 - val_accuracy: 0.7217\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5476 - accuracy: 0.7147 - val_loss: 0.5594 - val_accuracy: 0.7167\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5507 - accuracy: 0.7097 - val_loss: 0.5402 - val_accuracy: 0.7231\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5480 - accuracy: 0.7149 - val_loss: 0.5361 - val_accuracy: 0.7185\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5477 - accuracy: 0.7149 - val_loss: 0.5565 - val_accuracy: 0.7210\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5494 - accuracy: 0.7162 - val_loss: 0.5394 - val_accuracy: 0.7207\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5495 - accuracy: 0.7145 - val_loss: 0.5437 - val_accuracy: 0.7225\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5422 - accuracy: 0.7205 - val_loss: 0.5499 - val_accuracy: 0.7167\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5480 - accuracy: 0.7129 - val_loss: 0.5424 - val_accuracy: 0.7193\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5454 - accuracy: 0.7169 - val_loss: 0.5444 - val_accuracy: 0.7215\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5455 - accuracy: 0.7166 - val_loss: 0.5484 - val_accuracy: 0.7213\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5409 - accuracy: 0.7208 - val_loss: 0.5432 - val_accuracy: 0.7222\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5416 - accuracy: 0.7169 - val_loss: 0.5518 - val_accuracy: 0.7222\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5457 - accuracy: 0.7143 - val_loss: 0.5355 - val_accuracy: 0.7237\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5450 - accuracy: 0.7193 - val_loss: 0.5463 - val_accuracy: 0.7171\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5414 - accuracy: 0.7192 - val_loss: 0.5476 - val_accuracy: 0.7221\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5440 - accuracy: 0.7145 - val_loss: 0.5400 - val_accuracy: 0.7213\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5443 - accuracy: 0.7181 - val_loss: 0.5392 - val_accuracy: 0.7234\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5487 - accuracy: 0.7139 - val_loss: 0.5400 - val_accuracy: 0.7233\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5445 - accuracy: 0.7160 - val_loss: 0.5432 - val_accuracy: 0.7216\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5421 - accuracy: 0.7190 - val_loss: 0.5456 - val_accuracy: 0.7234\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5428 - accuracy: 0.7166 - val_loss: 0.5447 - val_accuracy: 0.7226\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5437 - accuracy: 0.7172 - val_loss: 0.5496 - val_accuracy: 0.7219\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5465 - accuracy: 0.7144 - val_loss: 0.5428 - val_accuracy: 0.7237\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5445 - accuracy: 0.7163 - val_loss: 0.5424 - val_accuracy: 0.7212\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5488 - accuracy: 0.7134 - val_loss: 0.5413 - val_accuracy: 0.7232\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5471 - accuracy: 0.7166 - val_loss: 0.5401 - val_accuracy: 0.7230\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5457 - accuracy: 0.7136 - val_loss: 0.5386 - val_accuracy: 0.7233\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5434 - accuracy: 0.7167 - val_loss: 0.5403 - val_accuracy: 0.7224\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5454 - accuracy: 0.7151 - val_loss: 0.5399 - val_accuracy: 0.7227\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5408 - accuracy: 0.7158 - val_loss: 0.5332 - val_accuracy: 0.7235\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5446 - accuracy: 0.7160 - val_loss: 0.5417 - val_accuracy: 0.7215\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5451 - accuracy: 0.7151 - val_loss: 0.5415 - val_accuracy: 0.7214\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5391 - accuracy: 0.7184 - val_loss: 0.5455 - val_accuracy: 0.7240\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5455 - accuracy: 0.7149 - val_loss: 0.5419 - val_accuracy: 0.7238\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5483 - accuracy: 0.7143 - val_loss: 0.5385 - val_accuracy: 0.7235\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5394 - accuracy: 0.7225 - val_loss: 0.5396 - val_accuracy: 0.7226\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5485 - accuracy: 0.7148 - val_loss: 0.5478 - val_accuracy: 0.7234\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5435 - accuracy: 0.7192 - val_loss: 0.5367 - val_accuracy: 0.7243\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5431 - accuracy: 0.7185 - val_loss: 0.5392 - val_accuracy: 0.7219\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5433 - accuracy: 0.7190 - val_loss: 0.5361 - val_accuracy: 0.7227\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5416 - accuracy: 0.7176 - val_loss: 0.5359 - val_accuracy: 0.7229\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5375 - accuracy: 0.7224\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5401 - accuracy: 0.7194\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5405 - accuracy: 0.7194\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5395 - accuracy: 0.7194\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5399 - accuracy: 0.7186\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5398 - accuracy: 0.7186\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5400 - accuracy: 0.7186\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5394 - accuracy: 0.7183\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5388 - accuracy: 0.7195\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5394 - accuracy: 0.7205\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5394 - accuracy: 0.7195\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6288 - accuracy: 0.6471 - val_loss: 0.5932 - val_accuracy: 0.7158\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5652 - accuracy: 0.7014 - val_loss: 0.6032 - val_accuracy: 0.7188\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5588 - accuracy: 0.7089 - val_loss: 0.5770 - val_accuracy: 0.7208\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5544 - accuracy: 0.7128 - val_loss: 0.5611 - val_accuracy: 0.7166\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5511 - accuracy: 0.7138 - val_loss: 0.5602 - val_accuracy: 0.7207\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5513 - accuracy: 0.7100 - val_loss: 0.5457 - val_accuracy: 0.7180\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5511 - accuracy: 0.7152 - val_loss: 0.5603 - val_accuracy: 0.7193\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5485 - accuracy: 0.7141 - val_loss: 0.5395 - val_accuracy: 0.7211\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5488 - accuracy: 0.7132 - val_loss: 0.5577 - val_accuracy: 0.7119\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5490 - accuracy: 0.7149 - val_loss: 0.5567 - val_accuracy: 0.7093\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5486 - accuracy: 0.7114 - val_loss: 0.5397 - val_accuracy: 0.7213\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5491 - accuracy: 0.7136 - val_loss: 0.5499 - val_accuracy: 0.7214\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5466 - accuracy: 0.7142 - val_loss: 0.5358 - val_accuracy: 0.7223\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5448 - accuracy: 0.7150 - val_loss: 0.5499 - val_accuracy: 0.7175\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5445 - accuracy: 0.7151 - val_loss: 0.5461 - val_accuracy: 0.7228\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5450 - accuracy: 0.7154 - val_loss: 0.5414 - val_accuracy: 0.7179\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5443 - accuracy: 0.7160 - val_loss: 0.5428 - val_accuracy: 0.7167\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5407 - accuracy: 0.7169 - val_loss: 0.5360 - val_accuracy: 0.7193\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5431 - accuracy: 0.7131 - val_loss: 0.5536 - val_accuracy: 0.7231\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5435 - accuracy: 0.7127 - val_loss: 0.5523 - val_accuracy: 0.7082\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5439 - accuracy: 0.7154 - val_loss: 0.5374 - val_accuracy: 0.7229\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5412 - accuracy: 0.7174 - val_loss: 0.5382 - val_accuracy: 0.7230\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5408 - accuracy: 0.7180 - val_loss: 0.5378 - val_accuracy: 0.7225\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5454 - accuracy: 0.7117 - val_loss: 0.5429 - val_accuracy: 0.7226\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5403 - accuracy: 0.7175 - val_loss: 0.5434 - val_accuracy: 0.7235\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5394 - accuracy: 0.7178 - val_loss: 0.5573 - val_accuracy: 0.7212\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5389 - accuracy: 0.7185 - val_loss: 0.5491 - val_accuracy: 0.7240\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5409 - accuracy: 0.7157 - val_loss: 0.5461 - val_accuracy: 0.7202\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5432 - accuracy: 0.7142 - val_loss: 0.5553 - val_accuracy: 0.7224\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5418 - accuracy: 0.7137 - val_loss: 0.5408 - val_accuracy: 0.7223\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5395 - accuracy: 0.7175 - val_loss: 0.5527 - val_accuracy: 0.7193\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5400 - accuracy: 0.7145 - val_loss: 0.5381 - val_accuracy: 0.7236\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5430 - accuracy: 0.7131 - val_loss: 0.5488 - val_accuracy: 0.7200\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5435 - accuracy: 0.7144 - val_loss: 0.5541 - val_accuracy: 0.7228\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5387 - accuracy: 0.7174 - val_loss: 0.5436 - val_accuracy: 0.7208\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5411 - accuracy: 0.7129 - val_loss: 0.5360 - val_accuracy: 0.7216\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5438 - accuracy: 0.7189 - val_loss: 0.5309 - val_accuracy: 0.7237\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5400 - accuracy: 0.7182 - val_loss: 0.5434 - val_accuracy: 0.7235\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5416 - accuracy: 0.7152 - val_loss: 0.5422 - val_accuracy: 0.7230\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5386 - accuracy: 0.7201 - val_loss: 0.5446 - val_accuracy: 0.7231\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5378 - accuracy: 0.7187 - val_loss: 0.5507 - val_accuracy: 0.7225\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5404 - accuracy: 0.7163 - val_loss: 0.5442 - val_accuracy: 0.7243\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5420 - accuracy: 0.7162 - val_loss: 0.5429 - val_accuracy: 0.7204\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5371 - accuracy: 0.7209 - val_loss: 0.5463 - val_accuracy: 0.7211\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5370 - accuracy: 0.7200 - val_loss: 0.5421 - val_accuracy: 0.7224\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5408 - accuracy: 0.7151 - val_loss: 0.5493 - val_accuracy: 0.7201\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5361 - accuracy: 0.7196 - val_loss: 0.5561 - val_accuracy: 0.7163\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5380 - accuracy: 0.7188 - val_loss: 0.5363 - val_accuracy: 0.7183\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5373 - accuracy: 0.7191 - val_loss: 0.5430 - val_accuracy: 0.7226\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5372 - accuracy: 0.7240 - val_loss: 0.5374 - val_accuracy: 0.7212\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5376 - accuracy: 0.7196\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5337 - accuracy: 0.7208\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5337 - accuracy: 0.7241\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5330 - accuracy: 0.7235\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5327 - accuracy: 0.7225\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5324 - accuracy: 0.7236\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5322 - accuracy: 0.7239\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5312 - accuracy: 0.7234\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5317 - accuracy: 0.7259\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5308 - accuracy: 0.7252\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5319 - accuracy: 0.7251\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6226 - accuracy: 0.6499 - val_loss: 0.5794 - val_accuracy: 0.7164\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5631 - accuracy: 0.6997 - val_loss: 0.6032 - val_accuracy: 0.7099\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5612 - accuracy: 0.7085 - val_loss: 0.5820 - val_accuracy: 0.7189\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5571 - accuracy: 0.7111 - val_loss: 0.5830 - val_accuracy: 0.7194\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5531 - accuracy: 0.7163 - val_loss: 0.5770 - val_accuracy: 0.7203\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5544 - accuracy: 0.7124 - val_loss: 0.5533 - val_accuracy: 0.7118\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5494 - accuracy: 0.7129 - val_loss: 0.5606 - val_accuracy: 0.7220\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5501 - accuracy: 0.7147 - val_loss: 0.5577 - val_accuracy: 0.7217\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5473 - accuracy: 0.7190 - val_loss: 0.5560 - val_accuracy: 0.7149\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5531 - accuracy: 0.7109 - val_loss: 0.5527 - val_accuracy: 0.7151\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5442 - accuracy: 0.7186 - val_loss: 0.5399 - val_accuracy: 0.7229\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5453 - accuracy: 0.7186 - val_loss: 0.5521 - val_accuracy: 0.7217\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5459 - accuracy: 0.7120 - val_loss: 0.5486 - val_accuracy: 0.7228\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5458 - accuracy: 0.7178 - val_loss: 0.5546 - val_accuracy: 0.7185\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5432 - accuracy: 0.7172 - val_loss: 0.5450 - val_accuracy: 0.7219\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5453 - accuracy: 0.7182 - val_loss: 0.5403 - val_accuracy: 0.7229\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5464 - accuracy: 0.7153 - val_loss: 0.5440 - val_accuracy: 0.7216\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5489 - accuracy: 0.7135 - val_loss: 0.5442 - val_accuracy: 0.7215\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5461 - accuracy: 0.7135 - val_loss: 0.5558 - val_accuracy: 0.7213\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5440 - accuracy: 0.7187 - val_loss: 0.5465 - val_accuracy: 0.7201\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5438 - accuracy: 0.7138 - val_loss: 0.5417 - val_accuracy: 0.7198\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5434 - accuracy: 0.7146 - val_loss: 0.5451 - val_accuracy: 0.7188\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5422 - accuracy: 0.7164 - val_loss: 0.5365 - val_accuracy: 0.7231\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5412 - accuracy: 0.7163 - val_loss: 0.5339 - val_accuracy: 0.7202\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5448 - accuracy: 0.7141 - val_loss: 0.5379 - val_accuracy: 0.7224\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5428 - accuracy: 0.7140 - val_loss: 0.5350 - val_accuracy: 0.7215\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5424 - accuracy: 0.7129 - val_loss: 0.5422 - val_accuracy: 0.7232\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5377 - accuracy: 0.7171 - val_loss: 0.5494 - val_accuracy: 0.7187\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5381 - accuracy: 0.7189 - val_loss: 0.5473 - val_accuracy: 0.7198\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5395 - accuracy: 0.7133 - val_loss: 0.5367 - val_accuracy: 0.7197\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5446 - accuracy: 0.7157 - val_loss: 0.5427 - val_accuracy: 0.7202\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5385 - accuracy: 0.7173 - val_loss: 0.5335 - val_accuracy: 0.7215\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5414 - accuracy: 0.7143 - val_loss: 0.5302 - val_accuracy: 0.7220\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5386 - accuracy: 0.7179 - val_loss: 0.5430 - val_accuracy: 0.7226\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5393 - accuracy: 0.7195 - val_loss: 0.5378 - val_accuracy: 0.7231\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5394 - accuracy: 0.7179 - val_loss: 0.5381 - val_accuracy: 0.7214\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5374 - accuracy: 0.7175 - val_loss: 0.5336 - val_accuracy: 0.7239\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5386 - accuracy: 0.7204 - val_loss: 0.5361 - val_accuracy: 0.7193\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5423 - accuracy: 0.7170 - val_loss: 0.5479 - val_accuracy: 0.7239\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5352 - accuracy: 0.7183 - val_loss: 0.5364 - val_accuracy: 0.7187\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5388 - accuracy: 0.7209 - val_loss: 0.5435 - val_accuracy: 0.7189\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5396 - accuracy: 0.7161 - val_loss: 0.5345 - val_accuracy: 0.7241\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5396 - accuracy: 0.7197 - val_loss: 0.5346 - val_accuracy: 0.7213\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5436 - accuracy: 0.7151 - val_loss: 0.5372 - val_accuracy: 0.7243\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5411 - accuracy: 0.7162 - val_loss: 0.5384 - val_accuracy: 0.7239\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5370 - accuracy: 0.7195 - val_loss: 0.5443 - val_accuracy: 0.7188\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5353 - accuracy: 0.7224 - val_loss: 0.5356 - val_accuracy: 0.7221\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5389 - accuracy: 0.7207 - val_loss: 0.5404 - val_accuracy: 0.7240\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5380 - accuracy: 0.7172 - val_loss: 0.5365 - val_accuracy: 0.7244\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5409 - accuracy: 0.7136 - val_loss: 0.5395 - val_accuracy: 0.7239\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5393 - accuracy: 0.7249\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5388 - accuracy: 0.7168\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5373 - accuracy: 0.7205\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5377 - accuracy: 0.7198\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5383 - accuracy: 0.7180\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5378 - accuracy: 0.7176\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5381 - accuracy: 0.7189\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.5381 - accuracy: 0.7181\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5372 - accuracy: 0.7174\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5375 - accuracy: 0.7198\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5371 - accuracy: 0.7181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6250 - accuracy: 0.6536 - val_loss: 0.5754 - val_accuracy: 0.7165\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5688 - accuracy: 0.7011 - val_loss: 0.5972 - val_accuracy: 0.7194\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5600 - accuracy: 0.7106 - val_loss: 0.5707 - val_accuracy: 0.7200\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5550 - accuracy: 0.7112 - val_loss: 0.5804 - val_accuracy: 0.7182\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5505 - accuracy: 0.7163 - val_loss: 0.5668 - val_accuracy: 0.7222\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5496 - accuracy: 0.7154 - val_loss: 0.5700 - val_accuracy: 0.7234\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5518 - accuracy: 0.7123 - val_loss: 0.5562 - val_accuracy: 0.7226\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5446 - accuracy: 0.7184 - val_loss: 0.5539 - val_accuracy: 0.7128\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5451 - accuracy: 0.7158 - val_loss: 0.5491 - val_accuracy: 0.7165\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5502 - accuracy: 0.7127 - val_loss: 0.5488 - val_accuracy: 0.7210\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5473 - accuracy: 0.7143 - val_loss: 0.5539 - val_accuracy: 0.7221\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5435 - accuracy: 0.7174 - val_loss: 0.5503 - val_accuracy: 0.7212\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5501 - accuracy: 0.7168 - val_loss: 0.5441 - val_accuracy: 0.7239\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5483 - accuracy: 0.7151 - val_loss: 0.5383 - val_accuracy: 0.7232\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5482 - accuracy: 0.7139 - val_loss: 0.5520 - val_accuracy: 0.7230\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5441 - accuracy: 0.7182 - val_loss: 0.5485 - val_accuracy: 0.7161\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5462 - accuracy: 0.7130 - val_loss: 0.5551 - val_accuracy: 0.7162\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5451 - accuracy: 0.7178 - val_loss: 0.5387 - val_accuracy: 0.7232\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5477 - accuracy: 0.7135 - val_loss: 0.5419 - val_accuracy: 0.7226\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5418 - accuracy: 0.7176 - val_loss: 0.5481 - val_accuracy: 0.7243\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5452 - accuracy: 0.7160 - val_loss: 0.5412 - val_accuracy: 0.7224\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5445 - accuracy: 0.7146 - val_loss: 0.5501 - val_accuracy: 0.7215\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5385 - accuracy: 0.7201 - val_loss: 0.5393 - val_accuracy: 0.7229\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5412 - accuracy: 0.7169 - val_loss: 0.5365 - val_accuracy: 0.7235\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5418 - accuracy: 0.7156 - val_loss: 0.5386 - val_accuracy: 0.7225\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5414 - accuracy: 0.7196 - val_loss: 0.5359 - val_accuracy: 0.7229\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5426 - accuracy: 0.7160 - val_loss: 0.5388 - val_accuracy: 0.7238\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5409 - accuracy: 0.7169 - val_loss: 0.5492 - val_accuracy: 0.7177\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5410 - accuracy: 0.7132 - val_loss: 0.5453 - val_accuracy: 0.7192\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5410 - accuracy: 0.7168 - val_loss: 0.5464 - val_accuracy: 0.7243\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5405 - accuracy: 0.7173 - val_loss: 0.5441 - val_accuracy: 0.7233\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5405 - accuracy: 0.7160 - val_loss: 0.5400 - val_accuracy: 0.7235\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5392 - accuracy: 0.7209 - val_loss: 0.5412 - val_accuracy: 0.7230\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5373 - accuracy: 0.7230 - val_loss: 0.5483 - val_accuracy: 0.7233\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5373 - accuracy: 0.7210 - val_loss: 0.5584 - val_accuracy: 0.7228\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.5487 - accuracy: 0.7221\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5390 - accuracy: 0.7199\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5383 - accuracy: 0.7197\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5372 - accuracy: 0.7176\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5369 - accuracy: 0.7172\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5364 - accuracy: 0.7186\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.5362 - accuracy: 0.7187\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5359 - accuracy: 0.7176\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5362 - accuracy: 0.7175\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5344 - accuracy: 0.7185\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.5343 - accuracy: 0.7181\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KjNg_cv4Jye7",
        "outputId": "76fd4b06-295d-42cb-be47-c13998489728"
      },
      "source": [
        "cm6 = confusion_matrix(y, cv6_preds)\n",
        "print(cm6)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[41600 16400]\n",
            " [ 8383 22264]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL2R3T7WdCN0"
      },
      "source": [
        "# neural network on top 10 most important features per recursive feature elimination package \n",
        "\n",
        "## (big thanks to Jack for getting these for us)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "gfeHOaPu2yPU",
        "outputId": "eff7a230-546f-470b-ac94-d769e08a3a96"
      },
      "source": [
        "y = full_df.iloc[:,-1]\n",
        "\n",
        "features = ['qty_slash_url', 'length_url', 'qty_hyphen_directory', 'qty_underline_directory', 'qty_slash_directory', 'directory_length', 'qty_hyphen_file', 'file_length', 'asn_ip', 'time_domain_activation']\n",
        "X = full_df[features]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.25, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004613</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999989</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000984</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.932855</td>\n",
              "      <td>0.360252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.956567</td>\n",
              "      <td>0.291508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000921</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.975658</td>\n",
              "      <td>0.219294</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_slash_url  length_url  ...    asn_ip  time_domain_activation\n",
              "5676        0.000004    0.000042  ...  1.000000                0.000591\n",
              "39002       0.000000    0.004613  ...  0.999989                0.000000\n",
              "1732        0.000000    0.000984  ...  0.932855                0.360252\n",
              "39668       0.000000    0.001769  ...  0.956567                0.291508\n",
              "82035       0.000000    0.000921  ...  0.975658                0.219294\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 205
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLQoSVVZ2yPU",
        "outputId": "aad2469e-be35-4752-918e-ffebcbfebe0a"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "top_10_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[10]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "top_10_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "top_10_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                704       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 13,601\n",
            "Trainable params: 13,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylYcNGZM2yPV",
        "outputId": "fdb30023-abe2-40df-a257-84097bfa22bd"
      },
      "source": [
        "history1 = top_10_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6274 - binary_accuracy: 0.6444 - auc: 0.6111 - val_loss: 0.5834 - val_binary_accuracy: 0.7110 - val_auc: 0.7485\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5638 - binary_accuracy: 0.6969 - auc: 0.7355 - val_loss: 0.5623 - val_binary_accuracy: 0.7130 - val_auc: 0.7572\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5506 - binary_accuracy: 0.7082 - auc: 0.7470 - val_loss: 0.5650 - val_binary_accuracy: 0.7164 - val_auc: 0.7717\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5285 - binary_accuracy: 0.7223 - auc: 0.7655 - val_loss: 0.5552 - val_binary_accuracy: 0.6950 - val_auc: 0.7802\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4760 - binary_accuracy: 0.7565 - auc: 0.8120 - val_loss: 0.4350 - val_binary_accuracy: 0.8071 - val_auc: 0.8747\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4330 - binary_accuracy: 0.7929 - auc: 0.8515 - val_loss: 0.4248 - val_binary_accuracy: 0.8122 - val_auc: 0.8940\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4255 - binary_accuracy: 0.7950 - auc: 0.8623 - val_loss: 0.4306 - val_binary_accuracy: 0.8129 - val_auc: 0.9085\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3989 - binary_accuracy: 0.8138 - auc: 0.8874 - val_loss: 0.4069 - val_binary_accuracy: 0.8402 - val_auc: 0.9214\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3852 - binary_accuracy: 0.8236 - auc: 0.8971 - val_loss: 0.4765 - val_binary_accuracy: 0.8078 - val_auc: 0.9264\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3685 - binary_accuracy: 0.8346 - auc: 0.9081 - val_loss: 0.4064 - val_binary_accuracy: 0.8188 - val_auc: 0.9208\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3616 - binary_accuracy: 0.8387 - auc: 0.9115 - val_loss: 0.4274 - val_binary_accuracy: 0.8000 - val_auc: 0.9072\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3508 - binary_accuracy: 0.8481 - auc: 0.9138 - val_loss: 0.3784 - val_binary_accuracy: 0.8418 - val_auc: 0.9323\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3481 - binary_accuracy: 0.8494 - auc: 0.9157 - val_loss: 0.3680 - val_binary_accuracy: 0.8793 - val_auc: 0.9377\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3360 - binary_accuracy: 0.8577 - auc: 0.9205 - val_loss: 0.3837 - val_binary_accuracy: 0.8815 - val_auc: 0.9410\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3252 - binary_accuracy: 0.8628 - auc: 0.9249 - val_loss: 0.3859 - val_binary_accuracy: 0.8242 - val_auc: 0.9235\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3255 - binary_accuracy: 0.8635 - auc: 0.9251 - val_loss: 0.3000 - val_binary_accuracy: 0.8853 - val_auc: 0.9491\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3210 - binary_accuracy: 0.8641 - auc: 0.9265 - val_loss: 0.3198 - val_binary_accuracy: 0.8865 - val_auc: 0.9407\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3118 - binary_accuracy: 0.8683 - auc: 0.9318 - val_loss: 0.3411 - val_binary_accuracy: 0.8941 - val_auc: 0.9505\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3123 - binary_accuracy: 0.8671 - auc: 0.9310 - val_loss: 0.3955 - val_binary_accuracy: 0.8802 - val_auc: 0.9475\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3038 - binary_accuracy: 0.8702 - auc: 0.9341 - val_loss: 0.3157 - val_binary_accuracy: 0.8784 - val_auc: 0.9493\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3214 - binary_accuracy: 0.8650 - auc: 0.9269 - val_loss: 0.3327 - val_binary_accuracy: 0.8910 - val_auc: 0.9529\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3188 - binary_accuracy: 0.8671 - auc: 0.9279 - val_loss: 0.3906 - val_binary_accuracy: 0.8896 - val_auc: 0.9458\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3106 - binary_accuracy: 0.8748 - auc: 0.9307 - val_loss: 0.3283 - val_binary_accuracy: 0.8919 - val_auc: 0.9549\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3062 - binary_accuracy: 0.8723 - auc: 0.9338 - val_loss: 0.3085 - val_binary_accuracy: 0.8903 - val_auc: 0.9564\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2984 - binary_accuracy: 0.8765 - auc: 0.9369 - val_loss: 0.3019 - val_binary_accuracy: 0.8940 - val_auc: 0.9506\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2924 - binary_accuracy: 0.8794 - auc: 0.9402 - val_loss: 0.3032 - val_binary_accuracy: 0.8876 - val_auc: 0.9518\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3012 - binary_accuracy: 0.8764 - auc: 0.9359 - val_loss: 0.2856 - val_binary_accuracy: 0.9060 - val_auc: 0.9506\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3003 - binary_accuracy: 0.8779 - auc: 0.9367 - val_loss: 0.2960 - val_binary_accuracy: 0.8923 - val_auc: 0.9543\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2905 - binary_accuracy: 0.8826 - auc: 0.9402 - val_loss: 0.3327 - val_binary_accuracy: 0.9009 - val_auc: 0.9535\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2967 - binary_accuracy: 0.8795 - auc: 0.9384 - val_loss: 0.3450 - val_binary_accuracy: 0.9120 - val_auc: 0.9547\n",
            "Epoch 31/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3046 - binary_accuracy: 0.8737 - auc: 0.9357 - val_loss: 0.3024 - val_binary_accuracy: 0.8825 - val_auc: 0.9539\n",
            "Epoch 32/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3001 - binary_accuracy: 0.8752 - auc: 0.9366 - val_loss: 0.3070 - val_binary_accuracy: 0.8986 - val_auc: 0.9565\n",
            "Epoch 33/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2882 - binary_accuracy: 0.8837 - auc: 0.9426 - val_loss: 0.3042 - val_binary_accuracy: 0.8997 - val_auc: 0.9574\n",
            "Epoch 34/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2903 - binary_accuracy: 0.8828 - auc: 0.9404 - val_loss: 0.3037 - val_binary_accuracy: 0.9029 - val_auc: 0.9598\n",
            "Epoch 35/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2868 - binary_accuracy: 0.8821 - auc: 0.9419 - val_loss: 0.3143 - val_binary_accuracy: 0.8853 - val_auc: 0.9555\n",
            "Epoch 36/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2900 - binary_accuracy: 0.8808 - auc: 0.9408 - val_loss: 0.2864 - val_binary_accuracy: 0.8933 - val_auc: 0.9570\n",
            "Epoch 37/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2942 - binary_accuracy: 0.8803 - auc: 0.9387 - val_loss: 0.2805 - val_binary_accuracy: 0.8984 - val_auc: 0.9597\n",
            "Epoch 38/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2841 - binary_accuracy: 0.8849 - auc: 0.9436 - val_loss: 0.2905 - val_binary_accuracy: 0.9039 - val_auc: 0.9614\n",
            "Epoch 39/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2930 - binary_accuracy: 0.8787 - auc: 0.9408 - val_loss: 0.2751 - val_binary_accuracy: 0.8958 - val_auc: 0.9570\n",
            "Epoch 40/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2891 - binary_accuracy: 0.8802 - auc: 0.9417 - val_loss: 0.2809 - val_binary_accuracy: 0.9040 - val_auc: 0.9573\n",
            "Epoch 41/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2846 - binary_accuracy: 0.8861 - auc: 0.9429 - val_loss: 0.2949 - val_binary_accuracy: 0.8963 - val_auc: 0.9541\n",
            "Epoch 42/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2917 - binary_accuracy: 0.8793 - auc: 0.9406 - val_loss: 0.2948 - val_binary_accuracy: 0.9079 - val_auc: 0.9561\n",
            "Epoch 43/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2822 - binary_accuracy: 0.8849 - auc: 0.9432 - val_loss: 0.3056 - val_binary_accuracy: 0.9009 - val_auc: 0.9598\n",
            "Epoch 44/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2806 - binary_accuracy: 0.8859 - auc: 0.9448 - val_loss: 0.3855 - val_binary_accuracy: 0.9002 - val_auc: 0.9607\n",
            "Epoch 45/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2760 - binary_accuracy: 0.8856 - auc: 0.9475 - val_loss: 0.2821 - val_binary_accuracy: 0.9011 - val_auc: 0.9598\n",
            "Epoch 46/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2891 - binary_accuracy: 0.8795 - auc: 0.9422 - val_loss: 0.2818 - val_binary_accuracy: 0.8968 - val_auc: 0.9604\n",
            "Epoch 47/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2763 - binary_accuracy: 0.8864 - auc: 0.9461 - val_loss: 0.3332 - val_binary_accuracy: 0.8968 - val_auc: 0.9610\n",
            "Epoch 48/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2765 - binary_accuracy: 0.8838 - auc: 0.9464 - val_loss: 0.2871 - val_binary_accuracy: 0.9014 - val_auc: 0.9583\n",
            "Epoch 49/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2830 - binary_accuracy: 0.8828 - auc: 0.9446 - val_loss: 0.3084 - val_binary_accuracy: 0.9050 - val_auc: 0.9626\n",
            "Epoch 50/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2750 - binary_accuracy: 0.8903 - auc: 0.9462 - val_loss: 0.3062 - val_binary_accuracy: 0.9036 - val_auc: 0.9622\n",
            "Epoch 51/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2743 - binary_accuracy: 0.8871 - auc: 0.9474 - val_loss: 0.4849 - val_binary_accuracy: 0.8956 - val_auc: 0.9592\n",
            "Epoch 52/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2872 - binary_accuracy: 0.8861 - auc: 0.9434 - val_loss: 0.2693 - val_binary_accuracy: 0.8905 - val_auc: 0.9585\n",
            "Epoch 53/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2815 - binary_accuracy: 0.8865 - auc: 0.9444 - val_loss: 0.3142 - val_binary_accuracy: 0.8985 - val_auc: 0.9600\n",
            "Epoch 54/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2781 - binary_accuracy: 0.8856 - auc: 0.9462 - val_loss: 0.2503 - val_binary_accuracy: 0.9007 - val_auc: 0.9605\n",
            "Epoch 55/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2792 - binary_accuracy: 0.8855 - auc: 0.9461 - val_loss: 0.2845 - val_binary_accuracy: 0.8910 - val_auc: 0.9565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "pDzx6eli2yPV",
        "outputId": "2cc5983d-8a53-4a09-b941-0ff689d55f3e"
      },
      "source": [
        "history_df1 = pd.DataFrame(history1.history)\n",
        "\n",
        "history_df1.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.329842</td>\n",
              "      <td>0.855387</td>\n",
              "      <td>0.915682</td>\n",
              "      <td>0.351159</td>\n",
              "      <td>0.868943</td>\n",
              "      <td>0.934057</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.075952</td>\n",
              "      <td>0.051974</td>\n",
              "      <td>0.055611</td>\n",
              "      <td>0.081758</td>\n",
              "      <td>0.054248</td>\n",
              "      <td>0.051370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.274853</td>\n",
              "      <td>0.651174</td>\n",
              "      <td>0.694853</td>\n",
              "      <td>0.250324</td>\n",
              "      <td>0.694976</td>\n",
              "      <td>0.748510</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.286742</td>\n",
              "      <td>0.858893</td>\n",
              "      <td>0.922186</td>\n",
              "      <td>0.295454</td>\n",
              "      <td>0.878823</td>\n",
              "      <td>0.939191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.299963</td>\n",
              "      <td>0.876598</td>\n",
              "      <td>0.936873</td>\n",
              "      <td>0.314257</td>\n",
              "      <td>0.891858</td>\n",
              "      <td>0.954127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.331790</td>\n",
              "      <td>0.882722</td>\n",
              "      <td>0.942204</td>\n",
              "      <td>0.388238</td>\n",
              "      <td>0.899955</td>\n",
              "      <td>0.958403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.595459</td>\n",
              "      <td>0.887943</td>\n",
              "      <td>0.947103</td>\n",
              "      <td>0.583396</td>\n",
              "      <td>0.912012</td>\n",
              "      <td>0.962620</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  55.000000        55.000000  ...            55.000000  55.000000\n",
              "mean    0.329842         0.855387  ...             0.868943   0.934057\n",
              "std     0.075952         0.051974  ...             0.054248   0.051370\n",
              "min     0.274853         0.651174  ...             0.694976   0.748510\n",
              "25%     0.286742         0.858893  ...             0.878823   0.939191\n",
              "50%     0.299963         0.876598  ...             0.891858   0.954127\n",
              "75%     0.331790         0.882722  ...             0.899955   0.958403\n",
              "max     0.595459         0.887943  ...             0.912012   0.962620\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5vC7Dd72yPV",
        "outputId": "580f4bd9-07da-4537-f5df-de9dfb84bb1c"
      },
      "source": [
        "train_acc = top_10_nn.evaluate(train_X, train_y)\n",
        "test_acc = top_10_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 1ms/step - loss: 0.3448 - binary_accuracy: 0.9114 - auc: 0.9547\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3457 - binary_accuracy: 0.9107 - auc: 0.9543\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN12uM3b2yPW",
        "outputId": "2021ca06-0ac6-4d73-e424-e033ae3d4dc8"
      },
      "source": [
        "dict(zip(top_10_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.9543376564979553,\n",
              " 'binary_accuracy': 0.9106578826904297,\n",
              " 'loss': 0.3456972539424896}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "9BSxXa_h2yPW",
        "outputId": "277025e4-a253-4c29-b566-446ae6aa0b8b"
      },
      "source": [
        "history_df1.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df1['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.2503238916397095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "yNRlMCwX2yPW",
        "outputId": "e3fe588c-2c05-42b3-bcb7-46a7511e07d9"
      },
      "source": [
        "history_df1.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df1['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.9626195430755615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXxU9b3/8dcn+75vkBDCvslqRMAFhKK4a125aq2tpXpttXa11p9aW3ttbze9td7ayhWrlVoqChZ3wKWCEkBk38KSDbKRdTJJZub7++M7QAgJmYSEZCaf5+Mxj5k5c87M94Twnm8+53u+R4wxKKWUClxBvd0ApZRSPUuDXimlApwGvVJKBTgNeqWUCnAa9EopFeBCersBraWkpJicnJzeboZSSvmV9evXlxtjUtt6rc8FfU5ODnl5eb3dDKWU8isicqC917R0o5RSAU6DXimlApwGvVJKBTgNeqWUCnAa9EopFeA06JVSKsBp0CulVIDrc+PolVLqjCpYB/mrITQSwqIhPNbeh8VAxlkQmdjbLTxtGvRK9XeNdVC4Dgo+BXcTTPtPiE7pvfY0N4AxEBbl+zbGQGMNOCrAUWnvE7IhbUz72xw5AO89CltfbX+dkAgY92XIvQOyzgER39vUh2jQK+VPqotg/8cweAYkDOraezTVw5734MAncHANHNoCxg2IDbLP/gKzfgTnfANCwrq1+SeoK4Mdy+HIfqg66L0VQH2pfT0yEeKyID4T4rMgdgC4nN4w9wZ6fbl93FAJHtfJn5F+Foy/Hs66zgY/gLMGPv4trPkjSBDM/BFMv+f4z6apHhprwVkF25fDF6/Apr9B2jgb+BNuhIj49vfLGDiyD0o2QckXUHcYGqrs+x29R2DqnTD1m537Qusi6WtXmMrNzTU6BYJSLVTstYGzfTkUef9vhETYnvf590NEXMfv4XZB/iobWjv+Bc31EBIJWbmQPc3ess6BmhJ4+0HY+z4kj4B5/wUj5nbv/lQVwCf/AxteAFcDBIfZIE/Itrf4bAgKsl9qNUVQXWhvRwMyKgmiUiAq2fu45XPvLTIRSj63+1v4mf3c7On2C3L9InCUw8T5MPv/2S+SU2msgy1LIG+hDe+gUPulE5MK0WnH713O4+HeWG23DQqxr0UmQmQCRCTY+5pi++8Rkw4XfB/Ovh1Cwk/rxyoi640xuW2+pkGvVB/Q3GD/89eW2PuaIht0B9fA4S12nQGTYMyVMPg8GzqbX4HoVLjoQZj8FQhu9Qe6oxIOfWGDfcurNtwi4mHsNTD+BhvuwaEnt8UY2P2ODfyKPTB8Loy8BJod0Oy04dzcAAiMvcoGqC8ljfLd8PHv4YvF9vmEm21POnW0DXZffkbBYRAU3PG6LVXus0H9xT+gfKf9+V38c8ic0rn3ASjaANtet/9OdaVQX3b8FhRqa/oDJh6/pY1tP8AProX3H4MD/7ZfbrN+ZH8mrf8dfaRBrwJXcwMc3mp7byWb7M3VCJf/FnLO6+3WnVp9BWx7zYbwgX8Drf4vhsdD+lgb7qOvgMTBJ75etB7efggOfmLDcuoC+wVxaLMtx9QW2/WCw2HUPBh/o+2d+9pzdDXBZ8/CB7+09e+jgkLtgUt3k+3Fpo2zZYjxN0J4zPH1jIHKfNj3Aex+F3a+aT97yu0w49tdLz11lTG21BOd0v21do8HjKfzIW0M7F0JK38GxRshMxfufK9L7dOgV4GnrhT+fps9iGjcdllkou31Vh2wB9rmPAwz7vWtt9gRY2xN21kNsRneP93TjvcuPR6oKYTyXbbnWr7L/skfNxDiMr33A23IHPgENi+xf7p7XJAyEsZcBcnDjq8fO+DE0DxVu7Yvh/cesaEaFGLfL/0syBhve5iZZ5+6ptyRJoetW4dG2HLP0TBrqrf7se7P9sslPA4m/QdkTLDHEfZ9aH8mALEDYeJNMO0eW+pQJzIGdrxha/hTbuvSW5x20IvIPOBJIBj4izHmiVavDwYWAqlAJXCrMabQ+5ob2Oxd9aAx5qpTfZYGfT/WcMT2yIfMPHWPxtUIz19hw2X6PTBwsv0zOT7LbuesgWXftr3lkZfCtc90fYicMbYn+uF/Q/GGE1+TIG/9NcEeSGx2HH8tIsHWzmtKwNN88vvGZ8NZX7YHCTPGn34P09VkDwAm5px2rbfTjLEjdj77sy1reJohMgmGXABDLoQhs+yXmJ+OWPEXpxX0IhIM7ALmAoXAOmC+MWZbi3X+AbxhjFkkIrOBO4wxt3lfqzPG+NA1sTTo+6G6UljzNKx7DppqIfdrcNlv2u6JGwOv3Q2bXoYbFsG4a9p+T2Pg0z/BOw9B3AC77tGarKPS9rjLdtief9xAOwwvdQxEJ9t1PB47IuTD/7ZfKAmD4YLv2lCuPdTiVmK/oBIGQ8oI25tOGXm8PODx2FEhNUXHa/DpZ8GgqYEZfHVldtRM6pju+UtK+exUQe9LQWkqsMcYk+99s8XA1cC2FuuMBb7rfbwKeK3rzVV+yVEJa5+BoTPtwS5fQuzIAfjkKdj4oq33jr3GjphY92c7vO2aZ04+WPjvJ23Iz3qw/ZAH+/nT7rKjSl65HRZeYkeVlO8+PnzPrsgJtfHoVFvvri+zXwRJw2w7xt/Q9oHLjgQF2VJFTCoMnNT57f3N0X1VfYovQZ8JFLR4Xgic22qdTcCXseWda4FYEUk2xlQAESKSB7iAJ4wx+iXgL2qK7QG/MVeeej2PB179hh2b/eGvIHk4TL7N1mtj0k5cr2y7/TN/34ewbZktf0yaD+d9x/55D7aH/f5PbY37hudtbRhsCeW9R+0JLDN/6Ns+ZOXCXR/Bih/Y2v3IiyFlFKR6b/GDbC+7dIdt29H70Cj48l9seaWzozyU6mN8Kd1cD8wzxtzpfX4bcK4x5lst1hkI/AEYAnwIXAecZYypEpFMY0yRiAwFVgJzjDF7W33GAmABQHZ29tkHDrR7RSx1Jv3zTtj8D5j7Mzjv3vbXW/1LWP0LmPeEPei34QU7LDAoBEZdCunj7VjmgnXHxxdHp9le8vR72h7H/NmfYcX3IecCmP+yrYE/d7Etj3x1xRk5yUSpnuJsdhMeEoR0Y/nudGv004FHjTGXeJ//GMAY81/trB8D7DDGZLXx2vPYWv6S9j5Pa/R9hNsF/z3MHvh0NcA1/2t73q3teQ9evB4m3mxLHEd/cct2wYZFtsziqLA12+xzYdA0e584pOPyzqa/23r8wEm2lOJqggWrbI9f9QvGGDYVVrP4s4O8s+0wU7ITuHvWMM4enOTzezib3azbX8kHO8vYWFDF6IxYLhqVxozhyUSFtV/UaHZ7KKh0sL+invyyevZX1LO/3EFprZOxA+KYNjSZaUOTGZwcdcrANsawt6yevP2VfLa/krz9RzhY6SAsJIjk6DCSvLfk6DBGZcRx96xhnfoZHXW6QR+CPRg7ByjCHoz9D2PM1hbrpACVxhiPiDwOuI0xD4tIIuAwxjR611kDXN3yQG5rGvR9xP5/w/OXwXXP2R76/o9h/mJb+jiq6iD86UI7dO7O99ruZbub7Vh3X87ebMuOFfCPr9ovhTtW2KGCKuBVO5pZurGQxesK2HGolsjQYGaNSmVtfgVHHM1MzUnirllDuWhU2kkh62x2k19Wb8N9Vxlr9lbQ0OwmLDiIsQPj2HW4FkeTfX7u0CRmjkxlZHosBysd7CuvP3Y7WOnA7Tmej3ERIQxJjSE5OowvCqsor2sCID0unGlDkxmaEkNDsxtHkwtHk72vdbrYWlxDZb1dNzk6jNycRMYMiMPR5KaironK+kYq65uodDQxNCWGRV+b2qWfWXcMr7wM+D12eOVCY8zjIvIYkGeMWeYt7/wX9qjWh8A93nCfAfwJ8GCnRP69Mea5U32WBn0f8c5DsPZ/4Uf77AiWRVfYXvrty2HQObanv3CePXNywerj9fWeULLJfmFktfk7rPqQo3nSlZLEoWonH+wqZfXOMlbuKKXR5WF8Zjw3Tx3EVRMHEhsRiqPJxd/XFfCXj/ZRVNXA6IxYrp6UyeEaJ/nl9eSX1VFU1cDRWMtJjmLWqDRmjkzl3KFJRIWF0Ohyk7f/CKt3lrJqZxl7SuuOtSEiNIghKTEMTYkmJyWKISkxDEmJZkhKNIlRocf262gvfW1+BZ/uq2RtfgVltY2EBQcRFR5MdFgIkWHBRIcFMyI9lnNyEsnNSWJoSnS3lmta0hOmVOf94Rx74s5XvMfO60ptjdxZBV972w5dzHsObnqx44O1qkuMMXgMBAf1/DDMhiY3KzaXsKW4moTIMJKiQ0mMDiMpKoyEqDDio0KJCQ8hJjzkhPYcqW9iU2EVXxRW80VhFZ8XVFPT0ExWYiSDkqIYlBTJoMQoBiVFERMeQkiwEBIURHCQEBos1DpdfLi7jA92lrHjUC0AGXERXDwunRtzB3FWZtsnejW7PSzfVMwzq/eyu7SOqLBghqZGMzQlxt6nxjAhM56clOgO972g0kHBEQc5ydFkxEUQ1IWftzEGl8cQGtx7Q0o16JXlrIEt/7TDBwdPb3+9ir3wP1Pg0l/Bud88vrwyH567xA6FdFbZs04v/lnPt7sf+nh3OT95bTNH6pu4elKmN/TiOtUbrGt0sfHgEYyB0RmxpMaGn7T9tuIaFq87yNKNRdQ6XUSGBtPQ7D7l+0aFBR8L/JJqJ2Ara8NSY5iQFU9KTDiFRxwUVDZwsNJBdUMbJ4y1EBIk5OYkMmtUGrNGpTIqPdbn/fR4DJWOJpKjw3qsp+wvTnccvfJ35XvsnCWf/82ekBSfDfdubH9ejl1v2/uRl5y4PGko3LrEnpU6+HyY80jPtrsPc7k9bDhYxfs7DvP5wSqSY8LISoxiUGIkWYlRx3q0EaGdG5p5pL6Jn/9rO//cUHis7PBKXgF/XXuAMQPiuDE3i2smZZIYfeL0wcYYyuuajh3wW7e/km3FNbQoMZMYFcqojFhGZ8SRHhfBW1tK2FRYTVhIEJedlcHNU7M5d0gSLo+hytHMEUcTlfVNHKlvotbposbZTF2jizqni7pGF40uDyPTY5k4KJ7xmfHERrR9nkGNs5mCSgcNTW5cHoPbY2h2e3B7DCHBQZw9OJGY8K5FUVCQkBJzhs8E9kPaow9UHo+davbTP8Ged+1EVGd92Z61ufJncOMLMPbqtrdddKU9w/GetW2/Xl9hr8LTk3OVnwEut4eSaicDEyJ9Ko9UOZqO1Y8/2FVGdUMzIUHCuMx4ap3NFB5poMnlObb+0QOI887KYPbotHaDEGxQL9tUzGPLt1Hd0MyCC4dy75wRRIQGU93QzLJNxbyyroDNRdWEBAmRYcG4PQaX2+DyeE4I9PCQICZnJzA1J4lzhiQREhTEzkM17Dxcy45Dtew6VEt9k5tR6bHcPHUQ107OJCHKv/8tlZZu+p+aEjss8eh817lfh7O/CrHp4HHDU5Ns/f1rb528rbMafjUUpn8L5v70jDe9pxhjKK52sqmgis+9ty1F1Tia3KTHhXP5+IFcOXEAkwYlnFACqHI08c7Ww7yxuYR/7ynH7TGkxIQxa1Qas0encf6IFOK8Ae7xGMrrGik40kDhEQd5+4/w9tZDlHoP0p0/IoV54zJIjQ3H0eSmvslFQ5MbR5ObNfkVfLirjIlZ8Txx3QTGDGh7lNK24hr+tbmYhiYPIcFCkAghQUJwkBAbEcLk7ETGZ8YTFtJ+rVjLHYFJg74/2bECXr/HDmm8+Gd2StjWPe81T9u5xr+x6uQ5ube8CkvusAdcs6eduXZ3I0eTi12H69h5qIYdh2rZecj2ZI8OcTs6zG7SoASGpETz7z3lrN5ZRpPbw6CkSK6cMJDByVG8teUQH+0ux+UxZCdFcfmEAVwyLoMJmfE+H7DzeAwbDh7hzS2HeGvLIYqqGtpcLzY8hPvnjuT2GTln5OCrCjwa9P1BkwPe+Ym9IEXGeDv+PXVU2+s6q+G3Y2H05fDlZ0987dVv2otO/GCP35z67/EYthbXsHJHKSt3lvJFYdWx4XWRocGMzIhldHrssXAfMyDupB5vdUMz72w9xPIvjvfcMxMiuWLCAC6fMIDxmfGn3fs1xrDjUC3OZjdRYSFEhQV7byFEhHbvWZKq/9GDsYGuZJOdrqB8l72gw+z/d+qpaiPiYfKtdrbIuY/Z+dXBlnV2vwMjLu7zIW+MYdXOUt7acohVO8soq21EBCZmJfDti4YzdmA8ozNiyU6K8qn3HR8Zyg25g7ghdxAVdY2U1jYyOsP30R++EJF2SzJK9SQNen9XX25PXAqPg9uWwrDZvm03dYE9ULvuOZj9E7us4DN7keXWo226oNHlZsOBKv69p5yK+kayk6IZkhJFTko0g5OiiQzr+hdJXaOLB1/dzLJNxcRGhHDhyFRmj0pj5qjUbhmBkRwTTrKO5FABRIPe3+16y17w4o4V9gIcvkoeBiPn2ZOeLvienSFy11t2IrLhczrdDEeTi33l9azZW8FHu8v5bF8lDc1ugoOE+MjQY/XxowbER3De8BSunZzJtKHJPteldxyq4T9f2sD+8nq+N3ckd80a1qsnqSjlDzTo/d3ON+0ImgFdmOt82t3wwpt2hsopt9mgHzyj3cvOldc1srmomq1F1RRUNlBS4+RQdQOHqp3UOF3H1huaGs2NuVmcNzyFacOSiYsIpcbZzIFyB/sq6tlfXs/u0jre3nKIJesLSY8L56qJA7l6UibjBrZ/UtAreQU8/PoWYiNCefHOc5kxLKXz+6xUP6RB78+aG+yFhSf9R9euVjTkQnth50//F3LOtxfamHL7sZe3l9Tw7rbDbC6qZktR9bGzIAFSY8MZEB/B4ORopg1NJiM+gsyESM7JSWJgQuRJHxUXEcr4rHjGZx3/EnE2u3l/eymvfV7E85/s588f7SMnOYqR6ba2np1sT53PTIjkTx/k888NhUwfmsyT8yeRFhvR+f1Vqp/SoO9NRw7AqsftnOuTb+18WO/70JZtRl3atc8Xsb36Zd+yk5gBTcMu5s3Pi/jrmgPkHTgCwNCUaM7JSWJ8ZjxnZcYzLjPu2Njx0xERGszl3lEtVY4m/rW5hFU7ythfUc+Hu8twNh8/+UgE7p0zgvvmjNDhh0p1kg6v7A0eD6z7i71akqsBjAfGXQtXPtlu2aRNy++DzUvgh/ldvyB0sxN+NxYcFVRE5nBJ868pr2ticHIUt00bzHVTsk463f5MMMZQVtdIQaWDg5V2wqnJ2V28wLdS/YAOr+xLyvfYHvTBNTBsDlzxO9iyBFY+bi/bd91COw1wRzweW58fPqfrIQ/sr3azO/oK5joW8c/acUwakcBt03O4YHhKl2bx6y4iQlpsBGmxEZ26yIRS6mQa9GeK2wVrn4ZVv7DBfM0zMHG+rUlc8D1bvlnydXsR69kP2WuoBp1iNEnxRqg7DKMu71Jz9pfX84dVe1i6sYj0oOkMT9jMlVf/gAUjOjFyRynlFzToz5Tl98LnL8HoK+Dy3xw/SemoQVPtRayX32cvjL3vQ3ut1NCTD2wCsHMFSDCMmNupZuwrr+dpb8CHBAm3T8/hrplDSYtr4zKBSqmAoEF/Jhz4xIb8jHvtmajtHXSNTIAbnrdj2//1PVj7R9vbb8vONyF7OkR1XNZwuT28v6OUF9ce4KPd5YSHBLUIeB29olSg06DvaR43rPghxGXBrAc6HlkjAufcCXtWwke/hcm3QUzaiesc2Q+lW+Hix0/5VqU1ThavK+Dlzw5SUu0kIy6C+780kvnnDtLhiUr1Ixr0PS1vIRzebHvqYR1f1uyYuY/BH8+1wy+vfPLE13Z6pxduY1ilMYZP91Xy17UHeHvLIVwewwUjUnj0qnHMGZ1GiJ5FqlS/o0Hfk+orYOXP7YlJY6/p3LYpw+Gcb8Bnf7Lz0qSPO/7azhWQMuqEC3LXOptZutGOf99dWkd8ZChfnZHDLdMGM8SH62YqpQKXBn1PWvkYNNbaa6925czVmT+ETS/D2z+B25ZS0+hi74EiJu7/N7uG3c7HH+VT3dBMcZWTt7aUUN/kZnxmPL+6fgJXTRzY6cvYKaUCkwZ9TyneCOsX2TNP08Z07T2ikmDmj+DtH/P6kkU8tGUAs5o/5H/CXDy4dRAbtmwnSCAhKox5Zw3gK9MHM3FQQvfuh1LK72nQ9wSPxx6AjU6xB2C7yNns5m9Nc5jDAMZu/hXnD/8/HnLtx1WazO/vuZP4mAhiw0N69cQmpVTfp0HfE75YDIWfwdV/7NyUBl5uj+EfeQU8+f5uSqqdVGd9k/vLH+WZkRth1Ucw5kqyU2N7oOFKqUCkQd/dnDXw7iOQdY4987WTjDE8/PoWXvr0IJOzE/jNjROZMfQyWPS+vVSgx9X1ScyUUv2SjrXrbp88BfWl9gDsqaYwaMefP8rnpU8P8s0Lh/Lq3TPsnOsicMnjdkx+SAQMu6gHGq6UClTao+9OdaWw5o92JsrMKZ3e/M3NJfxixQ4uHz+AH80bfeIFOAZMtGfJepo7Nx5fKdXvadB3p49+Ay4nXPRQpzfdcPAI3/n750zxlmvaPMA65/91QyOVUv2NT7UFEZknIjtFZI+InDSMREQGi8j7IvKFiKwWkawWr90uIru9t9tbbxswqg7as2An32JPduqEgxUOvrEoj/S4CP78lVwd/66U6lYdBr2IBANPA5cCY4H5IjK21Wq/Bl4wxkwAHgP+y7ttEvAIcC4wFXhERALz6hGrnwAEZnZuOGW1o5k7nv8Ml8fwf3ecQ3JM1+eWV0qptvjSo58K7DHG5BtjmoDFwNWt1hkLrPQ+XtXi9UuAd40xlcaYI8C7wLzTb3YfU7rDnsE69RsQn+nzZs1uD998MY+Cygaeve1shqXG9GAjlVL9lS9BnwkUtHhe6F3W0ibgy97H1wKxIpLs47aIyAIRyRORvLKyMl/b3nes+jmERsP53+3UZj9dvpW1+ZX88vrxnDs0uYcap5Tq77preOX3gZkishGYCRQBbl83NsY8a4zJNcbkpqamdlOTzpCi9bB9Ocz4FkT7HtYvfXqAF9ce5Jszh3Lt5KyON1BKqS7yZdRNETCoxfMs77JjjDHFeHv0IhIDXGeMqRKRImBWq21Xn0Z7+573H4OoZJh+j8+bfLavkkde38rMkan88JLRPdg4pZTyrUe/DhghIkNEJAy4GVjWcgURSRGRo+/1Y2Ch9/HbwMUikug9CHuxd1lgyP8A8lfb8e3hvk1JUFTVwN0vric7KYqn5k8mWOepUUr1sA6D3hjjAr6FDejtwCvGmK0i8piIXOVdbRawU0R2AenA495tK4GfYb8s1gGPeZcFhpU/t1eOyv26T6s3NLlZ8EIeTS4Pz34ll/jI0B5uoFJK+XjClDFmBbCi1bKHWzxeAixpZ9uFHO/hB46STXbiskt/BaEdX5bPGMMP//kF20pqeO72XIan6QgbpdSZoXPddNX6RXbemQk3+rT6ok/2s3xTMT+4ZBSzR6f3cOOUUuo4DfquaKqHL16xlweM7Pj8L2MMz3+yn6k5Sdw9c1iH6yulVHfSoO+KrUuhqRbO9m1Ghy1FNeyvcHDd2ZknTlSmlFJngAZ9V6xfBCkjIXu6T6sv/6KY0GDhknEZPdwwpZQ6mQZ9Zx3eZg/CTrndpwt+ezyGNzYVc+GIVBKiws5AA5VS6kQa9J21YREEh/l89aj1B49QXO3kyokDe7hhSinVNg36zmh2wqbFMPoKn6c7WL6pmPCQIL40VkfaKKV6hwZ9Z2xfBs4qnw/CutweVmwuYc6YNGLC9RovSqneoUHfGesXQWIO5Fzo0+pr8yspr2viyglatlFK9R4Nel+V74EDH9uDsD5e9Hv5pmJiwkO4aHRaDzdOKaXap0Hvqw3PQ1AITLrFp9WbXB7e3FLCxWPT9dKASqlepUHvC1cTfP4yjJwHsb4dVP1odxk1TpeOtlFK9ToNel/s/Bc4yuHsr/q8yfJNxSREhXLe8JSea5dSSvlAg94XB9ZAWAwMm+3T6g1Nbt7ddphLz8ogLER/xEqp3qUp5AtHOUSnQpBvtfZVO0upb3LraBulVJ+gQe8LRwVE+16CWb6pmJSYcL3gt1KqT9Cg90V9BUT5FvS1zmZW7ijligkD9DKBSqk+QYPeF45yewFwH/x9XQGNLg9XTdKyjVKqb9Cg74gx3tJNx0Ff62zm6VV7uGBEClOyO74giVJKnQka9B1prAV3k0+lm798tI8jjmZ+cMmoM9AwpZTyjQZ9Rxzl9r6D0k1FXSN/+Sify8ZnMCEr4Qw0TCmlfKNB3xFHpb3vYNTN06v20tDs5rtztTevlOpbNOg7Un+0R99+0BdVNfDi2gNcf3YWw9NizlDDlFLKNxr0HTlWuklqd5Un39sFAvd9aeQZapRSSvlOg74jjgp7307pZk9pHUvWF3LbtMFkJkSewYYppZRvNOg7Ul8OweF2rps2/OadnUSGBvOfs4ad4YYppZRvNOg7cnT6Azn5LNcvCqt4c8sh7rxgKMkx4b3QOKWU6pgGfUfqy9utz//mnV0kRoVy5wVDznCjlFLKdz4FvYjME5GdIrJHRB5o4/VsEVklIhtF5AsRucy7PEdEGkTkc+/tf7t7B3qco+15bqobmvlodxm3ThtMbERoLzRMKaV8E9LRCiISDDwNzAUKgXUisswYs63Fag8BrxhjnhGRscAKIMf72l5jzKTubfYZ5CiHpJN77J/tq8Rj0AuLKKX6PF969FOBPcaYfGNME7AYuLrVOgaI8z6OB4q7r4m9rL6izbNi1+ytIDwkiMnZehasUqpv8yXoM4GCFs8LvctaehS4VUQKsb35b7d4bYi3pPOBiFzQ1geIyAIRyRORvLKyMt9b39NcjdBU22bpZk1+BWcPTiQ8RC/8rZTq27rrYOx84HljTBZwGfBXEQkCSoBsY8xk4LvA30QkrvXGxphnjTG5xpjc1NTUbmpSNzg2hv7EHv2R+ia2l9QwXS8sopTyA74EfREwqMXzLO+ylr4OvAJgjFkDRAApxphGY0yFd/l6YC/gP6ePtjP9waf77BfA9GEa9Eqpvs+XoF8HjBCRISISBtwMLGu1zkFgDoCIjMEGfZmIpHoP5iIiQ4ERQH53Nb7HHe3Rt6rRr9lbQWRosLRwVZ8AABULSURBVM5SqZTyCx2OujHGuETkW8DbQDCw0BizVUQeA/KMMcuA7wF/FpH7sQdmv2qMMSJyIfCYiDQDHuAuY0xlj+1Nd2tn+oM1+RXk5iQSFqKnISil+r4Ogx7AGLMCe5C15bKHWzzeBpzXxnb/BP55mm3sPW2UbsrrGtl1uI5rJrc+Hq2UUn2TdklPxVEBCEQeL9Gszbe9/BnDdPy8Uso/aNCfisM7/UHQ8SGUa/ZWEBMewlkDTxo8pJRSfZIG/anUl5804mZNfgVThyQREqw/OqWUf9C0OhVH5Qkjbg7XOMkvq9fx80opv6JBfyqO8hNOllqzV8fPK6X8jwb9qbQq3azZW0FcRAhjBmh9XinlPzTo2+PxQEPlCWPo1+RXcO7QZIKDTr4IiVJK9VUa9O1xVoHxHKvRF1U1cLDSofV5pZTf0aBvT6uTpbQ+r5TyVxr07XF4g957MHbN3goSo0IZlR7bi41SSqnO06BvT4sJzYwxrM2vYNrQZIK0Pq+U8jMa9O1pUbopqGygqKqBGVq2UUr5IQ369hwt3UQlsybfPtb6vFLKH2nQt8dRCWExEBrB5wXVxEeGMiw1prdbpZRSnaZB35768mNDK/PL6hiRFoOI1ueVUv5Hg749jvJjJ0vtLatnaGp0LzdIKaW6RoO+PY4KiEqh2tFMeV2jlm2UUn5Lg7499RUQlcze8joADXqllN/SoG+LMcdmrswvqwfQ0o1Sym9p0LelqR5cTohKYW9ZHaHBwqCkqN5ulVJKdYkGfVtanBW7t7SOwcnRhOoVpZRSfkrTqy3H5rmxPfphWrZRSvkxDfq21NsevSsiiYOVDobqgVillB/ToG+Lt3RT3BxNs9voiBullF/ToG+Lt3Szty4CQEs3Sim/pkHflvpyCAplV5V9qqUbpZQ/06Bvi6PCHogtryclJpz4yNDebpFSSnWZBn1bHN6zYsvqtWyjlPJ7PgW9iMwTkZ0iskdEHmjj9WwRWSUiG0XkCxG5rMVrP/Zut1NELunOxvcY78yVe8vqGJamZRullH/rMOhFJBh4GrgUGAvMF5GxrVZ7CHjFGDMZuBn4o3fbsd7n44B5wB+979e3OSpoDE+iytHM0BTt0Sul/JsvPfqpwB5jTL4xpglYDFzdah0DxHkfxwPF3sdXA4uNMY3GmH3AHu/79W2OcqqwFwHXHr1Syt/5EvSZQEGL54XeZS09CtwqIoXACuDbndgWEVkgInkikldWVuZj03uIuxmc1ZS6bcAP1xE3Sik/110HY+cDzxtjsoDLgL+KiM/vbYx51hiTa4zJTU1N7aYmdZGjEoDCpmjCQoIYmBDZu+1RSqnTFOLDOkXAoBbPs7zLWvo6tgaPMWaNiEQAKT5u27d4T5ba54hkaEo0wUF6+UCllH/zpde9DhghIkNEJAx7cHVZq3UOAnMARGQMEAGUede7WUTCRWQIMAL4rLsa3yPqbdDvqtWLgSulAkOHPXpjjEtEvgW8DQQDC40xW0XkMSDPGLMM+B7wZxG5H3tg9qvGGANsFZFXgG2AC7jHGOPuqZ3pFt4e/faacC7WMfRKqQDgS+kGY8wK7EHWlssebvF4G3BeO9s+Djx+Gm08s7w1+nJPrPbolVIBQc+Mbc1buqkiRoNeKRUQNOhbc5TjDInHTbBeJ1YpFRA06FtzVFAdFE9GXATR4T5VtpRSqk/ToG+tvpwKTwzD0rQ3r5QKDBr0rRhHBSXN0VqfV0oFDA36Vjz15Rx2x+hkZkqpgKFB35IxiKOSSuJ0MjOlVMDQoG/JWUWQcXHE6Bh6pVTg0KBvqXw3AHXBCWTERfRyY5RSqnvo+MGjaorhH3dQFZREUcK5BOlkZkqpAKE9eoCGKnjxenBWcX/oT0hKH9TxNkop5Sc06F2N8PdboXwnjdctYnXNAK3PK6UCSv8Oeo8Hln4T9n8E1zzDtsizMQaG64gbpVQA6b9Bbwy88xPYuhTmPgYTbmT1zjJEYPqw5N5unVJKdZv+G/Rr/2hv594NM+4FYOWOUqZkJ5IUHdbLjVNKqe7TP4O+ugje+ymMugwu+QWIUFrjZHNRNbNHp/V265RSqlv1z6D/8FdgPDDvCQiyP4JVO0sBNOiVUgGn/wV9xV7Y8FfIvQMSBx9b/P72UgbGRzA6I7YXG6eUUt2v/wX9ql9ASDhc8P1jixpdbj7eU87sMWmI6IlSSqnA0r+C/tBm2LIEzr0LYtOPLf40vxJHk1vLNkqpgNS/gn7l4xARD+fde+LiHaVEhAYxY1hKLzVMKaV6Tv8J+oOfwq434bz7IDLx2GJjDO/vOMx5w1KICA3uxQYqpVTP6B9Bbwy8/xhEp9qyTQt7y+ooqGzgIi3bKKUCVP8I+vxVcOBjuPAHEHbilaPe367DKpVSgS3wg/5obz4+G87+6kkvv7+jlDED4hiYEHnm26aUUmdA4Af9psVQvBFmPWCHVbZQ7Whm/YEjzB6d2kuNU0qpnhfYQX/gE1h+HwyaBhNvPunlD3aX4fYYZo9Ob2NjpZQKDIEb9KU74OWbISEb5r8MQSePqFm5/TBJ0WFMGpTQCw1USqkzw6egF5F5IrJTRPaIyANtvP47Efnce9slIlUtXnO3eG1Zdza+XTUl8NL1EBwOty6BqKSTVnF7DKt3lTFrZCrBetlApVQA6/CasSISDDwNzAUKgXUisswYs+3oOsaY+1us/21gcou3aDDGTOq+JnegsRb+dgM4KuGOFZCY0+ZqGw8eocrRzOwxOtpGKRXYfOnRTwX2GGPyjTFNwGLg6lOsPx94uTsa12nuZnjlK3B4G9z4Agxs//vl/R2lhAQJF4zQA7FKqcDmS9BnAgUtnhd6l51ERAYDQ4CVLRZHiEieiKwVkWva2W6Bd528srIyH5veijGw7F7YuxKufBJGfKndVRtdbpZvKuacnCTiI0O79nlKKeUnuvtg7M3AEmOMu8WywcaYXOA/gN+LyLDWGxljnjXG5BpjclNTu9jDrtgD216DWT+GKbedctXnPt5H4ZEG7p51UlOUUirgdFijB4qAQS2eZ3mXteVm4J6WC4wxRd77fBFZja3f7+10SzuSMgLu/qTdmvxRh2uc/GHlHr40Jp0LR2rZRikV+Hzp0a8DRojIEBEJw4b5SaNnRGQ0kAisabEsUUTCvY9TgPOAba237TZJQ6CD+eR/+dYOXG7DQ5eP6bFmKKVUX9Jhj94Y4xKRbwFvA8HAQmPMVhF5DMgzxhwN/ZuBxcYY02LzMcCfRMSD/VJ5ouVonTNt48EjvLqhiLtmDiMnJbrjDZRSKgDIibnc+3Jzc01eXl63v6/HY7j2mU8oqWpg5fdnERPuS9VKKaX8g4is9x4PPUngnhnbyqsbi9hUUMWP5o3WkFdK9Sv9IujrGl388q0dTByUwLWT2xwZqpRSAatfdG3/sHIPZbWNPHvb2QTpdAdKqX4m4Hv0ByrqWfjxPq6bksXk7MSON1BKqQAT8EH/6oYimj0efjhvVG83RSmlekXAB/22khqGpESTHhfR201RSqleEfA1+u0lNUzU+eaV8gvNzc0UFhbidDp7uyl9VkREBFlZWYSG+j5PV0AHfY2zmcIjDcyfmt3bTVFK+aCwsJDY2FhycnKQDs5y74+MMVRUVFBYWMiQIUN83i6gSzc7SmoBGDMgtpdbopTyhdPpJDk5WUO+HSJCcnJyp//iCeig315SA8CYAXG93BKllK805E+tKz+fgA/6xKhQMvRArFKqHwv4oB8zIE57CEqpfi1gg97l9rDjUK2WbZRS/V7AjrrZX1FPo8ujQa+Un/rp8q1sK67p1vccOzCOR64c1+F611xzDQUFBTidTu677z4WLFhATEwMdXV1ACxZsoQ33niD559/nsOHD3PXXXeRn58PwDPPPMOMGTO6td2nK2CDfpuOuFFKddHChQtJSkqioaGBc845h+uuu67dde+9915mzpzJ0qVLcbvdx74M+pKADfrtJTWEBAnD02J6uylKqS7wpefdU5566imWLl0KQEFBAbt372533ZUrV/LCCy8AEBwcTHx8/BlpY2cEdNAPT4shPCS4t5uilPIjq1ev5r333mPNmjVERUUxa9YsnE7nCYM6/O3M3YA9GHt0xI1SSnVGdXU1iYmJREVFsWPHDtauXQtAeno627dvx+PxHOvtA8yZM4dnnnkGALfbTXV1da+0+1QCMugr65s4XNOo9XmlVKfNmzcPl8vFmDFjeOCBB5g2bRoATzzxBFdccQUzZsxgwIABx9Z/8sknWbVqFePHj+fss89m27Zeuyx2uwKydHP0jNixA/perUwp1beFh4fz5ptvtvna9ddff9Ky9PR0Xn/99Z5u1mkJyB798akPtEevlFIBGfTbSmpIiw0nOSa8t5uilFK9LjCDvlgPxCql1FEBF/RNLg97y+o06JVSyivggn5PaR3NbqP1eaWU8gq4oD8+4kZ79EopBQEa9GEhQQxJie7tpiilVJ8QeEF/qIbRGbGEBAfcriml+qCYmL4/n5ZPJ0yJyDzgSSAY+Isx5olWr/8OuMj7NApIM8YkeF+7HXjI+9rPjTGLuqPhbTHGsL2klrlj0nvqI5RSZ8qbD8Chzd37nhnj4dInOl4vwHTY7RWRYOBp4FJgLDBfRMa2XMcYc78xZpIxZhLwP8Cr3m2TgEeAc4GpwCMikti9u3BcaW0jlfVNeiBWKdVlDzzwAE8//fSx548++ig///nPmTNnDlOmTGH8+PE+nwlbV1fX5nb79+/nrLPOOrber3/9ax599FEA9uzZw5e+9CUmTpzIlClT2Lt372nvky89+qnAHmNMPoCILAauBtqb0GE+NtwBLgHeNcZUerd9F5gHvHw6jW7PNr0YuFKBo5d63jfddBPf+c53uOeeewB45ZVXePvtt7n33nuJi4ujvLycadOmcdVVV3V4mdKIiAiWLl160nancsstt/DAAw9w7bXX4nQ68Xg8p71PvgR9JlDQ4nkhtod+EhEZDAwBVp5i28w2tlsALADIzs72oUltO3o1mtEa9EqpLpo8eTKlpaUUFxdTVlZGYmIiGRkZ3H///Xz44YcEBQVRVFTE4cOHycjIOOV7GWN48MEHT9quPbW1tRQVFXHttdcC9ouiO3T3pGY3A0uMMe7ObGSMeRZ4FiA3N9d09cO3l9SQmRBJfGRoV99CKaW44YYbWLJkCYcOHeKmm27ipZdeoqysjPXr1xMaGkpOTo5Pc9K3t11ISMgJPfWent/el6EpRcCgFs+zvMvacjMnlmU6s+1p0znolVLd4aabbmLx4sUsWbKEG264gerqatLS0ggNDWXVqlUcOHDAp/dpb7v09HRKS0upqKigsbGRN954A4DY2FiysrJ47bXXAGhsbMThcJz2/vgS9OuAESIyRETCsGG+rPVKIjIaSATWtFj8NnCxiCR6D8Je7F3W7ZzNbvaV1zNWD8QqpU7TuHHjqK2tJTMzkwEDBnDLLbeQl5fH+PHjeeGFFxg9erRP79PedqGhoTz88MNMnTqVuXPnnvB+f/3rX3nqqaeYMGECM2bM4NChQ6e9P2JMx5USEbkM+D12eOVCY8zjIvIYkGeMWeZd51EgwhjzQKttvwY86H36uDHm/071Wbm5uSYvL6/TO1JW28jP3tjGjbmDOH9ESqe3V0r1vu3btzNmzJjebkaf19bPSUTWG2Ny21rfpxq9MWYFsKLVsodbPX+0nW0XAgt9+ZzTkRobzlPzJ/f0xyillN8JyCtMKaXUmbR582Zuu+22E5aFh4fz6aef9lKLTqRBr5TqU4wxHY5P72vGjx/P559/fkY+y5dye2s6IYxSqs+IiIigoqKiS2HWHxhjqKio6PT4eu3RK6X6jKysLAoLCykrK+vtpvRZERERZGVldWobDXqlVJ8RGhrKkCFDersZAUdLN0opFeA06JVSKsBp0CulVIDz6czYM0lEygDfJpJoWwpQ3k3N6Yt0//xfoO+j7l/vGGyMSW3rhT4X9KdLRPLaOw04EOj++b9A30fdv75HSzdKKRXgNOiVUirABWLQP9vbDehhun/+L9D3Ufevjwm4Gr1SSqkTBWKPXimlVAsa9EopFeACJuhFZJ6I7BSRPSLyQMdb9H0islBESkVkS4tlSSLyrojs9t4n9mYbT4eIDBKRVSKyTUS2ish93uUBsY8iEiEin4nIJu/+/dS7fIiIfOr9Xf279xKdfktEgkVko4i84X0eaPu3X0Q2i8jnIpLnXeZXv6MBEfQiEgw8DVwKjAXmi8jY3m1Vt3gemNdq2QPA+8aYEcD73uf+ygV8zxgzFpgG3OP9dwuUfWwEZhtjJgKTgHkiMg34JfA7Y8xw4Ajw9V5sY3e4D9je4nmg7R/ARcaYSS3Gz/vV72hABD0wFdhjjMk3xjQBi4Gre7lNp80Y8yFQ2Wrx1cAi7+NFwDVntFHdyBhTYozZ4H1ciw2LTAJkH41V530a6r0ZYDawxLvcb/cPQESygMuBv3ifCwG0f6fgV7+jgRL0mUBBi+eF3mWBKN0YU+J9fAhI783GdBcRyQEmA58SQPvoLWt8DpQC7wJ7gSpjjMu7ir//rv4e+CHg8T5PJrD2D+yX8zsisl5EFniX+dXvqM5H78eMMUZE/H58rIjEAP8EvmOMqWl5GTl/30djjBuYJCIJwFJgdC83qduIyBVAqTFmvYjM6u329KDzjTFFIpIGvCsiO1q+6A+/o4HSoy8CBrV4nuVdFogOi8gAAO99aS+357SISCg25F8yxrzqXRxQ+whgjKkCVgHTgQQROdrJ8uff1fOAq0RkP7ZcOht4ksDZPwCMMUXe+1Lsl/VU/Ox3NFCCfh0wwnu0Pwy4GVjWy23qKcuA272Pbwde78W2nBZvPfc5YLsx5rctXgqIfRSRVG9PHhGJBOZij0OsAq73rua3+2eM+bExJssYk4P9P7fSGHMLAbJ/ACISLSKxRx8DFwNb8LPf0YA5M1ZELsPWC4OBhcaYx3u5SadNRF4GZmGnRT0MPAK8BrwCZGOnc77RGNP6gK1fEJHzgY+AzRyv8T6IrdP7/T6KyATsgbpgbKfqFWPMYyIyFNsDTgI2ArcaYxp7r6Wnz1u6+b4x5opA2j/vviz1Pg0B/maMeVxEkvGj39GACXqllFJtC5TSjVJKqXZo0CulVIDToFdKqQCnQa+UUgFOg14ppQKcBr1SSgU4DXqllApw/x/vKBhFDMBp6wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "BtfRZSPn2yPW",
        "outputId": "670ea00d-6d6f-4d5e-a755-7133bef5253f"
      },
      "source": [
        "history_df1.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df1['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.9120124578475952\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI6H-m4m2yPX",
        "outputId": "ea0d2261-bbeb-4760-87f5-07a9430e6340"
      },
      "source": [
        "history_df1.binary_accuracy.max() - history_df1.val_binary_accuracy.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.024069011211395264"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orPls6y32yPX",
        "outputId": "97ff9319-46df-4636-b32f-60c5cdeaf321"
      },
      "source": [
        "preds1 = top_10_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.8667137 ],\n",
              "       [0.09227356],\n",
              "       [0.60327375],\n",
              "       ...,\n",
              "       [0.47996977],\n",
              "       [0.15259656],\n",
              "       [0.8667137 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 215
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8TY9mCm2yPX",
        "outputId": "17a84eb2-0593-4537-c4e1-e7c8e59745e0"
      },
      "source": [
        "len(preds1[preds1 <= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13617"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25tPkguC2yPX",
        "outputId": "a744c237-cb86-4afd-e95e-b3803ee9bb76"
      },
      "source": [
        "len(preds1[preds1 > 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8545"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "puz8iPfG2yPX",
        "outputId": "e163a86c-bd3a-4bd3-ebce-03f9bb1ffb58"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 218
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "NZt5yDHS2yPY",
        "outputId": "7f43cb3f-d826-4a6e-ac78-ccdf1783ae4b"
      },
      "source": [
        "preds_df = pd.DataFrame(preds1, columns = ['preds'])\n",
        "\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.866714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.092274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.603274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.216972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.806232</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      preds\n",
              "0  0.866714\n",
              "1  0.092274\n",
              "2  0.603274\n",
              "3  0.216972\n",
              "4  0.806232"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 219
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "pOwNb0Uw2yPY",
        "outputId": "489e05bc-6c37-419d-c50c-7155d2b887da"
      },
      "source": [
        "preds_df = pd.concat([preds_df, val_y.reset_index(drop=True), val_X.reset_index()], axis=1)\n",
        "\n",
        "preds_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>phishing</th>\n",
              "      <th>index</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.866714</td>\n",
              "      <td>1</td>\n",
              "      <td>62575</td>\n",
              "      <td>0.002483</td>\n",
              "      <td>0.025323</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.002483</td>\n",
              "      <td>0.016882</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004965</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.092274</td>\n",
              "      <td>0</td>\n",
              "      <td>38126</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.949305</td>\n",
              "      <td>0.314353</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.603274</td>\n",
              "      <td>0</td>\n",
              "      <td>1617</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.002349</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.921329</td>\n",
              "      <td>0.388775</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.216972</td>\n",
              "      <td>0</td>\n",
              "      <td>8228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.992151</td>\n",
              "      <td>0.125046</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.806232</td>\n",
              "      <td>1</td>\n",
              "      <td>55594</td>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.060606</td>\n",
              "      <td>0.424242</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.303030</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22157</th>\n",
              "      <td>0.048747</td>\n",
              "      <td>0</td>\n",
              "      <td>65294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001558</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.720224</td>\n",
              "      <td>0.693739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22158</th>\n",
              "      <td>0.067605</td>\n",
              "      <td>0</td>\n",
              "      <td>10038</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001086</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.904747</td>\n",
              "      <td>0.425947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22159</th>\n",
              "      <td>0.479970</td>\n",
              "      <td>0</td>\n",
              "      <td>43642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22160</th>\n",
              "      <td>0.152597</td>\n",
              "      <td>0</td>\n",
              "      <td>73632</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000730</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.980816</td>\n",
              "      <td>0.194933</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22161</th>\n",
              "      <td>0.866714</td>\n",
              "      <td>1</td>\n",
              "      <td>25895</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000686</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.998568</td>\n",
              "      <td>0.053500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22162 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          preds  phishing  index  ...  file_length    asn_ip  time_domain_activation\n",
              "0      0.866714         1  62575  ...     0.004965  0.000000                0.999518\n",
              "1      0.092274         0  38126  ...     0.000000  0.949305                0.314353\n",
              "2      0.603274         0   1617  ...     0.000967  0.921329                0.388775\n",
              "3      0.216972         0   8228  ...     0.000000  0.992151                0.125046\n",
              "4      0.806232         1  55594  ...     0.303030  0.000000                0.000000\n",
              "...         ...       ...    ...  ...          ...       ...                     ...\n",
              "22157  0.048747         0  65294  ...     0.000000  0.720224                0.693739\n",
              "22158  0.067605         0  10038  ...     0.000000  0.904747                0.425947\n",
              "22159  0.479970         0  43642  ...     0.000000  0.999999                0.000000\n",
              "22160  0.152597         0  73632  ...     0.000000  0.980816                0.194933\n",
              "22161  0.866714         1  25895  ...     0.000236  0.998568                0.053500\n",
              "\n",
              "[22162 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTrYk0rB2yPY",
        "outputId": "04f3b463-9300-43ac-e233-c4c5f4497863"
      },
      "source": [
        "pred_classes = np.argmax(preds1, axis=1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(val_y, pred_classes)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14519     0]\n",
            " [ 7643     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrn-vv5GfHiG"
      },
      "source": [
        "# neural network on top 10 most important features per recursive feature elimination package [model as feature]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "iZkt9Klrklue",
        "outputId": "43ffb750-ebdd-44a8-bb41-9b363565c216"
      },
      "source": [
        "y = full_df.iloc[:,-1]\n",
        "\n",
        "features = ['qty_slash_url', 'length_url', 'qty_hyphen_directory', 'qty_underline_directory', 'qty_slash_directory', 'directory_length', 'qty_hyphen_file', 'file_length', 'asn_ip', 'time_domain_activation']\n",
        "X = full_df[features]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.25, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004613</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999989</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000984</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.932855</td>\n",
              "      <td>0.360252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001769</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.956567</td>\n",
              "      <td>0.291508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000921</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.975658</td>\n",
              "      <td>0.219294</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_slash_url  length_url  ...    asn_ip  time_domain_activation\n",
              "5676        0.000004    0.000042  ...  1.000000                0.000591\n",
              "39002       0.000000    0.004613  ...  0.999989                0.000000\n",
              "1732        0.000000    0.000984  ...  0.932855                0.360252\n",
              "39668       0.000000    0.001769  ...  0.956567                0.291508\n",
              "82035       0.000000    0.000921  ...  0.975658                0.219294\n",
              "\n",
              "[5 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 222
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc0sZh1UfEpG"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_top10():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[10]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmzd8pHhkt-d"
      },
      "source": [
        "mod_top10 = KerasClassifier(build_fn=phish_nn_top10,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqoGLvQIkt-k",
        "outputId": "c625b0e1-be14-44c9-a371-55dee6ac8224"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_top10 = cross_val_score(mod_top10,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6247 - accuracy: 0.6463 - val_loss: 0.5855 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5634 - accuracy: 0.6939 - val_loss: 0.5744 - val_accuracy: 0.7128\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5475 - accuracy: 0.7073 - val_loss: 0.5806 - val_accuracy: 0.7096\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5447 - accuracy: 0.7114 - val_loss: 0.5398 - val_accuracy: 0.7284\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5104 - accuracy: 0.7307 - val_loss: 0.4619 - val_accuracy: 0.7722\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4723 - accuracy: 0.7588 - val_loss: 0.4395 - val_accuracy: 0.8025\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4321 - accuracy: 0.7926 - val_loss: 0.4184 - val_accuracy: 0.8101\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4103 - accuracy: 0.8082 - val_loss: 0.4547 - val_accuracy: 0.8077\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3951 - accuracy: 0.8164 - val_loss: 0.3647 - val_accuracy: 0.8552\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3820 - accuracy: 0.8256 - val_loss: 0.3842 - val_accuracy: 0.8293\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3715 - accuracy: 0.8334 - val_loss: 0.4933 - val_accuracy: 0.7910\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3626 - accuracy: 0.8375 - val_loss: 0.4271 - val_accuracy: 0.7946\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3477 - accuracy: 0.8492 - val_loss: 0.3526 - val_accuracy: 0.8297\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8585 - val_loss: 0.3156 - val_accuracy: 0.8710\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3291 - accuracy: 0.8563 - val_loss: 0.3809 - val_accuracy: 0.8800\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3289 - accuracy: 0.8575 - val_loss: 0.3422 - val_accuracy: 0.8655\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3190 - accuracy: 0.8622 - val_loss: 0.3135 - val_accuracy: 0.8878\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3256 - accuracy: 0.8595 - val_loss: 0.3451 - val_accuracy: 0.8790\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3190 - accuracy: 0.8649 - val_loss: 0.3249 - val_accuracy: 0.8867\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3148 - accuracy: 0.8660 - val_loss: 0.3633 - val_accuracy: 0.8771\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3097 - accuracy: 0.8696 - val_loss: 0.3692 - val_accuracy: 0.8797\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3040 - accuracy: 0.8732 - val_loss: 0.2979 - val_accuracy: 0.8901\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3114 - accuracy: 0.8671 - val_loss: 0.3372 - val_accuracy: 0.8829\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8694 - val_loss: 0.3282 - val_accuracy: 0.8902\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3140 - accuracy: 0.8636 - val_loss: 0.3401 - val_accuracy: 0.8760\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3035 - accuracy: 0.8694 - val_loss: 0.3472 - val_accuracy: 0.8890\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8734 - val_loss: 0.3367 - val_accuracy: 0.8979\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8697 - val_loss: 0.3526 - val_accuracy: 0.8800\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3050 - accuracy: 0.8693 - val_loss: 0.3236 - val_accuracy: 0.8905\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3033 - accuracy: 0.8720 - val_loss: 0.2937 - val_accuracy: 0.8784\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2945 - accuracy: 0.8746 - val_loss: 0.3063 - val_accuracy: 0.8848\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2937 - accuracy: 0.8730 - val_loss: 0.3630 - val_accuracy: 0.8931\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2994 - accuracy: 0.8764 - val_loss: 0.3135 - val_accuracy: 0.8808\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2950 - accuracy: 0.8766 - val_loss: 0.2909 - val_accuracy: 0.8878\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2956 - accuracy: 0.8736 - val_loss: 0.3298 - val_accuracy: 0.8972\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2916 - accuracy: 0.8784 - val_loss: 0.2758 - val_accuracy: 0.8981\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2890 - accuracy: 0.8779 - val_loss: 0.2775 - val_accuracy: 0.8910\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2952 - accuracy: 0.8757 - val_loss: 0.3422 - val_accuracy: 0.8957\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2860 - accuracy: 0.8808 - val_loss: 0.2610 - val_accuracy: 0.8949\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2869 - accuracy: 0.8812 - val_loss: 0.2893 - val_accuracy: 0.8894\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8807 - val_loss: 0.3227 - val_accuracy: 0.9001\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2898 - accuracy: 0.8759 - val_loss: 0.2903 - val_accuracy: 0.8923\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2940 - accuracy: 0.8774 - val_loss: 0.2965 - val_accuracy: 0.8973\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2867 - accuracy: 0.8801 - val_loss: 0.2602 - val_accuracy: 0.9012\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2855 - accuracy: 0.8812 - val_loss: 0.2729 - val_accuracy: 0.8963\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2927 - accuracy: 0.8775 - val_loss: 0.2977 - val_accuracy: 0.9078\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8807 - val_loss: 0.2643 - val_accuracy: 0.9024\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2799 - accuracy: 0.8857 - val_loss: 0.2581 - val_accuracy: 0.8949\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2835 - accuracy: 0.8851 - val_loss: 0.3445 - val_accuracy: 0.8143\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2898 - accuracy: 0.8799 - val_loss: 0.2642 - val_accuracy: 0.8990\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2659 - accuracy: 0.8984\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2819 - accuracy: 0.8823\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2809 - accuracy: 0.8837\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2825 - accuracy: 0.8825\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2769 - accuracy: 0.8861\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2776 - accuracy: 0.8856\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2775 - accuracy: 0.8857\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2856 - accuracy: 0.8778\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2842 - accuracy: 0.8789\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2789 - accuracy: 0.8838\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2722 - accuracy: 0.8879\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2590 - accuracy: 0.9130\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6276 - accuracy: 0.6486 - val_loss: 0.5712 - val_accuracy: 0.7068\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5647 - accuracy: 0.6851 - val_loss: 0.5827 - val_accuracy: 0.7156\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5528 - accuracy: 0.7088 - val_loss: 0.5460 - val_accuracy: 0.7295\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5161 - accuracy: 0.7321 - val_loss: 0.4641 - val_accuracy: 0.7797\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4470 - accuracy: 0.7825 - val_loss: 0.4578 - val_accuracy: 0.7908\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4227 - accuracy: 0.7989 - val_loss: 0.4587 - val_accuracy: 0.8181\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4041 - accuracy: 0.8122 - val_loss: 0.4137 - val_accuracy: 0.8130\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3846 - accuracy: 0.8247 - val_loss: 0.4325 - val_accuracy: 0.8299\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3772 - accuracy: 0.8338 - val_loss: 0.4210 - val_accuracy: 0.8203\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3669 - accuracy: 0.8387 - val_loss: 0.3654 - val_accuracy: 0.8528\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3606 - accuracy: 0.8426 - val_loss: 0.3588 - val_accuracy: 0.8683\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3516 - accuracy: 0.8460 - val_loss: 0.3588 - val_accuracy: 0.8674\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3382 - accuracy: 0.8561 - val_loss: 0.3637 - val_accuracy: 0.8797\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3337 - accuracy: 0.8622 - val_loss: 0.3975 - val_accuracy: 0.8937\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3227 - accuracy: 0.8671 - val_loss: 0.3176 - val_accuracy: 0.8951\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3209 - accuracy: 0.8662 - val_loss: 0.2910 - val_accuracy: 0.8864\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8683 - val_loss: 0.3374 - val_accuracy: 0.9078\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3199 - accuracy: 0.8689 - val_loss: 0.2899 - val_accuracy: 0.8892\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3150 - accuracy: 0.8685 - val_loss: 0.3799 - val_accuracy: 0.9023\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3075 - accuracy: 0.8737 - val_loss: 0.3688 - val_accuracy: 0.8814\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3077 - accuracy: 0.8761 - val_loss: 0.3877 - val_accuracy: 0.8922\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3005 - accuracy: 0.8778 - val_loss: 0.2937 - val_accuracy: 0.8701\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3011 - accuracy: 0.8746 - val_loss: 0.3010 - val_accuracy: 0.8938\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3030 - accuracy: 0.8750 - val_loss: 0.3033 - val_accuracy: 0.8992\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2957 - accuracy: 0.8794 - val_loss: 0.2934 - val_accuracy: 0.9032\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2980 - accuracy: 0.8793 - val_loss: 0.2800 - val_accuracy: 0.8992\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2949 - accuracy: 0.8815 - val_loss: 0.3201 - val_accuracy: 0.8956\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2927 - accuracy: 0.8808 - val_loss: 0.2923 - val_accuracy: 0.9015\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8863 - val_loss: 0.2814 - val_accuracy: 0.8961\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2860 - accuracy: 0.8830 - val_loss: 0.3908 - val_accuracy: 0.7989\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2928 - accuracy: 0.8811 - val_loss: 0.2894 - val_accuracy: 0.8930\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8847 - val_loss: 0.2861 - val_accuracy: 0.8984\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3368 - accuracy: 0.9070\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3172 - accuracy: 0.8695\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3104 - accuracy: 0.8719\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3090 - accuracy: 0.8725\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3031 - accuracy: 0.8744\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3016 - accuracy: 0.8764\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3036 - accuracy: 0.8742\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3007 - accuracy: 0.8770\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2970 - accuracy: 0.8773\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2948 - accuracy: 0.8780\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2935 - accuracy: 0.8791\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.3071 - accuracy: 0.8888\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6236 - accuracy: 0.6500 - val_loss: 0.5808 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5642 - accuracy: 0.6923 - val_loss: 0.5696 - val_accuracy: 0.7090\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5537 - accuracy: 0.7097 - val_loss: 0.5531 - val_accuracy: 0.7169\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5359 - accuracy: 0.7167 - val_loss: 0.4879 - val_accuracy: 0.7386\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4759 - accuracy: 0.7564 - val_loss: 0.4527 - val_accuracy: 0.8004\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4283 - accuracy: 0.7925 - val_loss: 0.4528 - val_accuracy: 0.8042\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4143 - accuracy: 0.8027 - val_loss: 0.4312 - val_accuracy: 0.8071\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3987 - accuracy: 0.8184 - val_loss: 0.4053 - val_accuracy: 0.8292\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3909 - accuracy: 0.8224 - val_loss: 0.4015 - val_accuracy: 0.8242\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3794 - accuracy: 0.8281 - val_loss: 0.4029 - val_accuracy: 0.8582\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3662 - accuracy: 0.8360 - val_loss: 0.3619 - val_accuracy: 0.8582\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3575 - accuracy: 0.8412 - val_loss: 0.4170 - val_accuracy: 0.8364\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3509 - accuracy: 0.8470 - val_loss: 0.3846 - val_accuracy: 0.8566\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3415 - accuracy: 0.8523 - val_loss: 0.3498 - val_accuracy: 0.8623\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3442 - accuracy: 0.8515 - val_loss: 0.4763 - val_accuracy: 0.8471\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3462 - accuracy: 0.8527 - val_loss: 0.3612 - val_accuracy: 0.8741\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3298 - accuracy: 0.8604 - val_loss: 0.3331 - val_accuracy: 0.8773\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3299 - accuracy: 0.8607 - val_loss: 0.3980 - val_accuracy: 0.9000\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3165 - accuracy: 0.8682 - val_loss: 0.2927 - val_accuracy: 0.8877\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3145 - accuracy: 0.8697 - val_loss: 0.2819 - val_accuracy: 0.9018\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3131 - accuracy: 0.8702 - val_loss: 0.3117 - val_accuracy: 0.9018\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3142 - accuracy: 0.8699 - val_loss: 0.3970 - val_accuracy: 0.9023\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3022 - accuracy: 0.8775 - val_loss: 0.3213 - val_accuracy: 0.9075\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3033 - accuracy: 0.8736 - val_loss: 0.2812 - val_accuracy: 0.8933\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3092 - accuracy: 0.8680 - val_loss: 0.4052 - val_accuracy: 0.8994\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2986 - accuracy: 0.8760 - val_loss: 0.2973 - val_accuracy: 0.8873\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3014 - accuracy: 0.8742 - val_loss: 0.2907 - val_accuracy: 0.8838\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2950 - accuracy: 0.8785 - val_loss: 0.4565 - val_accuracy: 0.8995\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2899 - accuracy: 0.8832 - val_loss: 0.3421 - val_accuracy: 0.9001\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2957 - accuracy: 0.8775 - val_loss: 0.3152 - val_accuracy: 0.8978\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2879 - accuracy: 0.8838 - val_loss: 0.3143 - val_accuracy: 0.8985\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3034 - accuracy: 0.8770 - val_loss: 0.2944 - val_accuracy: 0.8989\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2913 - accuracy: 0.8821 - val_loss: 0.2695 - val_accuracy: 0.8920\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2943 - accuracy: 0.8796 - val_loss: 0.2581 - val_accuracy: 0.8984\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2802 - accuracy: 0.8838 - val_loss: 0.3020 - val_accuracy: 0.9003\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2918 - accuracy: 0.8804 - val_loss: 0.3832 - val_accuracy: 0.8922\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3078 - accuracy: 0.8709 - val_loss: 0.3015 - val_accuracy: 0.8999\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3184 - accuracy: 0.8726 - val_loss: 0.3478 - val_accuracy: 0.8800\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3222 - accuracy: 0.9048\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3087 - accuracy: 0.8720\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.3026 - accuracy: 0.8755\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2999 - accuracy: 0.8774\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2994 - accuracy: 0.8786\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2951 - accuracy: 0.8795\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2925 - accuracy: 0.8802\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2958 - accuracy: 0.8796\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3048 - accuracy: 0.8709\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3007 - accuracy: 0.8730\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3017 - accuracy: 0.8726\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2806 - accuracy: 0.8854\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6331 - accuracy: 0.6423 - val_loss: 0.5656 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5640 - accuracy: 0.6818 - val_loss: 0.6013 - val_accuracy: 0.7123\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5528 - accuracy: 0.7085 - val_loss: 0.5620 - val_accuracy: 0.7160\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5401 - accuracy: 0.7111 - val_loss: 0.4845 - val_accuracy: 0.7462\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4792 - accuracy: 0.7534 - val_loss: 0.4290 - val_accuracy: 0.8077\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4398 - accuracy: 0.7875 - val_loss: 0.4109 - val_accuracy: 0.8014\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4365 - accuracy: 0.7908 - val_loss: 0.4143 - val_accuracy: 0.8132\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4019 - accuracy: 0.8109 - val_loss: 0.4093 - val_accuracy: 0.8143\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4004 - accuracy: 0.8117 - val_loss: 0.4279 - val_accuracy: 0.8490\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3915 - accuracy: 0.8192 - val_loss: 0.3950 - val_accuracy: 0.8532\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3787 - accuracy: 0.8288 - val_loss: 0.3524 - val_accuracy: 0.8592\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3667 - accuracy: 0.8354 - val_loss: 0.3748 - val_accuracy: 0.8727\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3543 - accuracy: 0.8438 - val_loss: 0.3780 - val_accuracy: 0.8621\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3496 - accuracy: 0.8500 - val_loss: 0.3541 - val_accuracy: 0.8636\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3492 - accuracy: 0.8464 - val_loss: 0.3680 - val_accuracy: 0.8403\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3490 - accuracy: 0.8475 - val_loss: 0.3127 - val_accuracy: 0.8831\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3293 - accuracy: 0.8621 - val_loss: 0.3849 - val_accuracy: 0.8910\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3245 - accuracy: 0.8627 - val_loss: 0.3769 - val_accuracy: 0.8475\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3258 - accuracy: 0.8618 - val_loss: 0.3228 - val_accuracy: 0.8712\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3338 - accuracy: 0.8572 - val_loss: 0.3110 - val_accuracy: 0.8539\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3271 - accuracy: 0.8585 - val_loss: 0.3324 - val_accuracy: 0.8719\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3232 - accuracy: 0.8665 - val_loss: 0.3163 - val_accuracy: 0.8940\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3166 - accuracy: 0.8688 - val_loss: 0.3234 - val_accuracy: 0.8816\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3106 - accuracy: 0.8689 - val_loss: 0.3350 - val_accuracy: 0.8660\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3183 - accuracy: 0.8678 - val_loss: 0.3346 - val_accuracy: 0.8926\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2989 - accuracy: 0.8755 - val_loss: 0.4088 - val_accuracy: 0.8354\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3001 - accuracy: 0.8745 - val_loss: 0.2786 - val_accuracy: 0.8985\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.8817 - val_loss: 0.2665 - val_accuracy: 0.8912\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2965 - accuracy: 0.8769 - val_loss: 0.3058 - val_accuracy: 0.8975\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3022 - accuracy: 0.8717 - val_loss: 0.2744 - val_accuracy: 0.8862\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2964 - accuracy: 0.8790 - val_loss: 0.2818 - val_accuracy: 0.8907\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2984 - accuracy: 0.8796 - val_loss: 0.2656 - val_accuracy: 0.8887\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2985 - accuracy: 0.8759 - val_loss: 0.3298 - val_accuracy: 0.8931\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2953 - accuracy: 0.8774 - val_loss: 0.3200 - val_accuracy: 0.9096\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3005 - accuracy: 0.8802 - val_loss: 0.2982 - val_accuracy: 0.9031\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2963 - accuracy: 0.8788 - val_loss: 0.2973 - val_accuracy: 0.8923\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.8770 - val_loss: 0.6405 - val_accuracy: 0.3408\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2957 - accuracy: 0.8805 - val_loss: 0.3590 - val_accuracy: 0.8596\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2928 - accuracy: 0.8803 - val_loss: 0.2703 - val_accuracy: 0.8850\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3078 - accuracy: 0.8756 - val_loss: 0.2792 - val_accuracy: 0.8923\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3048 - accuracy: 0.8763 - val_loss: 0.2582 - val_accuracy: 0.8910\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2869 - accuracy: 0.8833 - val_loss: 0.3121 - val_accuracy: 0.9076\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2987 - accuracy: 0.8781 - val_loss: 0.3267 - val_accuracy: 0.8893\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3118 - accuracy: 0.8718 - val_loss: 0.2777 - val_accuracy: 0.8893\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3059 - accuracy: 0.8724 - val_loss: 0.3272 - val_accuracy: 0.8776\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3039 - accuracy: 0.8748 - val_loss: 0.2770 - val_accuracy: 0.8790\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3073 - accuracy: 0.8738 - val_loss: 0.2868 - val_accuracy: 0.8936\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2902 - accuracy: 0.8793 - val_loss: 0.2857 - val_accuracy: 0.8950\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2878 - accuracy: 0.8818 - val_loss: 0.3154 - val_accuracy: 0.8868\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3217 - accuracy: 0.9093\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2891 - accuracy: 0.8815\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2965 - accuracy: 0.8793\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2826 - accuracy: 0.8844\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2984 - accuracy: 0.8773\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2948 - accuracy: 0.8802\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2924 - accuracy: 0.8822\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2929 - accuracy: 0.8805\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3004 - accuracy: 0.8783\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2893 - accuracy: 0.8825\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2921 - accuracy: 0.8825\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2759 - accuracy: 0.8915\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6299 - accuracy: 0.6423 - val_loss: 0.5789 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5662 - accuracy: 0.6765 - val_loss: 0.5671 - val_accuracy: 0.7141\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5560 - accuracy: 0.7069 - val_loss: 0.5675 - val_accuracy: 0.7193\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5366 - accuracy: 0.7163 - val_loss: 0.5336 - val_accuracy: 0.7493\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4860 - accuracy: 0.7472 - val_loss: 0.4977 - val_accuracy: 0.7821\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4421 - accuracy: 0.7860 - val_loss: 0.4544 - val_accuracy: 0.7869\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4231 - accuracy: 0.7952 - val_loss: 0.5606 - val_accuracy: 0.8083\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3999 - accuracy: 0.8154 - val_loss: 0.4882 - val_accuracy: 0.7931\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3785 - accuracy: 0.8290 - val_loss: 0.5342 - val_accuracy: 0.7777\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3742 - accuracy: 0.8332 - val_loss: 0.3973 - val_accuracy: 0.8090\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3566 - accuracy: 0.8449 - val_loss: 0.3520 - val_accuracy: 0.8555\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3551 - accuracy: 0.8450 - val_loss: 0.3377 - val_accuracy: 0.8549\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3390 - accuracy: 0.8538 - val_loss: 0.3491 - val_accuracy: 0.8773\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3296 - accuracy: 0.8635 - val_loss: 0.3642 - val_accuracy: 0.8921\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3284 - accuracy: 0.8629 - val_loss: 0.3038 - val_accuracy: 0.8792\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3237 - accuracy: 0.8637 - val_loss: 0.3173 - val_accuracy: 0.8965\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3217 - accuracy: 0.8644 - val_loss: 0.4892 - val_accuracy: 0.8592\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3313 - accuracy: 0.8583 - val_loss: 0.3283 - val_accuracy: 0.8934\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3268 - accuracy: 0.8586 - val_loss: 0.3004 - val_accuracy: 0.8905\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3283 - accuracy: 0.8594 - val_loss: 0.3008 - val_accuracy: 0.8620\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3075 - accuracy: 0.8713 - val_loss: 0.3629 - val_accuracy: 0.8943\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3199 - accuracy: 0.8657 - val_loss: 0.3164 - val_accuracy: 0.9020\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3128 - accuracy: 0.8661 - val_loss: 0.3710 - val_accuracy: 0.8813\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3179 - accuracy: 0.8660 - val_loss: 0.3156 - val_accuracy: 0.8736\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3148 - accuracy: 0.8713 - val_loss: 0.3242 - val_accuracy: 0.8825\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3133 - accuracy: 0.8685 - val_loss: 0.2932 - val_accuracy: 0.8866\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3064 - accuracy: 0.8723 - val_loss: 0.2912 - val_accuracy: 0.8955\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3029 - accuracy: 0.8723 - val_loss: 0.2991 - val_accuracy: 0.8805\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3048 - accuracy: 0.8748 - val_loss: 0.3558 - val_accuracy: 0.8933\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3006 - accuracy: 0.8757 - val_loss: 0.3227 - val_accuracy: 0.8995\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2959 - accuracy: 0.8772 - val_loss: 0.3367 - val_accuracy: 0.8975\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2941 - accuracy: 0.8799 - val_loss: 0.2828 - val_accuracy: 0.8871\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3064 - accuracy: 0.8739 - val_loss: 0.2967 - val_accuracy: 0.8937\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3137 - accuracy: 0.8710 - val_loss: 0.2968 - val_accuracy: 0.8915\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2936 - accuracy: 0.8812 - val_loss: 0.2741 - val_accuracy: 0.8984\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2922 - accuracy: 0.8818 - val_loss: 0.2809 - val_accuracy: 0.8996\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2946 - accuracy: 0.8808 - val_loss: 0.2807 - val_accuracy: 0.8989\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3180 - accuracy: 0.9000\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3134 - accuracy: 0.8687\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3114 - accuracy: 0.8698\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3064 - accuracy: 0.8715\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3044 - accuracy: 0.8721\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3053 - accuracy: 0.8716\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3077 - accuracy: 0.8707\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.3036 - accuracy: 0.8734\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2934 - accuracy: 0.8770\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2929 - accuracy: 0.8782\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2913 - accuracy: 0.8789\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.3315 - accuracy: 0.8914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAz18SQ6kt-l",
        "outputId": "5290cded-a477-403f-8d30-9f62af5617ec"
      },
      "source": [
        "print(round(cv_results_top10.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.894\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHsexMrzkt-l",
        "outputId": "3dfe2099-abfb-442b-e190-ca8af496d9d9"
      },
      "source": [
        "print(round(cv_results_top10.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od64296zqdz4",
        "outputId": "cba7b418-a753-446e-ab48-7a6879262c44"
      },
      "source": [
        "cv_results_top10"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.91302878, 0.88877606, 0.88544196, 0.89153367, 0.89136446])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 228
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbUPR9wAkt-l",
        "outputId": "8a32be11-3bf5-4391-e992-49923c353e32"
      },
      "source": [
        "cv_top10_preds = cross_val_predict(mod_top10, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6264 - accuracy: 0.6454 - val_loss: 0.5845 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5656 - accuracy: 0.6848 - val_loss: 0.5868 - val_accuracy: 0.7130\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5523 - accuracy: 0.7061 - val_loss: 0.5627 - val_accuracy: 0.7136\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5446 - accuracy: 0.7110 - val_loss: 0.5419 - val_accuracy: 0.7212\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5258 - accuracy: 0.7204 - val_loss: 0.4660 - val_accuracy: 0.7765\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4675 - accuracy: 0.7644 - val_loss: 0.4277 - val_accuracy: 0.7974\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4350 - accuracy: 0.7891 - val_loss: 0.4461 - val_accuracy: 0.7745\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4180 - accuracy: 0.7997 - val_loss: 0.3893 - val_accuracy: 0.8325\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3829 - accuracy: 0.8320 - val_loss: 0.3785 - val_accuracy: 0.8359\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3687 - accuracy: 0.8402 - val_loss: 0.4939 - val_accuracy: 0.7746\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3604 - accuracy: 0.8428 - val_loss: 0.3806 - val_accuracy: 0.8595\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3478 - accuracy: 0.8500 - val_loss: 0.3651 - val_accuracy: 0.8863\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3429 - accuracy: 0.8544 - val_loss: 0.3412 - val_accuracy: 0.8737\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3326 - accuracy: 0.8592 - val_loss: 0.3210 - val_accuracy: 0.8681\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8644 - val_loss: 0.3068 - val_accuracy: 0.8780\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3220 - accuracy: 0.8610 - val_loss: 0.3004 - val_accuracy: 0.8921\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3141 - accuracy: 0.8670 - val_loss: 0.3288 - val_accuracy: 0.8565\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3132 - accuracy: 0.8682 - val_loss: 0.3311 - val_accuracy: 0.8781\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3068 - accuracy: 0.8692 - val_loss: 0.3154 - val_accuracy: 0.8972\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3050 - accuracy: 0.8726 - val_loss: 0.3768 - val_accuracy: 0.8445\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2941 - accuracy: 0.8770 - val_loss: 0.4065 - val_accuracy: 0.8149\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3027 - accuracy: 0.8713 - val_loss: 0.3435 - val_accuracy: 0.8037\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3015 - accuracy: 0.8705 - val_loss: 0.3380 - val_accuracy: 0.8801\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3021 - accuracy: 0.8728 - val_loss: 0.4259 - val_accuracy: 0.7774\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3005 - accuracy: 0.8725 - val_loss: 0.3122 - val_accuracy: 0.8927\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2945 - accuracy: 0.8753 - val_loss: 0.3483 - val_accuracy: 0.8205\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2978 - accuracy: 0.8741 - val_loss: 0.3805 - val_accuracy: 0.7944\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2879 - accuracy: 0.8787 - val_loss: 0.3268 - val_accuracy: 0.9069\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2854 - accuracy: 0.8825 - val_loss: 0.2764 - val_accuracy: 0.8988\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2831 - accuracy: 0.8814 - val_loss: 0.2761 - val_accuracy: 0.9000\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.8780 - val_loss: 0.3314 - val_accuracy: 0.8443\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2983 - accuracy: 0.8732 - val_loss: 0.3135 - val_accuracy: 0.8293\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2823 - accuracy: 0.8814 - val_loss: 0.3215 - val_accuracy: 0.8926\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2967 - accuracy: 0.8761 - val_loss: 0.3239 - val_accuracy: 0.8958\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8731 - val_loss: 0.2968 - val_accuracy: 0.9031\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2792 - accuracy: 0.8835 - val_loss: 0.3207 - val_accuracy: 0.8186\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2847 - accuracy: 0.8792 - val_loss: 0.3327 - val_accuracy: 0.8773\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2766 - accuracy: 0.8827 - val_loss: 0.2808 - val_accuracy: 0.9091\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8785 - val_loss: 0.2960 - val_accuracy: 0.9076\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2794 - accuracy: 0.8834 - val_loss: 0.3007 - val_accuracy: 0.9026\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8836 - val_loss: 0.2736 - val_accuracy: 0.9060\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2760 - accuracy: 0.8836 - val_loss: 0.3002 - val_accuracy: 0.9023\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2757 - accuracy: 0.8840 - val_loss: 0.2894 - val_accuracy: 0.8976\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2775 - accuracy: 0.8829 - val_loss: 0.3403 - val_accuracy: 0.8048\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2827 - accuracy: 0.8803 - val_loss: 0.3734 - val_accuracy: 0.7873\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2904 - accuracy: 0.8768 - val_loss: 0.2865 - val_accuracy: 0.9040\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.8842 - val_loss: 0.3193 - val_accuracy: 0.8199\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2761 - accuracy: 0.8848 - val_loss: 0.3076 - val_accuracy: 0.8951\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2773 - accuracy: 0.8824 - val_loss: 0.2978 - val_accuracy: 0.9002\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8839 - val_loss: 0.3039 - val_accuracy: 0.9034\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3040 - accuracy: 0.9034\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2748 - accuracy: 0.8847\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2804 - accuracy: 0.8818\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2810 - accuracy: 0.8817\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2762 - accuracy: 0.8833\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2750 - accuracy: 0.8844\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2692 - accuracy: 0.8885\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2709 - accuracy: 0.8873\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2686 - accuracy: 0.8883\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2725 - accuracy: 0.8869\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2698 - accuracy: 0.8885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6271 - accuracy: 0.6402 - val_loss: 0.5748 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5623 - accuracy: 0.6970 - val_loss: 0.5726 - val_accuracy: 0.7144\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5508 - accuracy: 0.7093 - val_loss: 0.5720 - val_accuracy: 0.7199\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5292 - accuracy: 0.7224 - val_loss: 0.4729 - val_accuracy: 0.7648\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4647 - accuracy: 0.7645 - val_loss: 0.5198 - val_accuracy: 0.7746\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4314 - accuracy: 0.7921 - val_loss: 0.4899 - val_accuracy: 0.7816\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4090 - accuracy: 0.8103 - val_loss: 0.4609 - val_accuracy: 0.8059\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3866 - accuracy: 0.8240 - val_loss: 0.4642 - val_accuracy: 0.8052\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3703 - accuracy: 0.8327 - val_loss: 0.5033 - val_accuracy: 0.7974\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3747 - accuracy: 0.8336 - val_loss: 0.4933 - val_accuracy: 0.7876\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3595 - accuracy: 0.8416 - val_loss: 0.3678 - val_accuracy: 0.8323\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3491 - accuracy: 0.8463 - val_loss: 0.5594 - val_accuracy: 0.7704\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3508 - accuracy: 0.8469 - val_loss: 0.4523 - val_accuracy: 0.7973\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3454 - accuracy: 0.8529 - val_loss: 0.3910 - val_accuracy: 0.8619\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3324 - accuracy: 0.8575 - val_loss: 0.3671 - val_accuracy: 0.8122\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3279 - accuracy: 0.8614 - val_loss: 0.5758 - val_accuracy: 0.7435\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3286 - accuracy: 0.8583 - val_loss: 0.4545 - val_accuracy: 0.7832\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3251 - accuracy: 0.8593 - val_loss: 0.3953 - val_accuracy: 0.8466\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8648 - val_loss: 0.4311 - val_accuracy: 0.8113\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3091 - accuracy: 0.8672 - val_loss: 0.3542 - val_accuracy: 0.8520\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3121 - accuracy: 0.8661 - val_loss: 0.3379 - val_accuracy: 0.8914\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3078 - accuracy: 0.8693 - val_loss: 0.3066 - val_accuracy: 0.8840\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2985 - accuracy: 0.8717 - val_loss: 0.2962 - val_accuracy: 0.8897\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2986 - accuracy: 0.8716 - val_loss: 0.3833 - val_accuracy: 0.8120\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3026 - accuracy: 0.8728 - val_loss: 0.3403 - val_accuracy: 0.8078\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2970 - accuracy: 0.8755 - val_loss: 0.3517 - val_accuracy: 0.8137\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2900 - accuracy: 0.8781 - val_loss: 0.3046 - val_accuracy: 0.8875\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2914 - accuracy: 0.8745 - val_loss: 0.3095 - val_accuracy: 0.8656\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2912 - accuracy: 0.8792 - val_loss: 0.2831 - val_accuracy: 0.8931\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2954 - accuracy: 0.8743 - val_loss: 0.3325 - val_accuracy: 0.8854\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2843 - accuracy: 0.8810 - val_loss: 0.2893 - val_accuracy: 0.8951\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2903 - accuracy: 0.8774 - val_loss: 0.3763 - val_accuracy: 0.8054\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2952 - accuracy: 0.8764 - val_loss: 0.2753 - val_accuracy: 0.8927\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2857 - accuracy: 0.8838 - val_loss: 0.2835 - val_accuracy: 0.8956\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2842 - accuracy: 0.8814 - val_loss: 0.3934 - val_accuracy: 0.8897\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2765 - accuracy: 0.8854 - val_loss: 0.3145 - val_accuracy: 0.8996\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2821 - accuracy: 0.8844 - val_loss: 0.3408 - val_accuracy: 0.8860\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2837 - accuracy: 0.8857 - val_loss: 0.3312 - val_accuracy: 0.9039\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8861 - val_loss: 0.3345 - val_accuracy: 0.9042\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8855 - val_loss: 0.3575 - val_accuracy: 0.8948\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8860 - val_loss: 0.3229 - val_accuracy: 0.8988\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2683 - accuracy: 0.8895 - val_loss: 0.4024 - val_accuracy: 0.8243\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2724 - accuracy: 0.8847 - val_loss: 0.4655 - val_accuracy: 0.7969\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2748 - accuracy: 0.8888 - val_loss: 0.2773 - val_accuracy: 0.8991\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8894 - val_loss: 0.3142 - val_accuracy: 0.9032\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2803 - accuracy: 0.8860 - val_loss: 0.3314 - val_accuracy: 0.8989\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2817 - accuracy: 0.8843 - val_loss: 0.3207 - val_accuracy: 0.8976\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8868 - val_loss: 0.2915 - val_accuracy: 0.8872\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2745 - accuracy: 0.8893 - val_loss: 0.3179 - val_accuracy: 0.8929\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2748 - accuracy: 0.8886 - val_loss: 0.4556 - val_accuracy: 0.9019\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.4560 - accuracy: 0.9015\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2772 - accuracy: 0.8860\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2800 - accuracy: 0.8842\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2734 - accuracy: 0.8875\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2706 - accuracy: 0.8893\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2730 - accuracy: 0.8883\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2728 - accuracy: 0.8892\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2684 - accuracy: 0.8890\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2729 - accuracy: 0.8883\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2719 - accuracy: 0.8888\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2711 - accuracy: 0.8890\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6352 - accuracy: 0.6440 - val_loss: 0.5750 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5655 - accuracy: 0.6776 - val_loss: 0.5682 - val_accuracy: 0.7090\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5522 - accuracy: 0.7042 - val_loss: 0.5858 - val_accuracy: 0.7120\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5390 - accuracy: 0.7099 - val_loss: 0.4914 - val_accuracy: 0.7696\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4789 - accuracy: 0.7535 - val_loss: 0.4725 - val_accuracy: 0.7643\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4377 - accuracy: 0.7862 - val_loss: 0.4406 - val_accuracy: 0.8046\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4126 - accuracy: 0.8056 - val_loss: 0.4638 - val_accuracy: 0.7968\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4025 - accuracy: 0.8135 - val_loss: 0.4556 - val_accuracy: 0.8196\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3869 - accuracy: 0.8212 - val_loss: 0.4123 - val_accuracy: 0.8209\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3689 - accuracy: 0.8326 - val_loss: 0.4175 - val_accuracy: 0.8027\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3580 - accuracy: 0.8433 - val_loss: 0.4355 - val_accuracy: 0.8094\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3528 - accuracy: 0.8460 - val_loss: 0.4198 - val_accuracy: 0.8392\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3488 - accuracy: 0.8514 - val_loss: 0.3577 - val_accuracy: 0.8809\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3354 - accuracy: 0.8572 - val_loss: 0.3310 - val_accuracy: 0.8890\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3260 - accuracy: 0.8612 - val_loss: 0.3590 - val_accuracy: 0.8751\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3321 - accuracy: 0.8552 - val_loss: 0.4708 - val_accuracy: 0.8161\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3308 - accuracy: 0.8587 - val_loss: 0.3579 - val_accuracy: 0.8614\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3272 - accuracy: 0.8633 - val_loss: 0.3606 - val_accuracy: 0.8805\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3245 - accuracy: 0.8632 - val_loss: 0.3143 - val_accuracy: 0.8714\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3280 - accuracy: 0.8649 - val_loss: 0.3241 - val_accuracy: 0.8915\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3157 - accuracy: 0.8678 - val_loss: 0.2978 - val_accuracy: 0.8979\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3151 - accuracy: 0.8703 - val_loss: 0.3363 - val_accuracy: 0.8795\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3191 - accuracy: 0.8670 - val_loss: 0.3046 - val_accuracy: 0.8936\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3147 - accuracy: 0.8665 - val_loss: 0.3219 - val_accuracy: 0.8948\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3016 - accuracy: 0.8764 - val_loss: 0.2993 - val_accuracy: 0.8831\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3192 - accuracy: 0.8665 - val_loss: 0.2983 - val_accuracy: 0.8983\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3132 - accuracy: 0.8699 - val_loss: 0.3231 - val_accuracy: 0.8901\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3112 - accuracy: 0.8701 - val_loss: 0.3040 - val_accuracy: 0.8867\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3053 - accuracy: 0.8732 - val_loss: 0.2953 - val_accuracy: 0.8939\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3045 - accuracy: 0.8759 - val_loss: 0.2985 - val_accuracy: 0.8932\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3002 - accuracy: 0.8740 - val_loss: 0.2826 - val_accuracy: 0.8933\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3086 - accuracy: 0.8739 - val_loss: 0.3083 - val_accuracy: 0.8834\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3194 - accuracy: 0.8649 - val_loss: 0.3337 - val_accuracy: 0.8917\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3180 - accuracy: 0.8674 - val_loss: 0.2963 - val_accuracy: 0.8930\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3024 - accuracy: 0.8757 - val_loss: 0.2795 - val_accuracy: 0.8870\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3059 - accuracy: 0.8721 - val_loss: 0.2728 - val_accuracy: 0.8889\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3051 - accuracy: 0.8711 - val_loss: 0.2819 - val_accuracy: 0.9045\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3058 - accuracy: 0.8727 - val_loss: 0.3773 - val_accuracy: 0.8105\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2966 - accuracy: 0.8792 - val_loss: 0.3556 - val_accuracy: 0.8625\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3088 - accuracy: 0.8705 - val_loss: 0.3036 - val_accuracy: 0.8855\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8825 - val_loss: 0.2955 - val_accuracy: 0.8938\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2899 - accuracy: 0.8825 - val_loss: 0.2669 - val_accuracy: 0.8830\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2982 - accuracy: 0.8764 - val_loss: 0.3183 - val_accuracy: 0.8992\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2913 - accuracy: 0.8784 - val_loss: 0.3287 - val_accuracy: 0.8969\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2851 - accuracy: 0.8833 - val_loss: 0.2794 - val_accuracy: 0.8977\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2854 - accuracy: 0.8845 - val_loss: 0.3588 - val_accuracy: 0.9093\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2801 - accuracy: 0.8848 - val_loss: 0.3319 - val_accuracy: 0.9011\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2910 - accuracy: 0.8803 - val_loss: 0.3419 - val_accuracy: 0.8996\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2873 - accuracy: 0.8821 - val_loss: 0.3042 - val_accuracy: 0.9099\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8826 - val_loss: 0.2564 - val_accuracy: 0.8952\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2582 - accuracy: 0.8939\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2801 - accuracy: 0.8852\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2779 - accuracy: 0.8861\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2828 - accuracy: 0.8861\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2874 - accuracy: 0.8837\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2857 - accuracy: 0.8836\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2826 - accuracy: 0.8849\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2825 - accuracy: 0.8847\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2849 - accuracy: 0.8864\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2822 - accuracy: 0.8875\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2821 - accuracy: 0.8861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6296 - accuracy: 0.6428 - val_loss: 0.5738 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5647 - accuracy: 0.6691 - val_loss: 0.5712 - val_accuracy: 0.7105\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5561 - accuracy: 0.7047 - val_loss: 0.5710 - val_accuracy: 0.7150\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5490 - accuracy: 0.7058 - val_loss: 0.5450 - val_accuracy: 0.7234\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5209 - accuracy: 0.7291 - val_loss: 0.4905 - val_accuracy: 0.7428\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4738 - accuracy: 0.7601 - val_loss: 0.4858 - val_accuracy: 0.7740\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4371 - accuracy: 0.7892 - val_loss: 0.4101 - val_accuracy: 0.8129\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4168 - accuracy: 0.8046 - val_loss: 0.4631 - val_accuracy: 0.7851\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4062 - accuracy: 0.8096 - val_loss: 0.3936 - val_accuracy: 0.8096\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3887 - accuracy: 0.8165 - val_loss: 0.3768 - val_accuracy: 0.8429\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3885 - accuracy: 0.8227 - val_loss: 0.3844 - val_accuracy: 0.8596\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3702 - accuracy: 0.8341 - val_loss: 0.3522 - val_accuracy: 0.8767\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3516 - accuracy: 0.8463 - val_loss: 0.3984 - val_accuracy: 0.8316\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3596 - accuracy: 0.8388 - val_loss: 0.3856 - val_accuracy: 0.8474\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3469 - accuracy: 0.8475 - val_loss: 0.3826 - val_accuracy: 0.8322\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3301 - accuracy: 0.8577 - val_loss: 0.3794 - val_accuracy: 0.8370\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3329 - accuracy: 0.8580 - val_loss: 0.3783 - val_accuracy: 0.8839\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3261 - accuracy: 0.8603 - val_loss: 0.3746 - val_accuracy: 0.8839\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3163 - accuracy: 0.8668 - val_loss: 0.3312 - val_accuracy: 0.8881\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3219 - accuracy: 0.8628 - val_loss: 0.3235 - val_accuracy: 0.8876\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8605 - val_loss: 0.3315 - val_accuracy: 0.8818\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3161 - accuracy: 0.8675 - val_loss: 0.3634 - val_accuracy: 0.8717\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3135 - accuracy: 0.8659 - val_loss: 0.3076 - val_accuracy: 0.8801\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3076 - accuracy: 0.8703 - val_loss: 0.3108 - val_accuracy: 0.8906\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3182 - accuracy: 0.8652 - val_loss: 0.2935 - val_accuracy: 0.8921\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3132 - accuracy: 0.8660 - val_loss: 0.2919 - val_accuracy: 0.8877\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3011 - accuracy: 0.8717 - val_loss: 0.3434 - val_accuracy: 0.8991\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3011 - accuracy: 0.8744 - val_loss: 0.3408 - val_accuracy: 0.8591\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2982 - accuracy: 0.8737 - val_loss: 0.3186 - val_accuracy: 0.8782\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2930 - accuracy: 0.8777 - val_loss: 0.2929 - val_accuracy: 0.9016\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2984 - accuracy: 0.8762 - val_loss: 0.2949 - val_accuracy: 0.8904\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2956 - accuracy: 0.8801 - val_loss: 0.3454 - val_accuracy: 0.8995\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2962 - accuracy: 0.8788 - val_loss: 0.4181 - val_accuracy: 0.8839\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2969 - accuracy: 0.8778 - val_loss: 0.3123 - val_accuracy: 0.8869\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2955 - accuracy: 0.8795 - val_loss: 0.2675 - val_accuracy: 0.8975\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2895 - accuracy: 0.8842 - val_loss: 0.3105 - val_accuracy: 0.8907\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2870 - accuracy: 0.8831 - val_loss: 0.3274 - val_accuracy: 0.8963\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2783 - accuracy: 0.8880 - val_loss: 0.3431 - val_accuracy: 0.9088\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2832 - accuracy: 0.8824 - val_loss: 0.2946 - val_accuracy: 0.8987\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2839 - accuracy: 0.8864 - val_loss: 0.3608 - val_accuracy: 0.8900\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2893 - accuracy: 0.8831 - val_loss: 0.2739 - val_accuracy: 0.9096\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2772 - accuracy: 0.8850 - val_loss: 0.3229 - val_accuracy: 0.8614\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8855 - val_loss: 0.3110 - val_accuracy: 0.9036\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8833 - val_loss: 0.4409 - val_accuracy: 0.8976\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2781 - accuracy: 0.8865 - val_loss: 0.3523 - val_accuracy: 0.8930\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8830 - val_loss: 0.2984 - val_accuracy: 0.9007\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2821 - accuracy: 0.8829 - val_loss: 0.2631 - val_accuracy: 0.8935\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8877 - val_loss: 0.2949 - val_accuracy: 0.8840\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2848 - accuracy: 0.8873 - val_loss: 0.3064 - val_accuracy: 0.9036\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8899 - val_loss: 0.3481 - val_accuracy: 0.8987\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3474 - accuracy: 0.8998\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2803 - accuracy: 0.8865\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2749 - accuracy: 0.8870\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2781 - accuracy: 0.8884\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2784 - accuracy: 0.8863\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2683 - accuracy: 0.8914\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2776 - accuracy: 0.8874\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2734 - accuracy: 0.8895\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2720 - accuracy: 0.8876\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2710 - accuracy: 0.8902\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2701 - accuracy: 0.8909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6280 - accuracy: 0.6442 - val_loss: 0.5759 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5593 - accuracy: 0.6971 - val_loss: 0.5853 - val_accuracy: 0.7134\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5556 - accuracy: 0.7046 - val_loss: 0.5967 - val_accuracy: 0.7043\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5448 - accuracy: 0.7090 - val_loss: 0.5859 - val_accuracy: 0.7119\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5270 - accuracy: 0.7216 - val_loss: 0.5380 - val_accuracy: 0.7638\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4685 - accuracy: 0.7678 - val_loss: 0.4470 - val_accuracy: 0.8127\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4260 - accuracy: 0.7978 - val_loss: 0.4063 - val_accuracy: 0.8109\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3992 - accuracy: 0.8162 - val_loss: 0.3905 - val_accuracy: 0.8253\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3795 - accuracy: 0.8299 - val_loss: 0.4289 - val_accuracy: 0.7924\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3666 - accuracy: 0.8382 - val_loss: 0.3626 - val_accuracy: 0.8507\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3503 - accuracy: 0.8490 - val_loss: 0.3935 - val_accuracy: 0.8341\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3443 - accuracy: 0.8501 - val_loss: 0.5128 - val_accuracy: 0.7871\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3269 - accuracy: 0.8618 - val_loss: 0.3481 - val_accuracy: 0.8809\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3216 - accuracy: 0.8596 - val_loss: 0.3900 - val_accuracy: 0.8316\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3163 - accuracy: 0.8625 - val_loss: 0.3671 - val_accuracy: 0.8902\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3112 - accuracy: 0.8666 - val_loss: 0.3928 - val_accuracy: 0.8340\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3159 - accuracy: 0.8666 - val_loss: 0.4205 - val_accuracy: 0.7978\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3082 - accuracy: 0.8704 - val_loss: 0.3308 - val_accuracy: 0.8858\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3075 - accuracy: 0.8705 - val_loss: 0.3385 - val_accuracy: 0.8928\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2998 - accuracy: 0.8737 - val_loss: 0.3861 - val_accuracy: 0.8757\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3066 - accuracy: 0.8700 - val_loss: 0.3096 - val_accuracy: 0.8915\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3082 - accuracy: 0.8708 - val_loss: 0.3549 - val_accuracy: 0.8878\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3039 - accuracy: 0.8711 - val_loss: 0.3534 - val_accuracy: 0.8948\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3001 - accuracy: 0.8734 - val_loss: 0.3818 - val_accuracy: 0.8925\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2957 - accuracy: 0.8761 - val_loss: 0.3400 - val_accuracy: 0.8401\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2989 - accuracy: 0.8751 - val_loss: 0.3021 - val_accuracy: 0.8825\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2942 - accuracy: 0.8779 - val_loss: 0.3996 - val_accuracy: 0.8965\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2948 - accuracy: 0.8801 - val_loss: 0.3001 - val_accuracy: 0.8985\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3065 - accuracy: 0.8705 - val_loss: 0.3126 - val_accuracy: 0.8865\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3091 - accuracy: 0.8723 - val_loss: 0.3022 - val_accuracy: 0.8767\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2895 - accuracy: 0.8803 - val_loss: 0.2657 - val_accuracy: 0.9021\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2892 - accuracy: 0.8799 - val_loss: 0.2925 - val_accuracy: 0.8757\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8787 - val_loss: 0.2727 - val_accuracy: 0.8953\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8820 - val_loss: 0.2704 - val_accuracy: 0.8994\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2772 - accuracy: 0.8860 - val_loss: 0.2821 - val_accuracy: 0.8785\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2938 - accuracy: 0.8770 - val_loss: 0.2961 - val_accuracy: 0.8956\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2892 - accuracy: 0.8801 - val_loss: 0.2933 - val_accuracy: 0.8886\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2988 - accuracy: 0.8752 - val_loss: 0.3621 - val_accuracy: 0.8554\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2902 - accuracy: 0.8811 - val_loss: 0.2876 - val_accuracy: 0.8923\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2808 - accuracy: 0.8845 - val_loss: 0.2862 - val_accuracy: 0.8863\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2996 - accuracy: 0.8754 - val_loss: 0.3802 - val_accuracy: 0.9015\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2808 - accuracy: 0.8863 - val_loss: 0.2726 - val_accuracy: 0.8944\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8838 - val_loss: 0.3582 - val_accuracy: 0.8965\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2807 - accuracy: 0.8839 - val_loss: 0.3544 - val_accuracy: 0.9030\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2947 - accuracy: 0.8798 - val_loss: 0.3442 - val_accuracy: 0.9107\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2829 - accuracy: 0.8837 - val_loss: 0.3117 - val_accuracy: 0.8843\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8852 - val_loss: 0.2931 - val_accuracy: 0.8939\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2803 - accuracy: 0.8881 - val_loss: 0.3135 - val_accuracy: 0.9062\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2748 - accuracy: 0.8875 - val_loss: 0.3370 - val_accuracy: 0.8920\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2784 - accuracy: 0.8867 - val_loss: 0.2958 - val_accuracy: 0.8992\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2971 - accuracy: 0.8971\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2808 - accuracy: 0.8861\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 1s 4ms/step - loss: 0.2772 - accuracy: 0.8873\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2745 - accuracy: 0.8879\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2748 - accuracy: 0.8886\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2719 - accuracy: 0.8888\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2798 - accuracy: 0.8850\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2719 - accuracy: 0.8895\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2704 - accuracy: 0.8895\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2680 - accuracy: 0.8907\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2696 - accuracy: 0.8912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PFDSQ9kkt-l",
        "outputId": "09ec56d1-c16d-4f11-d7c7-0ebc517da246"
      },
      "source": [
        "cm_top10 = confusion_matrix(y, cv_top10_preds)\n",
        "print(cm_top10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[50015  7985]\n",
            " [ 2098 28549]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyBh_Ph_dXnT"
      },
      "source": [
        "# neural network on top 25 most important features per recursive feature elimination package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5_7jqzLu6dEV",
        "outputId": "3c074d40-6c51-4e21-c3e0-ad7ff4d69954"
      },
      "source": [
        "y = full_df.iloc[:,-1]\n",
        "\n",
        "features = ['qty_slash_url', 'length_url', 'qty_dot_domain', 'domain_length',\n",
        "       'qty_dot_directory', 'qty_hyphen_directory', 'qty_underline_directory',\n",
        "       'qty_slash_directory', 'qty_at_directory', 'qty_comma_directory',\n",
        "       'qty_asterisk_directory', 'qty_hashtag_directory', 'directory_length',\n",
        "       'qty_dot_file', 'qty_hyphen_file', 'qty_at_file',\n",
        "       'qty_exclamation_file', 'qty_space_file', 'qty_dollar_file',\n",
        "       'file_length', 'time_response', 'asn_ip', 'time_domain_activation',\n",
        "       'time_domain_expiration', 'ttl_hostname']\n",
        "X = full_df[features]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.25, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>qty_dot_directory</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>qty_at_directory</th>\n",
              "      <th>qty_comma_directory</th>\n",
              "      <th>qty_asterisk_directory</th>\n",
              "      <th>qty_hashtag_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_dot_file</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>qty_at_file</th>\n",
              "      <th>qty_exclamation_file</th>\n",
              "      <th>qty_space_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>time_response</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>ttl_hostname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.999934</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.011480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.730211</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.680200</td>\n",
              "      <td>0.064004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.698307</td>\n",
              "      <td>0.269674</td>\n",
              "      <td>0.016112</td>\n",
              "      <td>0.662860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.859058</td>\n",
              "      <td>0.261792</td>\n",
              "      <td>0.005926</td>\n",
              "      <td>0.439823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.795440</td>\n",
              "      <td>0.178787</td>\n",
              "      <td>0.007212</td>\n",
              "      <td>0.579014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_slash_url  length_url  ...  time_domain_expiration  ttl_hostname\n",
              "5676        0.000004    0.000042  ...                0.000797      0.011480\n",
              "39002       0.000000    0.003369  ...                0.680200      0.064004\n",
              "1732        0.000000    0.000737  ...                0.016112      0.662860\n",
              "39668       0.000000    0.001588  ...                0.005926      0.439823\n",
              "82035       0.000000    0.000751  ...                0.007212      0.579014\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 231
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BgEu9uC16dEV",
        "outputId": "6ac97909-2130-40c1-8d14-5aabfe5b0d22"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "top_25_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[25]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "top_25_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "top_25_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                1664      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 14,561\n",
            "Trainable params: 14,561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AeWAUC86dEV",
        "outputId": "13964b46-a896-4493-9d60-5c0daed9a3d9"
      },
      "source": [
        "history1 = top_25_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6288 - binary_accuracy: 0.6534 - auc: 0.6046 - val_loss: 0.5887 - val_binary_accuracy: 0.7220 - val_auc: 0.7622\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5574 - binary_accuracy: 0.6959 - auc: 0.7449 - val_loss: 0.5731 - val_binary_accuracy: 0.7196 - val_auc: 0.7675\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5450 - binary_accuracy: 0.7166 - auc: 0.7545 - val_loss: 0.5680 - val_binary_accuracy: 0.7290 - val_auc: 0.7751\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5325 - binary_accuracy: 0.7263 - auc: 0.7665 - val_loss: 0.5371 - val_binary_accuracy: 0.7465 - val_auc: 0.7997\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5013 - binary_accuracy: 0.7430 - auc: 0.7959 - val_loss: 0.4994 - val_binary_accuracy: 0.7631 - val_auc: 0.8365\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4616 - binary_accuracy: 0.7769 - auc: 0.8356 - val_loss: 0.4862 - val_binary_accuracy: 0.7942 - val_auc: 0.8700\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4236 - binary_accuracy: 0.8079 - auc: 0.8735 - val_loss: 0.4806 - val_binary_accuracy: 0.8325 - val_auc: 0.9126\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4007 - binary_accuracy: 0.8206 - auc: 0.8917 - val_loss: 0.4021 - val_binary_accuracy: 0.8275 - val_auc: 0.9213\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3700 - binary_accuracy: 0.8478 - auc: 0.9108 - val_loss: 0.3864 - val_binary_accuracy: 0.8459 - val_auc: 0.9316\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3675 - binary_accuracy: 0.8481 - auc: 0.9108 - val_loss: 0.3688 - val_binary_accuracy: 0.8625 - val_auc: 0.9305\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3418 - binary_accuracy: 0.8612 - auc: 0.9214 - val_loss: 0.3590 - val_binary_accuracy: 0.8778 - val_auc: 0.9489\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3348 - binary_accuracy: 0.8633 - auc: 0.9258 - val_loss: 0.4117 - val_binary_accuracy: 0.8833 - val_auc: 0.9443\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3239 - binary_accuracy: 0.8675 - auc: 0.9299 - val_loss: 0.3406 - val_binary_accuracy: 0.8928 - val_auc: 0.9501\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3149 - binary_accuracy: 0.8695 - auc: 0.9329 - val_loss: 0.3615 - val_binary_accuracy: 0.8809 - val_auc: 0.9479\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3062 - binary_accuracy: 0.8780 - auc: 0.9348 - val_loss: 0.3809 - val_binary_accuracy: 0.8823 - val_auc: 0.9565\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3044 - binary_accuracy: 0.8748 - auc: 0.9369 - val_loss: 0.4035 - val_binary_accuracy: 0.8669 - val_auc: 0.9563\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2964 - binary_accuracy: 0.8824 - auc: 0.9393 - val_loss: 0.3039 - val_binary_accuracy: 0.8998 - val_auc: 0.9589\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2914 - binary_accuracy: 0.8791 - auc: 0.9422 - val_loss: 0.3652 - val_binary_accuracy: 0.8769 - val_auc: 0.9559\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3001 - binary_accuracy: 0.8764 - auc: 0.9376 - val_loss: 0.2986 - val_binary_accuracy: 0.9134 - val_auc: 0.9632\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2806 - binary_accuracy: 0.8850 - auc: 0.9442 - val_loss: 0.2947 - val_binary_accuracy: 0.9019 - val_auc: 0.9631\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2849 - binary_accuracy: 0.8858 - auc: 0.9438 - val_loss: 0.3010 - val_binary_accuracy: 0.9076 - val_auc: 0.9629\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2737 - binary_accuracy: 0.8903 - auc: 0.9478 - val_loss: 0.2929 - val_binary_accuracy: 0.9014 - val_auc: 0.9565\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2782 - binary_accuracy: 0.8866 - auc: 0.9467 - val_loss: 0.3106 - val_binary_accuracy: 0.9102 - val_auc: 0.9597\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2826 - binary_accuracy: 0.8856 - auc: 0.9445 - val_loss: 0.3667 - val_binary_accuracy: 0.9095 - val_auc: 0.9646\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2669 - binary_accuracy: 0.8928 - auc: 0.9502 - val_loss: 0.2634 - val_binary_accuracy: 0.9072 - val_auc: 0.9603\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2673 - binary_accuracy: 0.8948 - auc: 0.9496 - val_loss: 0.3331 - val_binary_accuracy: 0.9121 - val_auc: 0.9622\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2723 - binary_accuracy: 0.8889 - auc: 0.9481 - val_loss: 0.3110 - val_binary_accuracy: 0.9003 - val_auc: 0.9584\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2621 - binary_accuracy: 0.8944 - auc: 0.9514 - val_loss: 0.3074 - val_binary_accuracy: 0.9014 - val_auc: 0.9624\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2645 - binary_accuracy: 0.8927 - auc: 0.9503 - val_loss: 0.3680 - val_binary_accuracy: 0.9058 - val_auc: 0.9667\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2587 - binary_accuracy: 0.8981 - auc: 0.9529 - val_loss: 0.2584 - val_binary_accuracy: 0.9108 - val_auc: 0.9665\n",
            "Epoch 31/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2604 - binary_accuracy: 0.8957 - auc: 0.9524 - val_loss: 0.2961 - val_binary_accuracy: 0.9077 - val_auc: 0.9613\n",
            "Epoch 32/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2649 - binary_accuracy: 0.8941 - auc: 0.9504 - val_loss: 0.2464 - val_binary_accuracy: 0.8968 - val_auc: 0.9619\n",
            "Epoch 33/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2742 - binary_accuracy: 0.8894 - auc: 0.9474 - val_loss: 0.3467 - val_binary_accuracy: 0.9065 - val_auc: 0.9653\n",
            "Epoch 34/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2649 - binary_accuracy: 0.8951 - auc: 0.9506 - val_loss: 0.2579 - val_binary_accuracy: 0.9016 - val_auc: 0.9646\n",
            "Epoch 35/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2633 - binary_accuracy: 0.8946 - auc: 0.9514 - val_loss: 0.2859 - val_binary_accuracy: 0.9122 - val_auc: 0.9647\n",
            "Epoch 36/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2603 - binary_accuracy: 0.8941 - auc: 0.9525 - val_loss: 0.2448 - val_binary_accuracy: 0.9028 - val_auc: 0.9635\n",
            "Epoch 37/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2579 - binary_accuracy: 0.8965 - auc: 0.9536 - val_loss: 0.3152 - val_binary_accuracy: 0.9165 - val_auc: 0.9670\n",
            "Epoch 38/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2533 - binary_accuracy: 0.8979 - auc: 0.9546 - val_loss: 0.2924 - val_binary_accuracy: 0.9102 - val_auc: 0.9671\n",
            "Epoch 39/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2508 - binary_accuracy: 0.9004 - auc: 0.9561 - val_loss: 0.3103 - val_binary_accuracy: 0.9075 - val_auc: 0.9669\n",
            "Epoch 40/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2526 - binary_accuracy: 0.8977 - auc: 0.9544 - val_loss: 0.3235 - val_binary_accuracy: 0.8960 - val_auc: 0.9634\n",
            "Epoch 41/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2537 - binary_accuracy: 0.8969 - auc: 0.9540 - val_loss: 0.3486 - val_binary_accuracy: 0.8370 - val_auc: 0.9644\n",
            "Epoch 42/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2487 - binary_accuracy: 0.8998 - auc: 0.9563 - val_loss: 0.2825 - val_binary_accuracy: 0.9109 - val_auc: 0.9671\n",
            "Epoch 43/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2524 - binary_accuracy: 0.8985 - auc: 0.9554 - val_loss: 0.3179 - val_binary_accuracy: 0.8923 - val_auc: 0.9611\n",
            "Epoch 44/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2577 - binary_accuracy: 0.8977 - auc: 0.9530 - val_loss: 0.3304 - val_binary_accuracy: 0.9103 - val_auc: 0.9689\n",
            "Epoch 45/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2581 - binary_accuracy: 0.8949 - auc: 0.9543 - val_loss: 0.2305 - val_binary_accuracy: 0.9118 - val_auc: 0.9681\n",
            "Epoch 46/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2465 - binary_accuracy: 0.9017 - auc: 0.9570 - val_loss: 0.2504 - val_binary_accuracy: 0.9077 - val_auc: 0.9670\n",
            "Epoch 47/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2506 - binary_accuracy: 0.8987 - auc: 0.9563 - val_loss: 0.3034 - val_binary_accuracy: 0.8973 - val_auc: 0.9645\n",
            "Epoch 48/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2655 - binary_accuracy: 0.8926 - auc: 0.9502 - val_loss: 0.2868 - val_binary_accuracy: 0.9071 - val_auc: 0.9669\n",
            "Epoch 49/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2504 - binary_accuracy: 0.9005 - auc: 0.9568 - val_loss: 0.2432 - val_binary_accuracy: 0.9121 - val_auc: 0.9682\n",
            "Epoch 50/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2449 - binary_accuracy: 0.9017 - auc: 0.9573 - val_loss: 0.2538 - val_binary_accuracy: 0.9155 - val_auc: 0.9694\n",
            "Epoch 51/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2473 - binary_accuracy: 0.9004 - auc: 0.9567 - val_loss: 0.2949 - val_binary_accuracy: 0.9087 - val_auc: 0.9683\n",
            "Epoch 52/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2406 - binary_accuracy: 0.9047 - auc: 0.9587 - val_loss: 0.3099 - val_binary_accuracy: 0.9153 - val_auc: 0.9687\n",
            "Epoch 53/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2443 - binary_accuracy: 0.9031 - auc: 0.9578 - val_loss: 0.4461 - val_binary_accuracy: 0.8897 - val_auc: 0.9679\n",
            "Epoch 54/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2491 - binary_accuracy: 0.9018 - auc: 0.9562 - val_loss: 0.3002 - val_binary_accuracy: 0.9108 - val_auc: 0.9686\n",
            "Epoch 55/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2486 - binary_accuracy: 0.9000 - auc: 0.9567 - val_loss: 0.2815 - val_binary_accuracy: 0.9084 - val_auc: 0.9658\n",
            "Epoch 56/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2452 - binary_accuracy: 0.9040 - auc: 0.9574 - val_loss: 0.2550 - val_binary_accuracy: 0.9084 - val_auc: 0.9665\n",
            "Epoch 57/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2470 - binary_accuracy: 0.9011 - auc: 0.9566 - val_loss: 0.3225 - val_binary_accuracy: 0.8396 - val_auc: 0.9655\n",
            "Epoch 58/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2505 - binary_accuracy: 0.8987 - auc: 0.9556 - val_loss: 0.3277 - val_binary_accuracy: 0.8991 - val_auc: 0.9678\n",
            "Epoch 59/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2441 - binary_accuracy: 0.9035 - auc: 0.9577 - val_loss: 0.3564 - val_binary_accuracy: 0.8922 - val_auc: 0.9690\n",
            "Epoch 60/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2547 - binary_accuracy: 0.8977 - auc: 0.9546 - val_loss: 0.3068 - val_binary_accuracy: 0.9111 - val_auc: 0.9636\n",
            "Epoch 61/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2410 - binary_accuracy: 0.9062 - auc: 0.9585 - val_loss: 0.2737 - val_binary_accuracy: 0.9156 - val_auc: 0.9687\n",
            "Epoch 62/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2416 - binary_accuracy: 0.9044 - auc: 0.9596 - val_loss: 0.2464 - val_binary_accuracy: 0.9167 - val_auc: 0.9700\n",
            "Epoch 63/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2461 - binary_accuracy: 0.9011 - auc: 0.9577 - val_loss: 0.2443 - val_binary_accuracy: 0.9145 - val_auc: 0.9666\n",
            "Epoch 64/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2427 - binary_accuracy: 0.9022 - auc: 0.9583 - val_loss: 0.3109 - val_binary_accuracy: 0.8818 - val_auc: 0.9687\n",
            "Epoch 65/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2417 - binary_accuracy: 0.9031 - auc: 0.9586 - val_loss: 0.2896 - val_binary_accuracy: 0.9006 - val_auc: 0.9680\n",
            "Epoch 66/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2346 - binary_accuracy: 0.9062 - auc: 0.9613 - val_loss: 0.2679 - val_binary_accuracy: 0.9127 - val_auc: 0.9684\n",
            "Epoch 67/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2362 - binary_accuracy: 0.9055 - auc: 0.9600 - val_loss: 0.2719 - val_binary_accuracy: 0.9172 - val_auc: 0.9689\n",
            "Epoch 68/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2490 - binary_accuracy: 0.8984 - auc: 0.9560 - val_loss: 0.2651 - val_binary_accuracy: 0.9121 - val_auc: 0.9633\n",
            "Epoch 69/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2373 - binary_accuracy: 0.9053 - auc: 0.9599 - val_loss: 0.2935 - val_binary_accuracy: 0.9032 - val_auc: 0.9700\n",
            "Epoch 70/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2436 - binary_accuracy: 0.9007 - auc: 0.9576 - val_loss: 0.2949 - val_binary_accuracy: 0.9015 - val_auc: 0.9687\n",
            "Epoch 71/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2563 - binary_accuracy: 0.8997 - auc: 0.9550 - val_loss: 0.3549 - val_binary_accuracy: 0.8365 - val_auc: 0.9677\n",
            "Epoch 72/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2393 - binary_accuracy: 0.9040 - auc: 0.9602 - val_loss: 0.2872 - val_binary_accuracy: 0.8972 - val_auc: 0.9691\n",
            "Epoch 73/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2366 - binary_accuracy: 0.9064 - auc: 0.9601 - val_loss: 0.2577 - val_binary_accuracy: 0.9127 - val_auc: 0.9670\n",
            "Epoch 74/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2440 - binary_accuracy: 0.9023 - auc: 0.9577 - val_loss: 0.2378 - val_binary_accuracy: 0.9100 - val_auc: 0.9674\n",
            "Epoch 75/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2389 - binary_accuracy: 0.9030 - auc: 0.9596 - val_loss: 0.2621 - val_binary_accuracy: 0.9098 - val_auc: 0.9678\n",
            "Epoch 76/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2361 - binary_accuracy: 0.9042 - auc: 0.9599 - val_loss: 0.2560 - val_binary_accuracy: 0.9071 - val_auc: 0.9702\n",
            "Epoch 77/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2499 - binary_accuracy: 0.9017 - auc: 0.9565 - val_loss: 0.3303 - val_binary_accuracy: 0.8772 - val_auc: 0.9703\n",
            "Epoch 78/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2441 - binary_accuracy: 0.9023 - auc: 0.9591 - val_loss: 0.2206 - val_binary_accuracy: 0.9084 - val_auc: 0.9689\n",
            "Epoch 79/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2377 - binary_accuracy: 0.9036 - auc: 0.9607 - val_loss: 0.2585 - val_binary_accuracy: 0.9095 - val_auc: 0.9712\n",
            "Epoch 80/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2313 - binary_accuracy: 0.9068 - auc: 0.9622 - val_loss: 0.3327 - val_binary_accuracy: 0.8779 - val_auc: 0.9631\n",
            "Epoch 81/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2380 - binary_accuracy: 0.9063 - auc: 0.9590 - val_loss: 0.2633 - val_binary_accuracy: 0.9164 - val_auc: 0.9685\n",
            "Epoch 82/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2418 - binary_accuracy: 0.9021 - auc: 0.9588 - val_loss: 0.2326 - val_binary_accuracy: 0.9119 - val_auc: 0.9695\n",
            "Epoch 83/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2440 - binary_accuracy: 0.9023 - auc: 0.9580 - val_loss: 0.2150 - val_binary_accuracy: 0.9133 - val_auc: 0.9687\n",
            "Epoch 84/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2445 - binary_accuracy: 0.9011 - auc: 0.9584 - val_loss: 0.2433 - val_binary_accuracy: 0.9156 - val_auc: 0.9683\n",
            "Epoch 85/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2393 - binary_accuracy: 0.9035 - auc: 0.9602 - val_loss: 0.2381 - val_binary_accuracy: 0.9128 - val_auc: 0.9691\n",
            "Epoch 86/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2342 - binary_accuracy: 0.9052 - auc: 0.9618 - val_loss: 0.2669 - val_binary_accuracy: 0.8891 - val_auc: 0.9687\n",
            "Epoch 87/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2373 - binary_accuracy: 0.9026 - auc: 0.9602 - val_loss: 0.2485 - val_binary_accuracy: 0.9132 - val_auc: 0.9691\n",
            "Epoch 88/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2350 - binary_accuracy: 0.9059 - auc: 0.9601 - val_loss: 0.2884 - val_binary_accuracy: 0.8854 - val_auc: 0.9701\n",
            "Epoch 89/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2422 - binary_accuracy: 0.9044 - auc: 0.9588 - val_loss: 0.2369 - val_binary_accuracy: 0.9085 - val_auc: 0.9714\n",
            "Epoch 90/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2399 - binary_accuracy: 0.9017 - auc: 0.9599 - val_loss: 0.2510 - val_binary_accuracy: 0.9162 - val_auc: 0.9705\n",
            "Epoch 91/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2357 - binary_accuracy: 0.9061 - auc: 0.9609 - val_loss: 0.2385 - val_binary_accuracy: 0.9174 - val_auc: 0.9691\n",
            "Epoch 92/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2391 - binary_accuracy: 0.9026 - auc: 0.9591 - val_loss: 0.2389 - val_binary_accuracy: 0.9152 - val_auc: 0.9697\n",
            "Epoch 93/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2337 - binary_accuracy: 0.9075 - auc: 0.9609 - val_loss: 0.3039 - val_binary_accuracy: 0.9068 - val_auc: 0.9669\n",
            "Epoch 94/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2371 - binary_accuracy: 0.9045 - auc: 0.9603 - val_loss: 0.2521 - val_binary_accuracy: 0.9139 - val_auc: 0.9699\n",
            "Epoch 95/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2405 - binary_accuracy: 0.9023 - auc: 0.9596 - val_loss: 0.2594 - val_binary_accuracy: 0.9161 - val_auc: 0.9698\n",
            "Epoch 96/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2336 - binary_accuracy: 0.9055 - auc: 0.9608 - val_loss: 0.2496 - val_binary_accuracy: 0.9087 - val_auc: 0.9679\n",
            "Epoch 97/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2354 - binary_accuracy: 0.9074 - auc: 0.9606 - val_loss: 0.2440 - val_binary_accuracy: 0.9186 - val_auc: 0.9701\n",
            "Epoch 98/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2430 - binary_accuracy: 0.9016 - auc: 0.9577 - val_loss: 0.2401 - val_binary_accuracy: 0.9164 - val_auc: 0.9707\n",
            "Epoch 99/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2317 - binary_accuracy: 0.9059 - auc: 0.9618 - val_loss: 0.2874 - val_binary_accuracy: 0.8924 - val_auc: 0.9699\n",
            "Epoch 100/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2288 - binary_accuracy: 0.9087 - auc: 0.9627 - val_loss: 0.2248 - val_binary_accuracy: 0.9115 - val_auc: 0.9695\n",
            "Epoch 101/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2312 - binary_accuracy: 0.9055 - auc: 0.9619 - val_loss: 0.2602 - val_binary_accuracy: 0.9093 - val_auc: 0.9693\n",
            "Epoch 102/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2367 - binary_accuracy: 0.9031 - auc: 0.9607 - val_loss: 0.2990 - val_binary_accuracy: 0.9106 - val_auc: 0.9701\n",
            "Epoch 103/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2340 - binary_accuracy: 0.9065 - auc: 0.9613 - val_loss: 0.2384 - val_binary_accuracy: 0.9174 - val_auc: 0.9705\n",
            "Epoch 104/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2397 - binary_accuracy: 0.9023 - auc: 0.9597 - val_loss: 0.2836 - val_binary_accuracy: 0.8922 - val_auc: 0.9688\n",
            "Epoch 105/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2305 - binary_accuracy: 0.9087 - auc: 0.9606 - val_loss: 0.2530 - val_binary_accuracy: 0.9159 - val_auc: 0.9698\n",
            "Epoch 106/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2297 - binary_accuracy: 0.9091 - auc: 0.9627 - val_loss: 0.2607 - val_binary_accuracy: 0.9086 - val_auc: 0.9694\n",
            "Epoch 107/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2357 - binary_accuracy: 0.9044 - auc: 0.9609 - val_loss: 0.2524 - val_binary_accuracy: 0.9106 - val_auc: 0.9693\n",
            "Epoch 108/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2319 - binary_accuracy: 0.9065 - auc: 0.9628 - val_loss: 0.2878 - val_binary_accuracy: 0.9087 - val_auc: 0.9705\n",
            "Epoch 109/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2346 - binary_accuracy: 0.9056 - auc: 0.9616 - val_loss: 0.2426 - val_binary_accuracy: 0.9155 - val_auc: 0.9716\n",
            "Epoch 110/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2326 - binary_accuracy: 0.9069 - auc: 0.9618 - val_loss: 0.2709 - val_binary_accuracy: 0.9098 - val_auc: 0.9691\n",
            "Epoch 111/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2286 - binary_accuracy: 0.9074 - auc: 0.9630 - val_loss: 0.2746 - val_binary_accuracy: 0.9010 - val_auc: 0.9704\n",
            "Epoch 112/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2327 - binary_accuracy: 0.9061 - auc: 0.9610 - val_loss: 0.2212 - val_binary_accuracy: 0.9202 - val_auc: 0.9704\n",
            "Epoch 113/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2371 - binary_accuracy: 0.9063 - auc: 0.9601 - val_loss: 0.2800 - val_binary_accuracy: 0.8968 - val_auc: 0.9646\n",
            "Epoch 114/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2382 - binary_accuracy: 0.9046 - auc: 0.9595 - val_loss: 0.3164 - val_binary_accuracy: 0.8573 - val_auc: 0.9703\n",
            "Epoch 115/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2372 - binary_accuracy: 0.9048 - auc: 0.9605 - val_loss: 0.2499 - val_binary_accuracy: 0.9077 - val_auc: 0.9709\n",
            "Epoch 116/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2327 - binary_accuracy: 0.9054 - auc: 0.9612 - val_loss: 0.2539 - val_binary_accuracy: 0.9098 - val_auc: 0.9706\n",
            "Epoch 117/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2402 - binary_accuracy: 0.9024 - auc: 0.9588 - val_loss: 0.2853 - val_binary_accuracy: 0.8713 - val_auc: 0.9708\n",
            "Epoch 118/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2331 - binary_accuracy: 0.9061 - auc: 0.9618 - val_loss: 0.2355 - val_binary_accuracy: 0.9098 - val_auc: 0.9708\n",
            "Epoch 119/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2310 - binary_accuracy: 0.9076 - auc: 0.9624 - val_loss: 0.2858 - val_binary_accuracy: 0.9104 - val_auc: 0.9697\n",
            "Epoch 120/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2345 - binary_accuracy: 0.9061 - auc: 0.9613 - val_loss: 0.2435 - val_binary_accuracy: 0.9170 - val_auc: 0.9708\n",
            "Epoch 121/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2320 - binary_accuracy: 0.9054 - auc: 0.9619 - val_loss: 0.2852 - val_binary_accuracy: 0.9054 - val_auc: 0.9697\n",
            "Epoch 122/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2361 - binary_accuracy: 0.9048 - auc: 0.9605 - val_loss: 0.2617 - val_binary_accuracy: 0.9033 - val_auc: 0.9701\n",
            "Epoch 123/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2291 - binary_accuracy: 0.9065 - auc: 0.9627 - val_loss: 0.2492 - val_binary_accuracy: 0.9120 - val_auc: 0.9706\n",
            "Epoch 124/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2289 - binary_accuracy: 0.9071 - auc: 0.9631 - val_loss: 0.2594 - val_binary_accuracy: 0.9108 - val_auc: 0.9713\n",
            "Epoch 125/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2302 - binary_accuracy: 0.9077 - auc: 0.9627 - val_loss: 0.2867 - val_binary_accuracy: 0.8741 - val_auc: 0.9701\n",
            "Epoch 126/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2302 - binary_accuracy: 0.9078 - auc: 0.9626 - val_loss: 0.2461 - val_binary_accuracy: 0.9048 - val_auc: 0.9686\n",
            "Epoch 127/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2292 - binary_accuracy: 0.9070 - auc: 0.9633 - val_loss: 0.2736 - val_binary_accuracy: 0.8941 - val_auc: 0.9694\n",
            "Epoch 128/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2302 - binary_accuracy: 0.9085 - auc: 0.9620 - val_loss: 0.2335 - val_binary_accuracy: 0.9189 - val_auc: 0.9711\n",
            "Epoch 129/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2321 - binary_accuracy: 0.9034 - auc: 0.9625 - val_loss: 0.2442 - val_binary_accuracy: 0.9131 - val_auc: 0.9720\n",
            "Epoch 130/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2311 - binary_accuracy: 0.9085 - auc: 0.9624 - val_loss: 0.3228 - val_binary_accuracy: 0.8840 - val_auc: 0.9713\n",
            "Epoch 131/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2386 - binary_accuracy: 0.9051 - auc: 0.9591 - val_loss: 0.2331 - val_binary_accuracy: 0.9110 - val_auc: 0.9690\n",
            "Epoch 132/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2379 - binary_accuracy: 0.9047 - auc: 0.9600 - val_loss: 0.3870 - val_binary_accuracy: 0.8130 - val_auc: 0.9696\n",
            "Epoch 133/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2347 - binary_accuracy: 0.9067 - auc: 0.9620 - val_loss: 0.3447 - val_binary_accuracy: 0.8721 - val_auc: 0.9724\n",
            "Epoch 134/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2288 - binary_accuracy: 0.9091 - auc: 0.9630 - val_loss: 0.2639 - val_binary_accuracy: 0.9035 - val_auc: 0.9688\n",
            "Epoch 135/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2282 - binary_accuracy: 0.9079 - auc: 0.9628 - val_loss: 0.2801 - val_binary_accuracy: 0.9067 - val_auc: 0.9705\n",
            "Epoch 136/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2302 - binary_accuracy: 0.9084 - auc: 0.9622 - val_loss: 0.2854 - val_binary_accuracy: 0.9086 - val_auc: 0.9699\n",
            "Epoch 137/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2375 - binary_accuracy: 0.9059 - auc: 0.9597 - val_loss: 0.3553 - val_binary_accuracy: 0.8438 - val_auc: 0.9704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "oOou8wmV6dEW",
        "outputId": "4fb3f17a-1daf-40ea-fe7e-72c0073248d0"
      },
      "source": [
        "history_df1 = pd.DataFrame(history1.history)\n",
        "\n",
        "history_df1.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>137.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>137.000000</td>\n",
              "      <td>137.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.265472</td>\n",
              "      <td>0.890208</td>\n",
              "      <td>0.946228</td>\n",
              "      <td>0.300549</td>\n",
              "      <td>0.892908</td>\n",
              "      <td>0.958388</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.065390</td>\n",
              "      <td>0.039583</td>\n",
              "      <td>0.041134</td>\n",
              "      <td>0.071232</td>\n",
              "      <td>0.038153</td>\n",
              "      <td>0.035725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.227534</td>\n",
              "      <td>0.655235</td>\n",
              "      <td>0.693803</td>\n",
              "      <td>0.215015</td>\n",
              "      <td>0.719593</td>\n",
              "      <td>0.762248</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.235851</td>\n",
              "      <td>0.896517</td>\n",
              "      <td>0.953038</td>\n",
              "      <td>0.252375</td>\n",
              "      <td>0.892309</td>\n",
              "      <td>0.963606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.243347</td>\n",
              "      <td>0.902469</td>\n",
              "      <td>0.958443</td>\n",
              "      <td>0.285353</td>\n",
              "      <td>0.907701</td>\n",
              "      <td>0.968473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.258374</td>\n",
              "      <td>0.905219</td>\n",
              "      <td>0.960552</td>\n",
              "      <td>0.322458</td>\n",
              "      <td>0.911912</td>\n",
              "      <td>0.969862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.597459</td>\n",
              "      <td>0.909216</td>\n",
              "      <td>0.963387</td>\n",
              "      <td>0.588668</td>\n",
              "      <td>0.920235</td>\n",
              "      <td>0.972425</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             loss  binary_accuracy  ...  val_binary_accuracy     val_auc\n",
              "count  137.000000       137.000000  ...           137.000000  137.000000\n",
              "mean     0.265472         0.890208  ...             0.892908    0.958388\n",
              "std      0.065390         0.039583  ...             0.038153    0.035725\n",
              "min      0.227534         0.655235  ...             0.719593    0.762248\n",
              "25%      0.235851         0.896517  ...             0.892309    0.963606\n",
              "50%      0.243347         0.902469  ...             0.907701    0.968473\n",
              "75%      0.258374         0.905219  ...             0.911912    0.969862\n",
              "max      0.597459         0.909216  ...             0.920235    0.972425\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 234
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V52xwKrN6dEW",
        "outputId": "71766908-d753-40ee-943b-b98939aa167f"
      },
      "source": [
        "train_acc = top_25_nn.evaluate(train_X, train_y)\n",
        "test_acc = top_25_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 2ms/step - loss: 0.2213 - binary_accuracy: 0.9197 - auc: 0.9704\n",
            "693/693 [==============================] - 1s 2ms/step - loss: 0.2246 - binary_accuracy: 0.9197 - auc: 0.9697\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cu49yBLl6dEW",
        "outputId": "b981b1aa-1ce6-4254-ca66-131adcec813d"
      },
      "source": [
        "dict(zip(top_25_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.969747006893158,\n",
              " 'binary_accuracy': 0.9196823239326477,\n",
              " 'loss': 0.22459127008914948}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "7KR5fTnM6dEW",
        "outputId": "46520a3e-19b8-4b29-fa2d-b85382c6cce6"
      },
      "source": [
        "history_df1.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df1['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.21501503884792328\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "uvreUWCX6dEW",
        "outputId": "6afb1d27-5ff5-4f60-8eb9-401154ed3f77"
      },
      "source": [
        "history_df1.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df1['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.9724253416061401\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Cs4WcnRK6dEX",
        "outputId": "4fdd7c3c-00e7-470d-f6c9-569b9a4016d2"
      },
      "source": [
        "history_df1.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df1['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.9202346205711365\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b54OSRyr6dEX",
        "outputId": "fef50c4a-266b-424b-c75d-cbfa88286351"
      },
      "source": [
        "history_df1.binary_accuracy.max() - history_df1.val_binary_accuracy.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.011018693447113037"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 240
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0GV6Ymb6dEY",
        "outputId": "b52e09da-b9ff-46f2-988e-ecdb886113c5"
      },
      "source": [
        "preds1 = top_25_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9255438 ],\n",
              "       [0.00619072],\n",
              "       [0.47685236],\n",
              "       ...,\n",
              "       [0.1934138 ],\n",
              "       [0.02817941],\n",
              "       [0.9255438 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vf8Qijhz6dEY",
        "outputId": "8fa2ff1b-7919-4768-c4c3-a933749a6aef"
      },
      "source": [
        "len(preds1[preds1 <= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 242
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmA3nL1_6dEY",
        "outputId": "4ac3a645-c993-4c01-b281-aaf90cd894b5"
      },
      "source": [
        "len(preds1[preds1 > 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8615"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 243
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OZdbeiN6dEY",
        "outputId": "734be7f0-16ec-4d4d-f837-aa9470fd31ae"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "fJ5COt256dEY",
        "outputId": "9887bee1-dd42-4ba6-c304-3fec06f4909a"
      },
      "source": [
        "preds_df = pd.DataFrame(preds1, columns = ['preds'])\n",
        "\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.925544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.006191</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.476852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.041735</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.631248</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      preds\n",
              "0  0.925544\n",
              "1  0.006191\n",
              "2  0.476852\n",
              "3  0.041735\n",
              "4  0.631248"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "iVGKSEE66dEY",
        "outputId": "fb31be5f-c3bc-4249-eaa7-d39b3a6528ba"
      },
      "source": [
        "preds_df = pd.concat([preds_df, val_y.reset_index(drop=True), val_X.reset_index()], axis=1)\n",
        "\n",
        "preds_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>phishing</th>\n",
              "      <th>index</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>qty_dot_directory</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>qty_at_directory</th>\n",
              "      <th>qty_comma_directory</th>\n",
              "      <th>qty_asterisk_directory</th>\n",
              "      <th>qty_hashtag_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_dot_file</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>qty_at_file</th>\n",
              "      <th>qty_exclamation_file</th>\n",
              "      <th>qty_space_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>time_response</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>ttl_hostname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.925544</td>\n",
              "      <td>1</td>\n",
              "      <td>62575</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.003517</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002345</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.138820</td>\n",
              "      <td>0.037377</td>\n",
              "      <td>0.989602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.006191</td>\n",
              "      <td>0</td>\n",
              "      <td>38126</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.786588</td>\n",
              "      <td>0.260471</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.559770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.476852</td>\n",
              "      <td>0</td>\n",
              "      <td>1617</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.921059</td>\n",
              "      <td>0.388661</td>\n",
              "      <td>0.014919</td>\n",
              "      <td>0.018994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.041735</td>\n",
              "      <td>0</td>\n",
              "      <td>8228</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.948421</td>\n",
              "      <td>0.119534</td>\n",
              "      <td>0.021672</td>\n",
              "      <td>0.292813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.631248</td>\n",
              "      <td>1</td>\n",
              "      <td>55594</td>\n",
              "      <td>0.055728</td>\n",
              "      <td>0.780189</td>\n",
              "      <td>0.027864</td>\n",
              "      <td>0.390095</td>\n",
              "      <td>0.027864</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027864</td>\n",
              "      <td>0.055728</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.390095</td>\n",
              "      <td>0.027864</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.278639</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22157</th>\n",
              "      <td>0.000008</td>\n",
              "      <td>0</td>\n",
              "      <td>65294</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001480</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.001480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.684142</td>\n",
              "      <td>0.658984</td>\n",
              "      <td>0.008618</td>\n",
              "      <td>0.312430</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22158</th>\n",
              "      <td>0.001727</td>\n",
              "      <td>0</td>\n",
              "      <td>10038</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.891957</td>\n",
              "      <td>0.419926</td>\n",
              "      <td>0.166352</td>\n",
              "      <td>0.020000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22159</th>\n",
              "      <td>0.193414</td>\n",
              "      <td>0</td>\n",
              "      <td>43642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001710</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.001710</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.999761</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.021711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22160</th>\n",
              "      <td>0.028179</td>\n",
              "      <td>0</td>\n",
              "      <td>73632</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.838648</td>\n",
              "      <td>0.166678</td>\n",
              "      <td>0.008666</td>\n",
              "      <td>0.518471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22161</th>\n",
              "      <td>0.925544</td>\n",
              "      <td>1</td>\n",
              "      <td>25895</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.954190</td>\n",
              "      <td>0.051122</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.294799</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22162 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          preds  phishing  ...  time_domain_expiration  ttl_hostname\n",
              "0      0.925544         1  ...                0.037377      0.989602\n",
              "1      0.006191         0  ...                0.009454      0.559770\n",
              "2      0.476852         0  ...                0.014919      0.018994\n",
              "3      0.041735         0  ...                0.021672      0.292813\n",
              "4      0.631248         1  ...                0.000000      0.000000\n",
              "...         ...       ...  ...                     ...           ...\n",
              "22157  0.000008         0  ...                0.008618      0.312430\n",
              "22158  0.001727         0  ...                0.166352      0.020000\n",
              "22159  0.193414         0  ...                0.000000      0.021711\n",
              "22160  0.028179         0  ...                0.008666      0.518471\n",
              "22161  0.925544         1  ...                0.001208      0.294799\n",
              "\n",
              "[22162 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QymzReM-6dEZ",
        "outputId": "7fb4a14d-7b79-4be4-d15a-de01fe061163"
      },
      "source": [
        "pred_classes = np.argmax(preds1, axis=1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(val_y, pred_classes)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14519     0]\n",
            " [ 7643     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PO5Qlco9oVi7"
      },
      "source": [
        "# neural network on top 25 most important features per recursive feature elimination package [model as feature]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "IJG4NbzYt9Yn",
        "outputId": "778023a0-3352-4e2c-f4f0-f83e36a546a2"
      },
      "source": [
        "y = full_df.iloc[:,-1]\n",
        "\n",
        "features = ['qty_slash_url', 'length_url', 'qty_dot_domain', 'domain_length',\n",
        "       'qty_dot_directory', 'qty_hyphen_directory', 'qty_underline_directory',\n",
        "       'qty_slash_directory', 'qty_at_directory', 'qty_comma_directory',\n",
        "       'qty_asterisk_directory', 'qty_hashtag_directory', 'directory_length',\n",
        "       'qty_dot_file', 'qty_hyphen_file', 'qty_at_file',\n",
        "       'qty_exclamation_file', 'qty_space_file', 'qty_dollar_file',\n",
        "       'file_length', 'time_response', 'asn_ip', 'time_domain_activation',\n",
        "       'time_domain_expiration', 'ttl_hostname']\n",
        "X = full_df[features]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.25, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>qty_dot_directory</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>qty_at_directory</th>\n",
              "      <th>qty_comma_directory</th>\n",
              "      <th>qty_asterisk_directory</th>\n",
              "      <th>qty_hashtag_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_dot_file</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>qty_at_file</th>\n",
              "      <th>qty_exclamation_file</th>\n",
              "      <th>qty_space_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>time_response</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>ttl_hostname</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.999934</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.011480</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.730211</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.680200</td>\n",
              "      <td>0.064004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.698307</td>\n",
              "      <td>0.269674</td>\n",
              "      <td>0.016112</td>\n",
              "      <td>0.662860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.859058</td>\n",
              "      <td>0.261792</td>\n",
              "      <td>0.005926</td>\n",
              "      <td>0.439823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.795440</td>\n",
              "      <td>0.178787</td>\n",
              "      <td>0.007212</td>\n",
              "      <td>0.579014</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_slash_url  length_url  ...  time_domain_expiration  ttl_hostname\n",
              "5676        0.000004    0.000042  ...                0.000797      0.011480\n",
              "39002       0.000000    0.003369  ...                0.680200      0.064004\n",
              "1732        0.000000    0.000737  ...                0.016112      0.662860\n",
              "39668       0.000000    0.001588  ...                0.005926      0.439823\n",
              "82035       0.000000    0.000751  ...                0.007212      0.579014\n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 248
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZye1U4CoVi_"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_top25():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[25]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StIxLEd0oVi_"
      },
      "source": [
        "mod_top25 = KerasClassifier(build_fn=phish_nn_top25,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPKTk5WkoVi_",
        "outputId": "eb219618-c2e2-497e-c7ba-d622aae5fb87"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_top25 = cross_val_score(mod_top25,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6254 - accuracy: 0.6462 - val_loss: 0.5668 - val_accuracy: 0.7192\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5590 - accuracy: 0.7064 - val_loss: 0.6071 - val_accuracy: 0.7219\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5469 - accuracy: 0.7184 - val_loss: 0.5618 - val_accuracy: 0.7266\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5416 - accuracy: 0.7186 - val_loss: 0.5276 - val_accuracy: 0.7342\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5297 - accuracy: 0.7248 - val_loss: 0.5220 - val_accuracy: 0.7491\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5059 - accuracy: 0.7440 - val_loss: 0.4728 - val_accuracy: 0.7688\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4554 - accuracy: 0.7817 - val_loss: 0.4314 - val_accuracy: 0.8351\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4159 - accuracy: 0.8137 - val_loss: 0.5177 - val_accuracy: 0.7590\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3950 - accuracy: 0.8238 - val_loss: 0.3720 - val_accuracy: 0.8639\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3554 - accuracy: 0.8540 - val_loss: 0.3560 - val_accuracy: 0.8673\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3490 - accuracy: 0.8596 - val_loss: 0.3262 - val_accuracy: 0.8838\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3239 - accuracy: 0.8712 - val_loss: 0.3406 - val_accuracy: 0.8739\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3221 - accuracy: 0.8699 - val_loss: 0.3344 - val_accuracy: 0.8870\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3127 - accuracy: 0.8732 - val_loss: 0.3877 - val_accuracy: 0.8859\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3026 - accuracy: 0.8783 - val_loss: 0.3470 - val_accuracy: 0.8987\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3041 - accuracy: 0.8738 - val_loss: 0.3916 - val_accuracy: 0.8214\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3062 - accuracy: 0.8745 - val_loss: 0.3120 - val_accuracy: 0.8952\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2927 - accuracy: 0.8810 - val_loss: 0.3958 - val_accuracy: 0.8149\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2922 - accuracy: 0.8816 - val_loss: 0.3767 - val_accuracy: 0.8555\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2884 - accuracy: 0.8809 - val_loss: 0.3492 - val_accuracy: 0.8464\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2810 - accuracy: 0.8848 - val_loss: 0.3862 - val_accuracy: 0.8832\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2734 - accuracy: 0.8900 - val_loss: 0.3080 - val_accuracy: 0.9126\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2793 - accuracy: 0.8889 - val_loss: 0.3333 - val_accuracy: 0.9039\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2732 - accuracy: 0.8901 - val_loss: 0.2965 - val_accuracy: 0.9042\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2733 - accuracy: 0.8888 - val_loss: 0.2544 - val_accuracy: 0.8912\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2675 - accuracy: 0.8917 - val_loss: 0.2872 - val_accuracy: 0.9088\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2697 - accuracy: 0.8920 - val_loss: 0.2744 - val_accuracy: 0.9140\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2638 - accuracy: 0.8962 - val_loss: 0.2693 - val_accuracy: 0.9143\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2740 - accuracy: 0.8902 - val_loss: 0.3111 - val_accuracy: 0.9087\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2606 - accuracy: 0.8950 - val_loss: 0.2852 - val_accuracy: 0.9116\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2606 - accuracy: 0.8930 - val_loss: 0.2791 - val_accuracy: 0.9008\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8963 - val_loss: 0.2734 - val_accuracy: 0.9077\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2663 - accuracy: 0.8945 - val_loss: 0.2565 - val_accuracy: 0.8995\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2540 - accuracy: 0.8985 - val_loss: 0.3089 - val_accuracy: 0.9062\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2579 - accuracy: 0.8956 - val_loss: 0.2573 - val_accuracy: 0.9112\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2487 - accuracy: 0.9000 - val_loss: 0.3569 - val_accuracy: 0.8683\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2518 - accuracy: 0.8983 - val_loss: 0.4600 - val_accuracy: 0.8267\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8973 - val_loss: 0.2582 - val_accuracy: 0.9093\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8979 - val_loss: 0.2631 - val_accuracy: 0.9104\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2511 - accuracy: 0.8988 - val_loss: 0.2515 - val_accuracy: 0.9132\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2448 - accuracy: 0.9016 - val_loss: 0.2571 - val_accuracy: 0.9091\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2475 - accuracy: 0.8982 - val_loss: 0.2637 - val_accuracy: 0.9144\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8968 - val_loss: 0.2847 - val_accuracy: 0.9110\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2537 - accuracy: 0.8973 - val_loss: 0.3072 - val_accuracy: 0.9036\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2505 - accuracy: 0.8969 - val_loss: 0.3857 - val_accuracy: 0.8212\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2475 - accuracy: 0.8989 - val_loss: 0.2878 - val_accuracy: 0.9130\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2478 - accuracy: 0.8989 - val_loss: 0.2631 - val_accuracy: 0.9135\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2463 - accuracy: 0.9021 - val_loss: 0.2906 - val_accuracy: 0.9039\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2458 - accuracy: 0.9019 - val_loss: 0.2457 - val_accuracy: 0.9104\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2350 - accuracy: 0.9053 - val_loss: 0.2731 - val_accuracy: 0.9082\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2747 - accuracy: 0.9075\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2473 - accuracy: 0.8991\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2449 - accuracy: 0.9026\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2451 - accuracy: 0.9015\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2462 - accuracy: 0.9005\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2444 - accuracy: 0.9028\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2446 - accuracy: 0.9019\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2443 - accuracy: 0.9019\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2431 - accuracy: 0.9019\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2432 - accuracy: 0.9012\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2416 - accuracy: 0.9030\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2782 - accuracy: 0.9151\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6189 - accuracy: 0.6540 - val_loss: 0.5604 - val_accuracy: 0.7194\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5577 - accuracy: 0.7079 - val_loss: 0.5696 - val_accuracy: 0.7178\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5470 - accuracy: 0.7179 - val_loss: 0.5748 - val_accuracy: 0.7259\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5421 - accuracy: 0.7198 - val_loss: 0.5480 - val_accuracy: 0.7320\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5291 - accuracy: 0.7249 - val_loss: 0.4962 - val_accuracy: 0.7511\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4846 - accuracy: 0.7566 - val_loss: 0.4254 - val_accuracy: 0.8012\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4440 - accuracy: 0.7859 - val_loss: 0.4327 - val_accuracy: 0.8076\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4233 - accuracy: 0.8052 - val_loss: 0.4403 - val_accuracy: 0.8000\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3927 - accuracy: 0.8242 - val_loss: 0.4717 - val_accuracy: 0.8173\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3644 - accuracy: 0.8468 - val_loss: 0.3707 - val_accuracy: 0.8817\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3503 - accuracy: 0.8549 - val_loss: 0.3708 - val_accuracy: 0.8856\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3463 - accuracy: 0.8598 - val_loss: 0.3099 - val_accuracy: 0.8784\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3331 - accuracy: 0.8643 - val_loss: 0.3504 - val_accuracy: 0.8870\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8671 - val_loss: 0.3798 - val_accuracy: 0.8765\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8657 - val_loss: 0.3151 - val_accuracy: 0.8949\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3072 - accuracy: 0.8736 - val_loss: 0.3809 - val_accuracy: 0.8881\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2957 - accuracy: 0.8821 - val_loss: 0.3906 - val_accuracy: 0.9064\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2901 - accuracy: 0.8845 - val_loss: 0.3462 - val_accuracy: 0.9059\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.8832 - val_loss: 0.3765 - val_accuracy: 0.8982\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2906 - accuracy: 0.8823 - val_loss: 0.2498 - val_accuracy: 0.9095\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2817 - accuracy: 0.8884 - val_loss: 0.3311 - val_accuracy: 0.9066\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2739 - accuracy: 0.8890 - val_loss: 0.2894 - val_accuracy: 0.9062\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2830 - accuracy: 0.8848 - val_loss: 0.3149 - val_accuracy: 0.9082\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2790 - accuracy: 0.8880 - val_loss: 0.2471 - val_accuracy: 0.8988\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2792 - accuracy: 0.8885 - val_loss: 0.2518 - val_accuracy: 0.9019\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2758 - accuracy: 0.8926 - val_loss: 0.3126 - val_accuracy: 0.8933\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8852 - val_loss: 0.2509 - val_accuracy: 0.9138\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2702 - accuracy: 0.8928 - val_loss: 0.2589 - val_accuracy: 0.9124\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2701 - accuracy: 0.8927 - val_loss: 0.4052 - val_accuracy: 0.8980\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8928 - val_loss: 0.3992 - val_accuracy: 0.8998\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8907 - val_loss: 0.3059 - val_accuracy: 0.9127\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2688 - accuracy: 0.8947 - val_loss: 0.4095 - val_accuracy: 0.8851\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2771 - accuracy: 0.8870 - val_loss: 0.2393 - val_accuracy: 0.9089\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8931 - val_loss: 0.2859 - val_accuracy: 0.9146\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2620 - accuracy: 0.8957 - val_loss: 0.2623 - val_accuracy: 0.9007\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2654 - accuracy: 0.8934 - val_loss: 0.2795 - val_accuracy: 0.9122\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8958 - val_loss: 0.3589 - val_accuracy: 0.9120\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2590 - accuracy: 0.8989 - val_loss: 0.2607 - val_accuracy: 0.9084\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8977 - val_loss: 0.3223 - val_accuracy: 0.9132\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2591 - accuracy: 0.8951 - val_loss: 0.2755 - val_accuracy: 0.9121\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8947 - val_loss: 0.2677 - val_accuracy: 0.9102\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8966 - val_loss: 0.2628 - val_accuracy: 0.9181\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2594 - accuracy: 0.8955 - val_loss: 0.2442 - val_accuracy: 0.9047\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2482 - accuracy: 0.9006 - val_loss: 0.2559 - val_accuracy: 0.9170\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2535 - accuracy: 0.9003 - val_loss: 0.2426 - val_accuracy: 0.9096\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2556 - accuracy: 0.8994 - val_loss: 0.2739 - val_accuracy: 0.9152\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8978 - val_loss: 0.2901 - val_accuracy: 0.9000\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2627 - accuracy: 0.8946 - val_loss: 0.2785 - val_accuracy: 0.9063\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.9008 - val_loss: 0.2560 - val_accuracy: 0.9102\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8982 - val_loss: 0.2898 - val_accuracy: 0.9135\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2907 - accuracy: 0.9130\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2589 - accuracy: 0.8982\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2574 - accuracy: 0.8973\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2573 - accuracy: 0.8977\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2538 - accuracy: 0.9004\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2524 - accuracy: 0.9000\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2487 - accuracy: 0.9017\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2516 - accuracy: 0.8999\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2545 - accuracy: 0.8989\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2489 - accuracy: 0.9020\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2487 - accuracy: 0.9009\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2605 - accuracy: 0.9160\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6271 - accuracy: 0.6523 - val_loss: 0.5790 - val_accuracy: 0.7180\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5648 - accuracy: 0.7008 - val_loss: 0.5770 - val_accuracy: 0.7228\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5530 - accuracy: 0.7088 - val_loss: 0.5836 - val_accuracy: 0.7269\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5447 - accuracy: 0.7152 - val_loss: 0.5456 - val_accuracy: 0.7215\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5411 - accuracy: 0.7147 - val_loss: 0.5328 - val_accuracy: 0.7265\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5295 - accuracy: 0.7260 - val_loss: 0.5076 - val_accuracy: 0.7544\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4957 - accuracy: 0.7481 - val_loss: 0.4915 - val_accuracy: 0.7737\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4494 - accuracy: 0.7826 - val_loss: 0.4214 - val_accuracy: 0.8374\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4137 - accuracy: 0.8129 - val_loss: 0.3832 - val_accuracy: 0.8266\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3909 - accuracy: 0.8300 - val_loss: 0.4378 - val_accuracy: 0.8305\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3705 - accuracy: 0.8380 - val_loss: 0.3926 - val_accuracy: 0.8683\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3336 - accuracy: 0.8648 - val_loss: 0.3523 - val_accuracy: 0.8941\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3270 - accuracy: 0.8677 - val_loss: 0.3352 - val_accuracy: 0.8886\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3226 - accuracy: 0.8693 - val_loss: 0.2853 - val_accuracy: 0.9008\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8695 - val_loss: 0.2968 - val_accuracy: 0.8894\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3033 - accuracy: 0.8781 - val_loss: 0.4291 - val_accuracy: 0.8907\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3017 - accuracy: 0.8779 - val_loss: 0.2774 - val_accuracy: 0.9087\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2967 - accuracy: 0.8819 - val_loss: 0.2838 - val_accuracy: 0.8815\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2960 - accuracy: 0.8793 - val_loss: 0.2832 - val_accuracy: 0.8874\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2911 - accuracy: 0.8823 - val_loss: 0.2640 - val_accuracy: 0.9127\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2742 - accuracy: 0.8919 - val_loss: 0.2952 - val_accuracy: 0.8976\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2838 - accuracy: 0.8855 - val_loss: 0.3125 - val_accuracy: 0.9089\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2778 - accuracy: 0.8916 - val_loss: 0.3599 - val_accuracy: 0.8933\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2817 - accuracy: 0.8863 - val_loss: 0.2633 - val_accuracy: 0.9056\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2705 - accuracy: 0.8927 - val_loss: 0.2598 - val_accuracy: 0.9137\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2782 - accuracy: 0.8882 - val_loss: 0.2456 - val_accuracy: 0.9135\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8949 - val_loss: 0.2920 - val_accuracy: 0.9098\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2705 - accuracy: 0.8905 - val_loss: 0.3135 - val_accuracy: 0.8853\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8865 - val_loss: 0.2658 - val_accuracy: 0.8928\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8944 - val_loss: 0.2520 - val_accuracy: 0.9087\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2610 - accuracy: 0.8966 - val_loss: 0.3294 - val_accuracy: 0.8580\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2736 - accuracy: 0.8888 - val_loss: 0.3130 - val_accuracy: 0.8581\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2680 - accuracy: 0.8913 - val_loss: 0.2368 - val_accuracy: 0.9179\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2681 - accuracy: 0.8933 - val_loss: 0.2372 - val_accuracy: 0.9115\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8949 - val_loss: 0.2779 - val_accuracy: 0.9105\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2756 - accuracy: 0.8912 - val_loss: 0.2494 - val_accuracy: 0.9073\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2746 - accuracy: 0.8889 - val_loss: 0.2476 - val_accuracy: 0.9129\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.8954 - val_loss: 0.2562 - val_accuracy: 0.9184\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2587 - accuracy: 0.8986 - val_loss: 0.2544 - val_accuracy: 0.9071\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2604 - accuracy: 0.8962 - val_loss: 0.3347 - val_accuracy: 0.9057\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8960 - val_loss: 0.2708 - val_accuracy: 0.9123\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8933 - val_loss: 0.5541 - val_accuracy: 0.8857\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8951 - val_loss: 0.2658 - val_accuracy: 0.9116\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8965 - val_loss: 0.3309 - val_accuracy: 0.8680\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2572 - accuracy: 0.9006 - val_loss: 0.2913 - val_accuracy: 0.9017\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2602 - accuracy: 0.8973 - val_loss: 0.2547 - val_accuracy: 0.9128\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8988 - val_loss: 0.2727 - val_accuracy: 0.9143\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2624 - accuracy: 0.8957 - val_loss: 0.2805 - val_accuracy: 0.8924\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2599 - accuracy: 0.8967 - val_loss: 0.2435 - val_accuracy: 0.9125\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2566 - accuracy: 0.8990 - val_loss: 0.2228 - val_accuracy: 0.9138\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2263 - accuracy: 0.9126\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2494 - accuracy: 0.9013\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2541 - accuracy: 0.8997\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2485 - accuracy: 0.8999\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2480 - accuracy: 0.9015\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2494 - accuracy: 0.9004\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2494 - accuracy: 0.9008\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2474 - accuracy: 0.9033\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2498 - accuracy: 0.9008\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2489 - accuracy: 0.9016\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2468 - accuracy: 0.9018\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2557 - accuracy: 0.9007\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6192 - accuracy: 0.6492 - val_loss: 0.5639 - val_accuracy: 0.7227\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5546 - accuracy: 0.7121 - val_loss: 0.5661 - val_accuracy: 0.7223\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5475 - accuracy: 0.7171 - val_loss: 0.5912 - val_accuracy: 0.7150\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5382 - accuracy: 0.7214 - val_loss: 0.5577 - val_accuracy: 0.7266\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5244 - accuracy: 0.7306 - val_loss: 0.5127 - val_accuracy: 0.7518\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4906 - accuracy: 0.7510 - val_loss: 0.4928 - val_accuracy: 0.7717\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4462 - accuracy: 0.7870 - val_loss: 0.5180 - val_accuracy: 0.7894\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4032 - accuracy: 0.8204 - val_loss: 0.3998 - val_accuracy: 0.8618\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3822 - accuracy: 0.8319 - val_loss: 0.5495 - val_accuracy: 0.8069\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3452 - accuracy: 0.8607 - val_loss: 0.3861 - val_accuracy: 0.8890\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3285 - accuracy: 0.8689 - val_loss: 0.4081 - val_accuracy: 0.8767\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3305 - accuracy: 0.8699 - val_loss: 0.3285 - val_accuracy: 0.9005\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3112 - accuracy: 0.8761 - val_loss: 0.3685 - val_accuracy: 0.9028\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3100 - accuracy: 0.8753 - val_loss: 0.3463 - val_accuracy: 0.8867\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2918 - accuracy: 0.8815 - val_loss: 0.3558 - val_accuracy: 0.8963\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2961 - accuracy: 0.8796 - val_loss: 0.2664 - val_accuracy: 0.8951\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2956 - accuracy: 0.8796 - val_loss: 0.4916 - val_accuracy: 0.7918\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2896 - accuracy: 0.8850 - val_loss: 0.2816 - val_accuracy: 0.9099\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8861 - val_loss: 0.4236 - val_accuracy: 0.8163\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2931 - accuracy: 0.8816 - val_loss: 0.2828 - val_accuracy: 0.8954\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2708 - accuracy: 0.8927 - val_loss: 0.4641 - val_accuracy: 0.8464\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2729 - accuracy: 0.8911 - val_loss: 0.3967 - val_accuracy: 0.8504\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2633 - accuracy: 0.8934 - val_loss: 0.2355 - val_accuracy: 0.9063\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2780 - accuracy: 0.8872 - val_loss: 0.2779 - val_accuracy: 0.9101\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8907 - val_loss: 0.2573 - val_accuracy: 0.9121\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2724 - accuracy: 0.8903 - val_loss: 0.2483 - val_accuracy: 0.9075\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8949 - val_loss: 0.3863 - val_accuracy: 0.8604\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2724 - accuracy: 0.8901 - val_loss: 0.3114 - val_accuracy: 0.8914\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2571 - accuracy: 0.8985 - val_loss: 0.2961 - val_accuracy: 0.9067\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.8985 - val_loss: 0.2529 - val_accuracy: 0.9042\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2580 - accuracy: 0.8963 - val_loss: 0.2926 - val_accuracy: 0.9156\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2521 - accuracy: 0.8977 - val_loss: 0.3028 - val_accuracy: 0.9089\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2531 - accuracy: 0.9010 - val_loss: 0.2771 - val_accuracy: 0.9077\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2597 - accuracy: 0.8935 - val_loss: 0.2826 - val_accuracy: 0.9092\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8935 - val_loss: 0.2879 - val_accuracy: 0.9049\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.8971 - val_loss: 0.2859 - val_accuracy: 0.9036\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2535 - accuracy: 0.9004 - val_loss: 0.3137 - val_accuracy: 0.9108\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.8987 - val_loss: 0.3171 - val_accuracy: 0.8860\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2629 - accuracy: 0.8951 - val_loss: 0.2932 - val_accuracy: 0.9165\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2540 - accuracy: 0.8945 - val_loss: 0.2346 - val_accuracy: 0.9177\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8927 - val_loss: 0.2641 - val_accuracy: 0.9095\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8982 - val_loss: 0.2700 - val_accuracy: 0.9161\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2514 - accuracy: 0.9008 - val_loss: 0.2432 - val_accuracy: 0.9161\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2475 - accuracy: 0.8995 - val_loss: 0.3105 - val_accuracy: 0.9147\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.9024 - val_loss: 0.2555 - val_accuracy: 0.9079\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2440 - accuracy: 0.9018 - val_loss: 0.2414 - val_accuracy: 0.9087\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2630 - accuracy: 0.8947 - val_loss: 0.2925 - val_accuracy: 0.9137\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2486 - accuracy: 0.9003 - val_loss: 0.2557 - val_accuracy: 0.9111\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2488 - accuracy: 0.8991 - val_loss: 0.2551 - val_accuracy: 0.9073\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2432 - accuracy: 0.9042 - val_loss: 0.2516 - val_accuracy: 0.9106\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2546 - accuracy: 0.9072\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2550 - accuracy: 0.8987\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.8991\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2463 - accuracy: 0.9016\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2486 - accuracy: 0.8985\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2492 - accuracy: 0.8993\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2511 - accuracy: 0.8995\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2454 - accuracy: 0.9020\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2486 - accuracy: 0.8992\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2423 - accuracy: 0.9026\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2424 - accuracy: 0.9027\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.4106 - accuracy: 0.8847\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6336 - accuracy: 0.6449 - val_loss: 0.5620 - val_accuracy: 0.7204\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5582 - accuracy: 0.6941 - val_loss: 0.5819 - val_accuracy: 0.7127\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5473 - accuracy: 0.7082 - val_loss: 0.5616 - val_accuracy: 0.7245\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5412 - accuracy: 0.7140 - val_loss: 0.5491 - val_accuracy: 0.7285\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5296 - accuracy: 0.7242 - val_loss: 0.4880 - val_accuracy: 0.7567\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4830 - accuracy: 0.7602 - val_loss: 0.4326 - val_accuracy: 0.7908\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4466 - accuracy: 0.7902 - val_loss: 0.4940 - val_accuracy: 0.7838\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4016 - accuracy: 0.8229 - val_loss: 0.3822 - val_accuracy: 0.8637\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3771 - accuracy: 0.8397 - val_loss: 0.5041 - val_accuracy: 0.8650\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3527 - accuracy: 0.8545 - val_loss: 0.3832 - val_accuracy: 0.8838\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3358 - accuracy: 0.8630 - val_loss: 0.5601 - val_accuracy: 0.7965\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3346 - accuracy: 0.8642 - val_loss: 0.3668 - val_accuracy: 0.8961\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3154 - accuracy: 0.8732 - val_loss: 0.5256 - val_accuracy: 0.7986\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3104 - accuracy: 0.8724 - val_loss: 0.4042 - val_accuracy: 0.8841\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3120 - accuracy: 0.8742 - val_loss: 0.3725 - val_accuracy: 0.8943\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3006 - accuracy: 0.8790 - val_loss: 0.4408 - val_accuracy: 0.8933\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3065 - accuracy: 0.8735 - val_loss: 0.3068 - val_accuracy: 0.9033\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2825 - accuracy: 0.8864 - val_loss: 0.3004 - val_accuracy: 0.8933\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2862 - accuracy: 0.8842 - val_loss: 0.3873 - val_accuracy: 0.8644\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2904 - accuracy: 0.8821 - val_loss: 0.3512 - val_accuracy: 0.9073\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2707 - accuracy: 0.8916 - val_loss: 0.2814 - val_accuracy: 0.9056\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2822 - accuracy: 0.8866 - val_loss: 0.2943 - val_accuracy: 0.8924\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2794 - accuracy: 0.8864 - val_loss: 0.2839 - val_accuracy: 0.9097\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2776 - accuracy: 0.8872 - val_loss: 0.2502 - val_accuracy: 0.9049\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2787 - accuracy: 0.8894 - val_loss: 0.3368 - val_accuracy: 0.9051\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2791 - accuracy: 0.8873 - val_loss: 0.3758 - val_accuracy: 0.9023\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8905 - val_loss: 0.3566 - val_accuracy: 0.9041\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2700 - accuracy: 0.8923 - val_loss: 0.2669 - val_accuracy: 0.9082\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2708 - accuracy: 0.8915 - val_loss: 0.3422 - val_accuracy: 0.9104\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2717 - accuracy: 0.8915 - val_loss: 0.3122 - val_accuracy: 0.9101\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2717 - accuracy: 0.8906 - val_loss: 0.2810 - val_accuracy: 0.9129\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8945 - val_loss: 0.3031 - val_accuracy: 0.9102\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8961 - val_loss: 0.3165 - val_accuracy: 0.9063\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2709 - accuracy: 0.8908 - val_loss: 0.2646 - val_accuracy: 0.8908\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2647 - accuracy: 0.8934 - val_loss: 0.3009 - val_accuracy: 0.9082\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2604 - accuracy: 0.8991 - val_loss: 0.2331 - val_accuracy: 0.9089\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8923 - val_loss: 0.3181 - val_accuracy: 0.9102\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2670 - accuracy: 0.8933 - val_loss: 0.2705 - val_accuracy: 0.9049\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2527 - accuracy: 0.9006 - val_loss: 0.3232 - val_accuracy: 0.9098\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2570 - accuracy: 0.8963 - val_loss: 0.2833 - val_accuracy: 0.9121\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2583 - accuracy: 0.8974 - val_loss: 0.2915 - val_accuracy: 0.9142\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2626 - accuracy: 0.8951 - val_loss: 0.3160 - val_accuracy: 0.8998\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8999 - val_loss: 0.3077 - val_accuracy: 0.9061\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2556 - accuracy: 0.8977 - val_loss: 0.3015 - val_accuracy: 0.8996\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8939 - val_loss: 0.3408 - val_accuracy: 0.8918\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2496 - accuracy: 0.8984 - val_loss: 0.2745 - val_accuracy: 0.9111\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2538 - accuracy: 0.8977 - val_loss: 0.3867 - val_accuracy: 0.8479\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2557 - accuracy: 0.8986 - val_loss: 0.2840 - val_accuracy: 0.9148\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2554 - accuracy: 0.8972 - val_loss: 0.2971 - val_accuracy: 0.9082\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2552 - accuracy: 0.8971 - val_loss: 0.3239 - val_accuracy: 0.9083\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3249 - accuracy: 0.9067\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2543 - accuracy: 0.8980\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2524 - accuracy: 0.8997\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2484 - accuracy: 0.8998\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2610 - accuracy: 0.8953\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2471 - accuracy: 0.8994\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2481 - accuracy: 0.9004\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2442 - accuracy: 0.9014\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2493 - accuracy: 0.9012\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2423 - accuracy: 0.9023\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2506 - accuracy: 0.9013\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.3302 - accuracy: 0.8892\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V537WBvroVjA",
        "outputId": "63ed1251-5fd2-4e75-c1e7-97e8311def33"
      },
      "source": [
        "print(round(cv_results_top25.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9011\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzL7jYtMoVjA",
        "outputId": "cf3940b1-b3e3-4556-df5c-b05b6c3beaaf"
      },
      "source": [
        "print(round(cv_results_top25.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0129\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM5Q-AaUuRwm",
        "outputId": "e3eeac3d-b143-489d-d763-79302702190e"
      },
      "source": [
        "cv_results_top25"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.91511559, 0.91596162, 0.90067124, 0.88470864, 0.88922107])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 261
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k73OAacUoVjA",
        "outputId": "9acec75d-e67c-48be-e936-ef5734046859"
      },
      "source": [
        "cv_top25_preds = cross_val_predict(mod_top25, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6290 - accuracy: 0.6461 - val_loss: 0.5721 - val_accuracy: 0.7201\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5616 - accuracy: 0.7018 - val_loss: 0.5727 - val_accuracy: 0.7241\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5504 - accuracy: 0.7146 - val_loss: 0.5727 - val_accuracy: 0.7220\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5460 - accuracy: 0.7161 - val_loss: 0.5572 - val_accuracy: 0.7289\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5396 - accuracy: 0.7199 - val_loss: 0.5429 - val_accuracy: 0.7298\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5250 - accuracy: 0.7274 - val_loss: 0.5336 - val_accuracy: 0.7370\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4985 - accuracy: 0.7514 - val_loss: 0.4696 - val_accuracy: 0.7823\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4474 - accuracy: 0.7905 - val_loss: 0.4546 - val_accuracy: 0.8327\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4027 - accuracy: 0.8233 - val_loss: 0.4188 - val_accuracy: 0.8719\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3724 - accuracy: 0.8467 - val_loss: 0.4544 - val_accuracy: 0.8413\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3600 - accuracy: 0.8516 - val_loss: 0.4145 - val_accuracy: 0.8696\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3458 - accuracy: 0.8590 - val_loss: 0.3695 - val_accuracy: 0.8780\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3387 - accuracy: 0.8592 - val_loss: 0.3482 - val_accuracy: 0.8811\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3202 - accuracy: 0.8683 - val_loss: 0.3512 - val_accuracy: 0.8849\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3134 - accuracy: 0.8704 - val_loss: 0.3005 - val_accuracy: 0.9024\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2983 - accuracy: 0.8781 - val_loss: 0.3363 - val_accuracy: 0.8882\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2940 - accuracy: 0.8808 - val_loss: 0.2951 - val_accuracy: 0.8962\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2963 - accuracy: 0.8773 - val_loss: 0.3447 - val_accuracy: 0.8972\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2951 - accuracy: 0.8816 - val_loss: 0.3320 - val_accuracy: 0.8934\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2834 - accuracy: 0.8847 - val_loss: 0.2830 - val_accuracy: 0.8934\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2864 - accuracy: 0.8821 - val_loss: 0.2864 - val_accuracy: 0.9121\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.8882 - val_loss: 0.4652 - val_accuracy: 0.8142\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2885 - accuracy: 0.8815 - val_loss: 0.3246 - val_accuracy: 0.8989\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2901 - accuracy: 0.8831 - val_loss: 0.3313 - val_accuracy: 0.8924\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2893 - accuracy: 0.8820 - val_loss: 0.3233 - val_accuracy: 0.8875\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2741 - accuracy: 0.8887 - val_loss: 0.3210 - val_accuracy: 0.8963\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2804 - accuracy: 0.8854 - val_loss: 0.2810 - val_accuracy: 0.9060\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2755 - accuracy: 0.8884 - val_loss: 0.3839 - val_accuracy: 0.8903\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2800 - accuracy: 0.8868 - val_loss: 0.3047 - val_accuracy: 0.8861\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2777 - accuracy: 0.8882 - val_loss: 0.2531 - val_accuracy: 0.8945\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2858 - accuracy: 0.8848 - val_loss: 0.3090 - val_accuracy: 0.8999\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8915 - val_loss: 0.2670 - val_accuracy: 0.8972\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2676 - accuracy: 0.8908 - val_loss: 0.2993 - val_accuracy: 0.9031\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8938 - val_loss: 0.3224 - val_accuracy: 0.9100\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2677 - accuracy: 0.8911 - val_loss: 0.3037 - val_accuracy: 0.9113\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2617 - accuracy: 0.8944 - val_loss: 0.3984 - val_accuracy: 0.8874\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2882 - accuracy: 0.9093\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2866 - accuracy: 0.8826\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2814 - accuracy: 0.8855\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2794 - accuracy: 0.8872\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2790 - accuracy: 0.8867\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2737 - accuracy: 0.8889\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2747 - accuracy: 0.8895\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2728 - accuracy: 0.8899\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2698 - accuracy: 0.8909\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2616 - accuracy: 0.8942\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2689 - accuracy: 0.8912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6222 - accuracy: 0.6540 - val_loss: 0.5529 - val_accuracy: 0.7216\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5582 - accuracy: 0.7067 - val_loss: 0.5718 - val_accuracy: 0.7237\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5437 - accuracy: 0.7208 - val_loss: 0.5397 - val_accuracy: 0.7284\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5387 - accuracy: 0.7230 - val_loss: 0.5392 - val_accuracy: 0.7314\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5245 - accuracy: 0.7301 - val_loss: 0.5027 - val_accuracy: 0.7494\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4922 - accuracy: 0.7547 - val_loss: 0.4720 - val_accuracy: 0.7785\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4502 - accuracy: 0.7847 - val_loss: 0.4108 - val_accuracy: 0.8135\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4104 - accuracy: 0.8182 - val_loss: 0.4407 - val_accuracy: 0.8158\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3937 - accuracy: 0.8271 - val_loss: 0.3641 - val_accuracy: 0.8726\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3623 - accuracy: 0.8473 - val_loss: 0.3271 - val_accuracy: 0.8771\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3462 - accuracy: 0.8599 - val_loss: 0.3366 - val_accuracy: 0.8716\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3365 - accuracy: 0.8621 - val_loss: 0.3501 - val_accuracy: 0.8831\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3316 - accuracy: 0.8651 - val_loss: 0.3100 - val_accuracy: 0.8875\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3136 - accuracy: 0.8710 - val_loss: 0.3380 - val_accuracy: 0.8964\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3064 - accuracy: 0.8734 - val_loss: 0.4062 - val_accuracy: 0.8707\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3032 - accuracy: 0.8759 - val_loss: 0.4505 - val_accuracy: 0.8311\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3057 - accuracy: 0.8744 - val_loss: 0.2704 - val_accuracy: 0.8972\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2947 - accuracy: 0.8795 - val_loss: 0.3041 - val_accuracy: 0.8987\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2865 - accuracy: 0.8825 - val_loss: 0.2877 - val_accuracy: 0.8970\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2881 - accuracy: 0.8817 - val_loss: 0.2854 - val_accuracy: 0.8959\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2936 - accuracy: 0.8810 - val_loss: 0.2988 - val_accuracy: 0.9078\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8828 - val_loss: 0.3148 - val_accuracy: 0.9047\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2739 - accuracy: 0.8903 - val_loss: 0.2465 - val_accuracy: 0.9012\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2868 - accuracy: 0.8832 - val_loss: 0.2961 - val_accuracy: 0.9133\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2748 - accuracy: 0.8888 - val_loss: 0.5786 - val_accuracy: 0.8044\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2753 - accuracy: 0.8908 - val_loss: 0.2876 - val_accuracy: 0.8837\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2754 - accuracy: 0.8870 - val_loss: 0.2636 - val_accuracy: 0.9006\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2760 - accuracy: 0.8879 - val_loss: 0.2446 - val_accuracy: 0.9048\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.8938 - val_loss: 0.2560 - val_accuracy: 0.9134\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2681 - accuracy: 0.8901 - val_loss: 0.3366 - val_accuracy: 0.9030\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8921 - val_loss: 0.2631 - val_accuracy: 0.9117\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2679 - accuracy: 0.8934 - val_loss: 0.2380 - val_accuracy: 0.9042\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2635 - accuracy: 0.8932 - val_loss: 0.2800 - val_accuracy: 0.9086\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2566 - accuracy: 0.8958 - val_loss: 0.2619 - val_accuracy: 0.9087\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2558 - accuracy: 0.8975 - val_loss: 0.2794 - val_accuracy: 0.9143\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2577 - accuracy: 0.8958 - val_loss: 0.3038 - val_accuracy: 0.9119\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2539 - accuracy: 0.9004 - val_loss: 0.2555 - val_accuracy: 0.9145\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2575 - accuracy: 0.8984 - val_loss: 0.2556 - val_accuracy: 0.9061\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2604 - accuracy: 0.8951 - val_loss: 0.2697 - val_accuracy: 0.9139\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2538 - accuracy: 0.8988 - val_loss: 0.2570 - val_accuracy: 0.9082\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2626 - accuracy: 0.8964 - val_loss: 0.2620 - val_accuracy: 0.8975\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2665 - accuracy: 0.8940 - val_loss: 0.2636 - val_accuracy: 0.9123\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2565 - accuracy: 0.8982 - val_loss: 0.3598 - val_accuracy: 0.9031\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8949 - val_loss: 0.3860 - val_accuracy: 0.8788\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2501 - accuracy: 0.8993 - val_loss: 0.2768 - val_accuracy: 0.8825\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.8993 - val_loss: 0.2992 - val_accuracy: 0.9144\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2546 - accuracy: 0.8953 - val_loss: 0.2944 - val_accuracy: 0.9105\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2576 - accuracy: 0.8965 - val_loss: 0.2428 - val_accuracy: 0.9113\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2549 - accuracy: 0.8960 - val_loss: 0.2475 - val_accuracy: 0.9090\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2494 - accuracy: 0.8995 - val_loss: 0.2228 - val_accuracy: 0.9087\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2269 - accuracy: 0.9072\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2560 - accuracy: 0.8976\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2487 - accuracy: 0.9003\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2497 - accuracy: 0.8991\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2527 - accuracy: 0.8975\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2518 - accuracy: 0.8981\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2499 - accuracy: 0.8986\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2479 - accuracy: 0.9010\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2484 - accuracy: 0.9015\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2468 - accuracy: 0.9012\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2474 - accuracy: 0.8998\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6300 - accuracy: 0.6453 - val_loss: 0.5533 - val_accuracy: 0.7172\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5617 - accuracy: 0.7005 - val_loss: 0.5865 - val_accuracy: 0.7108\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5484 - accuracy: 0.7162 - val_loss: 0.5638 - val_accuracy: 0.7220\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5387 - accuracy: 0.7208 - val_loss: 0.5561 - val_accuracy: 0.7233\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5263 - accuracy: 0.7283 - val_loss: 0.5051 - val_accuracy: 0.7472\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4931 - accuracy: 0.7494 - val_loss: 0.4439 - val_accuracy: 0.7990\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4461 - accuracy: 0.7884 - val_loss: 0.4365 - val_accuracy: 0.7981\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4130 - accuracy: 0.8120 - val_loss: 0.3958 - val_accuracy: 0.8328\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3998 - accuracy: 0.8246 - val_loss: 0.4027 - val_accuracy: 0.8603\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3674 - accuracy: 0.8407 - val_loss: 0.4241 - val_accuracy: 0.8777\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3433 - accuracy: 0.8596 - val_loss: 0.3354 - val_accuracy: 0.8909\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3277 - accuracy: 0.8675 - val_loss: 0.2823 - val_accuracy: 0.8986\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3242 - accuracy: 0.8696 - val_loss: 0.2996 - val_accuracy: 0.8881\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3138 - accuracy: 0.8738 - val_loss: 0.3161 - val_accuracy: 0.9022\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3082 - accuracy: 0.8751 - val_loss: 0.2993 - val_accuracy: 0.8999\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3022 - accuracy: 0.8759 - val_loss: 0.2955 - val_accuracy: 0.9022\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3002 - accuracy: 0.8803 - val_loss: 0.3068 - val_accuracy: 0.8966\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2941 - accuracy: 0.8798 - val_loss: 0.3683 - val_accuracy: 0.9005\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2935 - accuracy: 0.8780 - val_loss: 0.2956 - val_accuracy: 0.9087\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2867 - accuracy: 0.8841 - val_loss: 0.2704 - val_accuracy: 0.8978\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2772 - accuracy: 0.8895 - val_loss: 0.3139 - val_accuracy: 0.9098\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2743 - accuracy: 0.8896 - val_loss: 0.2882 - val_accuracy: 0.8950\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2693 - accuracy: 0.8913 - val_loss: 0.2625 - val_accuracy: 0.9077\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2789 - accuracy: 0.8886 - val_loss: 0.2811 - val_accuracy: 0.9091\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2669 - accuracy: 0.8931 - val_loss: 0.3359 - val_accuracy: 0.8989\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2766 - accuracy: 0.8899 - val_loss: 0.2691 - val_accuracy: 0.9095\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8898 - val_loss: 0.2775 - val_accuracy: 0.9091\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2677 - accuracy: 0.8926 - val_loss: 0.3101 - val_accuracy: 0.9088\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2739 - accuracy: 0.8896 - val_loss: 0.2635 - val_accuracy: 0.9060\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.8920 - val_loss: 0.2537 - val_accuracy: 0.9107\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2647 - accuracy: 0.8953 - val_loss: 0.2671 - val_accuracy: 0.9057\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2724 - accuracy: 0.8890 - val_loss: 0.2342 - val_accuracy: 0.9095\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2658 - accuracy: 0.8930 - val_loss: 0.2390 - val_accuracy: 0.9145\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2650 - accuracy: 0.8931 - val_loss: 0.2587 - val_accuracy: 0.9135\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2615 - accuracy: 0.8952 - val_loss: 0.2770 - val_accuracy: 0.9180\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2605 - accuracy: 0.8947 - val_loss: 0.3550 - val_accuracy: 0.8579\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2648 - accuracy: 0.8951 - val_loss: 0.2465 - val_accuracy: 0.9120\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2649 - accuracy: 0.8933 - val_loss: 0.2459 - val_accuracy: 0.9151\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2630 - accuracy: 0.8960 - val_loss: 0.2565 - val_accuracy: 0.9013\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2619 - accuracy: 0.8938 - val_loss: 0.2572 - val_accuracy: 0.9101\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2557 - accuracy: 0.8977 - val_loss: 0.2486 - val_accuracy: 0.9132\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2574 - accuracy: 0.8954 - val_loss: 0.2906 - val_accuracy: 0.9064\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2542 - accuracy: 0.8974 - val_loss: 0.3947 - val_accuracy: 0.9100\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2552 - accuracy: 0.8960 - val_loss: 0.2776 - val_accuracy: 0.9067\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2570 - accuracy: 0.8985 - val_loss: 0.2777 - val_accuracy: 0.9043\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2528 - accuracy: 0.9009 - val_loss: 0.2472 - val_accuracy: 0.9116\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2502 - accuracy: 0.8998 - val_loss: 0.2599 - val_accuracy: 0.9136\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2482 - accuracy: 0.8995 - val_loss: 0.2903 - val_accuracy: 0.9120\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2570 - accuracy: 0.8991 - val_loss: 0.2549 - val_accuracy: 0.9121\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2506 - accuracy: 0.8983 - val_loss: 0.2512 - val_accuracy: 0.9105\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2783 - accuracy: 0.9174\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2644 - accuracy: 0.8950\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2640 - accuracy: 0.8943\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2623 - accuracy: 0.8938\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2542 - accuracy: 0.8987\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2541 - accuracy: 0.8975\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2495 - accuracy: 0.9004\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2571 - accuracy: 0.8973\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2555 - accuracy: 0.8967\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2595 - accuracy: 0.8974\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2501 - accuracy: 0.8992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6301 - accuracy: 0.6481 - val_loss: 0.5713 - val_accuracy: 0.7157\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5584 - accuracy: 0.7060 - val_loss: 0.5868 - val_accuracy: 0.7216\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5501 - accuracy: 0.7161 - val_loss: 0.5672 - val_accuracy: 0.7287\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5414 - accuracy: 0.7166 - val_loss: 0.5517 - val_accuracy: 0.7360\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5146 - accuracy: 0.7349 - val_loss: 0.4859 - val_accuracy: 0.7634\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4650 - accuracy: 0.7728 - val_loss: 0.4462 - val_accuracy: 0.8300\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4264 - accuracy: 0.8046 - val_loss: 0.4801 - val_accuracy: 0.8037\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4011 - accuracy: 0.8221 - val_loss: 0.4231 - val_accuracy: 0.8397\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3770 - accuracy: 0.8380 - val_loss: 0.4008 - val_accuracy: 0.8628\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3520 - accuracy: 0.8522 - val_loss: 0.3420 - val_accuracy: 0.8788\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3463 - accuracy: 0.8563 - val_loss: 0.4028 - val_accuracy: 0.8434\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3297 - accuracy: 0.8643 - val_loss: 0.3952 - val_accuracy: 0.8826\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3205 - accuracy: 0.8692 - val_loss: 0.3402 - val_accuracy: 0.8886\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3110 - accuracy: 0.8729 - val_loss: 0.3208 - val_accuracy: 0.8846\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3145 - accuracy: 0.8722 - val_loss: 0.2878 - val_accuracy: 0.9006\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2966 - accuracy: 0.8815 - val_loss: 0.3006 - val_accuracy: 0.9066\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2984 - accuracy: 0.8757 - val_loss: 0.3716 - val_accuracy: 0.9035\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2970 - accuracy: 0.8811 - val_loss: 0.3090 - val_accuracy: 0.8979\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8838 - val_loss: 0.3916 - val_accuracy: 0.8847\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.8863 - val_loss: 0.3134 - val_accuracy: 0.9017\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2786 - accuracy: 0.8855 - val_loss: 0.3080 - val_accuracy: 0.9018\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2774 - accuracy: 0.8874 - val_loss: 0.2776 - val_accuracy: 0.9120\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2737 - accuracy: 0.8873 - val_loss: 0.2868 - val_accuracy: 0.9024\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2746 - accuracy: 0.8890 - val_loss: 0.3118 - val_accuracy: 0.9056\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2760 - accuracy: 0.8867 - val_loss: 0.2857 - val_accuracy: 0.9075\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8890 - val_loss: 0.2893 - val_accuracy: 0.9081\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2645 - accuracy: 0.8945 - val_loss: 0.3159 - val_accuracy: 0.9095\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2677 - accuracy: 0.8906 - val_loss: 0.2622 - val_accuracy: 0.9047\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2583 - accuracy: 0.8937 - val_loss: 0.3237 - val_accuracy: 0.9035\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2570 - accuracy: 0.8979 - val_loss: 0.2567 - val_accuracy: 0.9015\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2599 - accuracy: 0.8961 - val_loss: 0.3049 - val_accuracy: 0.9129\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8955 - val_loss: 0.2847 - val_accuracy: 0.9026\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2560 - accuracy: 0.8966 - val_loss: 0.2514 - val_accuracy: 0.9116\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2573 - accuracy: 0.8970 - val_loss: 0.2513 - val_accuracy: 0.9017\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2566 - accuracy: 0.8955 - val_loss: 0.2490 - val_accuracy: 0.9104\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8958 - val_loss: 0.3439 - val_accuracy: 0.9034\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2484 - accuracy: 0.8988 - val_loss: 0.2705 - val_accuracy: 0.9003\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2448 - accuracy: 0.9030 - val_loss: 0.2876 - val_accuracy: 0.9129\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2477 - accuracy: 0.9019 - val_loss: 0.3144 - val_accuracy: 0.8827\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2545 - accuracy: 0.8949 - val_loss: 0.3234 - val_accuracy: 0.9119\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2542 - accuracy: 0.8971 - val_loss: 0.2549 - val_accuracy: 0.9106\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2499 - accuracy: 0.8991 - val_loss: 0.2587 - val_accuracy: 0.9174\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2466 - accuracy: 0.9008 - val_loss: 0.2883 - val_accuracy: 0.8886\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2473 - accuracy: 0.9001 - val_loss: 0.5457 - val_accuracy: 0.8130\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2427 - accuracy: 0.9017 - val_loss: 0.3136 - val_accuracy: 0.8947\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2453 - accuracy: 0.9037 - val_loss: 0.2468 - val_accuracy: 0.9047\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2516 - accuracy: 0.8994 - val_loss: 0.2983 - val_accuracy: 0.9098\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2415 - accuracy: 0.9018 - val_loss: 0.3054 - val_accuracy: 0.8758\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2456 - accuracy: 0.9021 - val_loss: 0.2650 - val_accuracy: 0.9059\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2505 - accuracy: 0.8991 - val_loss: 0.2537 - val_accuracy: 0.9089\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2553 - accuracy: 0.9067\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2504 - accuracy: 0.8992\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2470 - accuracy: 0.9003\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2462 - accuracy: 0.9005\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2437 - accuracy: 0.9019\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2488 - accuracy: 0.9003\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2427 - accuracy: 0.9014\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2401 - accuracy: 0.9025\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2429 - accuracy: 0.9015\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2389 - accuracy: 0.9030\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2375 - accuracy: 0.9050\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6328 - accuracy: 0.6494 - val_loss: 0.5472 - val_accuracy: 0.7163\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5557 - accuracy: 0.7095 - val_loss: 0.5709 - val_accuracy: 0.7219\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5463 - accuracy: 0.7134 - val_loss: 0.5693 - val_accuracy: 0.7220\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5389 - accuracy: 0.7178 - val_loss: 0.5617 - val_accuracy: 0.7289\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5302 - accuracy: 0.7247 - val_loss: 0.5181 - val_accuracy: 0.7517\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4961 - accuracy: 0.7497 - val_loss: 0.4243 - val_accuracy: 0.8173\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4204 - accuracy: 0.7994 - val_loss: 0.4155 - val_accuracy: 0.8305\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3887 - accuracy: 0.8202 - val_loss: 0.3968 - val_accuracy: 0.8615\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3555 - accuracy: 0.8369 - val_loss: 0.3910 - val_accuracy: 0.8714\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3341 - accuracy: 0.8518 - val_loss: 0.3735 - val_accuracy: 0.8839\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3294 - accuracy: 0.8546 - val_loss: 0.3351 - val_accuracy: 0.8877\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3228 - accuracy: 0.8606 - val_loss: 0.3278 - val_accuracy: 0.8856\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3126 - accuracy: 0.8682 - val_loss: 0.3230 - val_accuracy: 0.8991\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3083 - accuracy: 0.8707 - val_loss: 0.3484 - val_accuracy: 0.8860\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2929 - accuracy: 0.8786 - val_loss: 0.4373 - val_accuracy: 0.8894\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2953 - accuracy: 0.8772 - val_loss: 0.2851 - val_accuracy: 0.8965\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2845 - accuracy: 0.8828 - val_loss: 0.3143 - val_accuracy: 0.8863\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2826 - accuracy: 0.8836 - val_loss: 0.3388 - val_accuracy: 0.8898\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2787 - accuracy: 0.8834 - val_loss: 0.2811 - val_accuracy: 0.9060\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2777 - accuracy: 0.8846 - val_loss: 0.3231 - val_accuracy: 0.9025\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2726 - accuracy: 0.8885 - val_loss: 0.2689 - val_accuracy: 0.8949\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2750 - accuracy: 0.8865 - val_loss: 0.2945 - val_accuracy: 0.9106\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8909 - val_loss: 0.2641 - val_accuracy: 0.9011\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2693 - accuracy: 0.8893 - val_loss: 0.2734 - val_accuracy: 0.8973\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2687 - accuracy: 0.8875 - val_loss: 0.2654 - val_accuracy: 0.9038\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2610 - accuracy: 0.8944 - val_loss: 0.2723 - val_accuracy: 0.9104\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2613 - accuracy: 0.8942 - val_loss: 0.2659 - val_accuracy: 0.9067\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2636 - accuracy: 0.8924 - val_loss: 0.2511 - val_accuracy: 0.9101\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2640 - accuracy: 0.8919 - val_loss: 0.3014 - val_accuracy: 0.8743\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2695 - accuracy: 0.8916 - val_loss: 0.2440 - val_accuracy: 0.9163\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2558 - accuracy: 0.8971 - val_loss: 0.2492 - val_accuracy: 0.9098\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8951 - val_loss: 0.2780 - val_accuracy: 0.9002\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.8956 - val_loss: 0.2483 - val_accuracy: 0.9016\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2541 - accuracy: 0.8949 - val_loss: 0.2631 - val_accuracy: 0.9142\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8940 - val_loss: 0.2598 - val_accuracy: 0.9140\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2517 - accuracy: 0.8966 - val_loss: 0.2359 - val_accuracy: 0.9136\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8920 - val_loss: 0.2425 - val_accuracy: 0.9128\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2452 - accuracy: 0.9001 - val_loss: 0.2539 - val_accuracy: 0.9100\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2560 - accuracy: 0.8948 - val_loss: 0.2712 - val_accuracy: 0.9003\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2564 - accuracy: 0.8935 - val_loss: 0.2571 - val_accuracy: 0.9128\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2500 - accuracy: 0.8988 - val_loss: 0.2394 - val_accuracy: 0.9158\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2474 - accuracy: 0.8989 - val_loss: 0.2492 - val_accuracy: 0.9117\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2479 - accuracy: 0.8989 - val_loss: 0.2381 - val_accuracy: 0.9118\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2390 - accuracy: 0.9021 - val_loss: 0.2642 - val_accuracy: 0.9151\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2493 - accuracy: 0.8987 - val_loss: 0.2372 - val_accuracy: 0.9192\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2472 - accuracy: 0.8995 - val_loss: 0.2311 - val_accuracy: 0.9130\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.8978 - val_loss: 0.2780 - val_accuracy: 0.9167\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2491 - accuracy: 0.8984 - val_loss: 0.2340 - val_accuracy: 0.9171\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9030 - val_loss: 0.2422 - val_accuracy: 0.9109\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2442 - accuracy: 0.8998 - val_loss: 0.2540 - val_accuracy: 0.9162\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2565 - accuracy: 0.9135\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2461 - accuracy: 0.8988\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2421 - accuracy: 0.9017\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2427 - accuracy: 0.9007\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2417 - accuracy: 0.9025\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2548 - accuracy: 0.8964\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2463 - accuracy: 0.8994\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2482 - accuracy: 0.8982\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2431 - accuracy: 0.9021\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2406 - accuracy: 0.9014\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2422 - accuracy: 0.9011\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lDiBbWAoVjA",
        "outputId": "27f0061e-3b5d-409d-d865-81f8458add4d"
      },
      "source": [
        "cm_top25 = confusion_matrix(y, cv_top25_preds)\n",
        "print(cm_top25)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[51192  6808]\n",
            " [ 2421 28226]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCvgAZwFdiUw"
      },
      "source": [
        "# neural network on top 50 most important features per recursive feature elimination package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5i7RNJz96hp2",
        "outputId": "56dc94e5-4d75-458b-e799-710dd029979c"
      },
      "source": [
        "y = full_df.iloc[:,-1]\n",
        "\n",
        "features = ['qty_dot_url', 'qty_hyphen_url', 'qty_slash_url', 'length_url',\n",
        "       'qty_dot_domain', 'qty_vowels_domain', 'domain_length',\n",
        "       'qty_dot_directory', 'qty_hyphen_directory', 'qty_underline_directory',\n",
        "       'qty_slash_directory', 'qty_at_directory', 'qty_and_directory',\n",
        "       'qty_space_directory', 'qty_tilde_directory', 'qty_comma_directory',\n",
        "       'qty_plus_directory', 'qty_asterisk_directory', 'qty_hashtag_directory',\n",
        "       'qty_dollar_directory', 'directory_length', 'qty_dot_file',\n",
        "       'qty_hyphen_file', 'qty_underline_file', 'qty_slash_file',\n",
        "       'qty_questionmark_file', 'qty_equal_file', 'qty_at_file',\n",
        "       'qty_and_file', 'qty_exclamation_file', 'qty_space_file',\n",
        "       'qty_tilde_file', 'qty_comma_file', 'qty_plus_file',\n",
        "       'qty_asterisk_file', 'qty_hashtag_file', 'qty_dollar_file',\n",
        "       'qty_percent_file', 'file_length', 'params_length', 'time_response',\n",
        "       'asn_ip', 'time_domain_activation', 'time_domain_expiration',\n",
        "       'qty_ip_resolved', 'qty_nameservers', 'qty_mx_servers', 'ttl_hostname',\n",
        "       'tls_ssl_certificate', 'qty_redirects']\n",
        "X = full_df[features]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.25, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>qty_dot_directory</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>qty_at_directory</th>\n",
              "      <th>qty_and_directory</th>\n",
              "      <th>qty_space_directory</th>\n",
              "      <th>qty_tilde_directory</th>\n",
              "      <th>qty_comma_directory</th>\n",
              "      <th>qty_plus_directory</th>\n",
              "      <th>qty_asterisk_directory</th>\n",
              "      <th>qty_hashtag_directory</th>\n",
              "      <th>qty_dollar_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_dot_file</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>qty_underline_file</th>\n",
              "      <th>qty_slash_file</th>\n",
              "      <th>qty_questionmark_file</th>\n",
              "      <th>qty_equal_file</th>\n",
              "      <th>qty_at_file</th>\n",
              "      <th>qty_and_file</th>\n",
              "      <th>qty_exclamation_file</th>\n",
              "      <th>qty_space_file</th>\n",
              "      <th>qty_tilde_file</th>\n",
              "      <th>qty_comma_file</th>\n",
              "      <th>qty_plus_file</th>\n",
              "      <th>qty_asterisk_file</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>params_length</th>\n",
              "      <th>time_response</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.999934</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.011480</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.730210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.680199</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.698307</td>\n",
              "      <td>0.269674</td>\n",
              "      <td>0.016112</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.662860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.859057</td>\n",
              "      <td>0.261792</td>\n",
              "      <td>0.005926</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.439823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.795440</td>\n",
              "      <td>0.178787</td>\n",
              "      <td>0.007212</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.579014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_url  qty_hyphen_url  ...  tls_ssl_certificate  qty_redirects\n",
              "5676      0.000004             0.0  ...             0.000004       0.000000\n",
              "39002     0.000777             0.0  ...             0.000259       0.000259\n",
              "1732      0.000092             0.0  ...             0.000000       0.000000\n",
              "39668     0.000122             0.0  ...             0.000000       0.000061\n",
              "82035     0.000054             0.0  ...             0.000000       0.000000\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 264
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M2NwTTde6hp3",
        "outputId": "de48b929-7436-4ccb-efde-20822bf0c6cf"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#neural net\n",
        "\n",
        "top_50_nn = keras.Sequential([\n",
        "                          layers.InputLayer(input_shape=[50]),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=64, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=50, activation='relu'),\n",
        "                          layers.Dropout(0.20),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=32, activation='relu'),\n",
        "                          layers.Dropout(0.2),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=16, activation='relu'),\n",
        "                          layers.Dropout(0.40),\n",
        "                          layers.Dense(units=111, activation='relu'),\n",
        "                          layers.Flatten(),\n",
        "                          layers.Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "top_50_nn.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=[tf.keras.metrics.BinaryAccuracy(threshold=0.5), \n",
        "             tf.keras.metrics.AUC(),\n",
        "             ]\n",
        ")\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_binary_accuracy', mode = 'max',\n",
        "                                       patience = 25, restore_best_weights = True)\n",
        "\n",
        "\n",
        "top_50_nn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 64)                3264      \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 64)                4160      \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 50)                3250      \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 32)                1632      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 16)                528       \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 16)                272       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 111)               1887      \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 111)               0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 112       \n",
            "=================================================================\n",
            "Total params: 16,161\n",
            "Trainable params: 16,161\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBMNuNFP6hp4",
        "outputId": "2e8a0785-5739-473b-e1d0-2bb4d4acd8c5"
      },
      "source": [
        "history1 = top_50_nn.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=500, callbacks = [earlystopping])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6273 - binary_accuracy: 0.6490 - auc: 0.6124 - val_loss: 0.5745 - val_binary_accuracy: 0.7207 - val_auc: 0.7613\n",
            "Epoch 2/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5524 - binary_accuracy: 0.7067 - auc: 0.7498 - val_loss: 0.5888 - val_binary_accuracy: 0.7207 - val_auc: 0.7646\n",
            "Epoch 3/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5474 - binary_accuracy: 0.7141 - auc: 0.7528 - val_loss: 0.5827 - val_binary_accuracy: 0.7264 - val_auc: 0.7741\n",
            "Epoch 4/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5343 - binary_accuracy: 0.7221 - auc: 0.7631 - val_loss: 0.5580 - val_binary_accuracy: 0.7292 - val_auc: 0.7981\n",
            "Epoch 5/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5060 - binary_accuracy: 0.7347 - auc: 0.7920 - val_loss: 0.4737 - val_binary_accuracy: 0.7696 - val_auc: 0.8302\n",
            "Epoch 6/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.4572 - binary_accuracy: 0.7746 - auc: 0.8404 - val_loss: 0.4488 - val_binary_accuracy: 0.7913 - val_auc: 0.8822\n",
            "Epoch 7/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4228 - binary_accuracy: 0.8047 - auc: 0.8774 - val_loss: 0.4422 - val_binary_accuracy: 0.8625 - val_auc: 0.9287\n",
            "Epoch 8/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3958 - binary_accuracy: 0.8311 - auc: 0.8981 - val_loss: 0.4105 - val_binary_accuracy: 0.8550 - val_auc: 0.9353\n",
            "Epoch 9/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3708 - binary_accuracy: 0.8491 - auc: 0.9132 - val_loss: 0.3264 - val_binary_accuracy: 0.8799 - val_auc: 0.9417\n",
            "Epoch 10/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3427 - binary_accuracy: 0.8601 - auc: 0.9236 - val_loss: 0.3729 - val_binary_accuracy: 0.8817 - val_auc: 0.9443\n",
            "Epoch 11/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3248 - binary_accuracy: 0.8706 - auc: 0.9322 - val_loss: 0.3339 - val_binary_accuracy: 0.8952 - val_auc: 0.9534\n",
            "Epoch 12/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3199 - binary_accuracy: 0.8755 - auc: 0.9327 - val_loss: 0.3034 - val_binary_accuracy: 0.8829 - val_auc: 0.9480\n",
            "Epoch 13/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3151 - binary_accuracy: 0.8713 - auc: 0.9336 - val_loss: 0.3481 - val_binary_accuracy: 0.8811 - val_auc: 0.9473\n",
            "Epoch 14/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3196 - binary_accuracy: 0.8695 - auc: 0.9310 - val_loss: 0.3429 - val_binary_accuracy: 0.8947 - val_auc: 0.9555\n",
            "Epoch 15/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3013 - binary_accuracy: 0.8803 - auc: 0.9378 - val_loss: 0.2923 - val_binary_accuracy: 0.8903 - val_auc: 0.9581\n",
            "Epoch 16/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3049 - binary_accuracy: 0.8784 - auc: 0.9364 - val_loss: 0.3235 - val_binary_accuracy: 0.8833 - val_auc: 0.9536\n",
            "Epoch 17/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2990 - binary_accuracy: 0.8827 - auc: 0.9386 - val_loss: 0.2755 - val_binary_accuracy: 0.8879 - val_auc: 0.9577\n",
            "Epoch 18/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3009 - binary_accuracy: 0.8786 - auc: 0.9379 - val_loss: 0.2520 - val_binary_accuracy: 0.9083 - val_auc: 0.9580\n",
            "Epoch 19/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2827 - binary_accuracy: 0.8871 - auc: 0.9437 - val_loss: 0.3216 - val_binary_accuracy: 0.8863 - val_auc: 0.9588\n",
            "Epoch 20/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2869 - binary_accuracy: 0.8864 - auc: 0.9427 - val_loss: 0.2724 - val_binary_accuracy: 0.8996 - val_auc: 0.9631\n",
            "Epoch 21/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2851 - binary_accuracy: 0.8862 - auc: 0.9441 - val_loss: 0.3838 - val_binary_accuracy: 0.8520 - val_auc: 0.9470\n",
            "Epoch 22/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2864 - binary_accuracy: 0.8854 - auc: 0.9427 - val_loss: 0.2457 - val_binary_accuracy: 0.9028 - val_auc: 0.9589\n",
            "Epoch 23/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2769 - binary_accuracy: 0.8893 - auc: 0.9469 - val_loss: 0.2452 - val_binary_accuracy: 0.9122 - val_auc: 0.9622\n",
            "Epoch 24/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2685 - binary_accuracy: 0.8926 - auc: 0.9498 - val_loss: 0.2807 - val_binary_accuracy: 0.9144 - val_auc: 0.9629\n",
            "Epoch 25/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2679 - binary_accuracy: 0.8942 - auc: 0.9495 - val_loss: 0.2668 - val_binary_accuracy: 0.9127 - val_auc: 0.9618\n",
            "Epoch 26/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2656 - binary_accuracy: 0.8936 - auc: 0.9505 - val_loss: 0.2668 - val_binary_accuracy: 0.9012 - val_auc: 0.9646\n",
            "Epoch 27/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2715 - binary_accuracy: 0.8919 - auc: 0.9502 - val_loss: 0.2429 - val_binary_accuracy: 0.9084 - val_auc: 0.9655\n",
            "Epoch 28/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2668 - binary_accuracy: 0.8913 - auc: 0.9511 - val_loss: 0.2947 - val_binary_accuracy: 0.9047 - val_auc: 0.9632\n",
            "Epoch 29/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2612 - binary_accuracy: 0.8973 - auc: 0.9522 - val_loss: 0.2304 - val_binary_accuracy: 0.9123 - val_auc: 0.9686\n",
            "Epoch 30/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2543 - binary_accuracy: 0.8996 - auc: 0.9545 - val_loss: 0.2544 - val_binary_accuracy: 0.9145 - val_auc: 0.9699\n",
            "Epoch 31/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2610 - binary_accuracy: 0.8978 - auc: 0.9520 - val_loss: 0.3309 - val_binary_accuracy: 0.9083 - val_auc: 0.9670\n",
            "Epoch 32/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2531 - binary_accuracy: 0.9024 - auc: 0.9558 - val_loss: 0.2729 - val_binary_accuracy: 0.9164 - val_auc: 0.9700\n",
            "Epoch 33/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2619 - binary_accuracy: 0.8958 - auc: 0.9522 - val_loss: 0.2287 - val_binary_accuracy: 0.9113 - val_auc: 0.9682\n",
            "Epoch 34/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2600 - binary_accuracy: 0.8987 - auc: 0.9524 - val_loss: 0.2456 - val_binary_accuracy: 0.9191 - val_auc: 0.9693\n",
            "Epoch 35/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2625 - binary_accuracy: 0.8959 - auc: 0.9529 - val_loss: 0.2676 - val_binary_accuracy: 0.9101 - val_auc: 0.9657\n",
            "Epoch 36/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2627 - binary_accuracy: 0.8964 - auc: 0.9521 - val_loss: 0.3652 - val_binary_accuracy: 0.8926 - val_auc: 0.9621\n",
            "Epoch 37/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2547 - binary_accuracy: 0.9015 - auc: 0.9546 - val_loss: 0.2657 - val_binary_accuracy: 0.9170 - val_auc: 0.9702\n",
            "Epoch 38/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2534 - binary_accuracy: 0.8996 - auc: 0.9552 - val_loss: 0.2742 - val_binary_accuracy: 0.8908 - val_auc: 0.9638\n",
            "Epoch 39/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2549 - binary_accuracy: 0.8976 - auc: 0.9559 - val_loss: 0.2733 - val_binary_accuracy: 0.9142 - val_auc: 0.9700\n",
            "Epoch 40/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2503 - binary_accuracy: 0.8996 - auc: 0.9554 - val_loss: 0.2319 - val_binary_accuracy: 0.9078 - val_auc: 0.9674\n",
            "Epoch 41/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2595 - binary_accuracy: 0.8952 - auc: 0.9526 - val_loss: 0.2555 - val_binary_accuracy: 0.9067 - val_auc: 0.9686\n",
            "Epoch 42/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2521 - binary_accuracy: 0.9007 - auc: 0.9553 - val_loss: 0.2172 - val_binary_accuracy: 0.9139 - val_auc: 0.9695\n",
            "Epoch 43/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2541 - binary_accuracy: 0.8994 - auc: 0.9550 - val_loss: 0.2455 - val_binary_accuracy: 0.9171 - val_auc: 0.9710\n",
            "Epoch 44/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2464 - binary_accuracy: 0.9031 - auc: 0.9584 - val_loss: 0.2175 - val_binary_accuracy: 0.9141 - val_auc: 0.9704\n",
            "Epoch 45/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2489 - binary_accuracy: 0.9026 - auc: 0.9573 - val_loss: 0.2642 - val_binary_accuracy: 0.9145 - val_auc: 0.9632\n",
            "Epoch 46/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2550 - binary_accuracy: 0.8973 - auc: 0.9543 - val_loss: 0.2497 - val_binary_accuracy: 0.9201 - val_auc: 0.9711\n",
            "Epoch 47/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2400 - binary_accuracy: 0.9043 - auc: 0.9601 - val_loss: 0.2631 - val_binary_accuracy: 0.9192 - val_auc: 0.9673\n",
            "Epoch 48/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2529 - binary_accuracy: 0.8976 - auc: 0.9556 - val_loss: 0.2401 - val_binary_accuracy: 0.9133 - val_auc: 0.9691\n",
            "Epoch 49/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2532 - binary_accuracy: 0.8987 - auc: 0.9561 - val_loss: 0.2291 - val_binary_accuracy: 0.9102 - val_auc: 0.9698\n",
            "Epoch 50/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2499 - binary_accuracy: 0.8995 - auc: 0.9576 - val_loss: 0.2724 - val_binary_accuracy: 0.9153 - val_auc: 0.9712\n",
            "Epoch 51/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2478 - binary_accuracy: 0.8976 - auc: 0.9571 - val_loss: 0.2499 - val_binary_accuracy: 0.9103 - val_auc: 0.9705\n",
            "Epoch 52/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2368 - binary_accuracy: 0.9072 - auc: 0.9611 - val_loss: 0.2631 - val_binary_accuracy: 0.9161 - val_auc: 0.9721\n",
            "Epoch 53/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2428 - binary_accuracy: 0.9031 - auc: 0.9597 - val_loss: 0.2613 - val_binary_accuracy: 0.9023 - val_auc: 0.9697\n",
            "Epoch 54/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2384 - binary_accuracy: 0.9075 - auc: 0.9610 - val_loss: 0.2364 - val_binary_accuracy: 0.9165 - val_auc: 0.9716\n",
            "Epoch 55/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2398 - binary_accuracy: 0.9076 - auc: 0.9594 - val_loss: 0.2385 - val_binary_accuracy: 0.9189 - val_auc: 0.9722\n",
            "Epoch 56/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2397 - binary_accuracy: 0.9058 - auc: 0.9601 - val_loss: 0.2341 - val_binary_accuracy: 0.9104 - val_auc: 0.9668\n",
            "Epoch 57/500\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2449 - binary_accuracy: 0.9035 - auc: 0.9587 - val_loss: 0.2593 - val_binary_accuracy: 0.9057 - val_auc: 0.9724\n",
            "Epoch 58/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2398 - binary_accuracy: 0.9044 - auc: 0.9605 - val_loss: 0.3040 - val_binary_accuracy: 0.8846 - val_auc: 0.9703\n",
            "Epoch 59/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2425 - binary_accuracy: 0.9042 - auc: 0.9589 - val_loss: 0.2407 - val_binary_accuracy: 0.9117 - val_auc: 0.9693\n",
            "Epoch 60/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2400 - binary_accuracy: 0.9053 - auc: 0.9599 - val_loss: 0.2456 - val_binary_accuracy: 0.9118 - val_auc: 0.9726\n",
            "Epoch 61/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2501 - binary_accuracy: 0.9010 - auc: 0.9570 - val_loss: 0.3006 - val_binary_accuracy: 0.9146 - val_auc: 0.9722\n",
            "Epoch 62/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2427 - binary_accuracy: 0.9029 - auc: 0.9595 - val_loss: 0.2547 - val_binary_accuracy: 0.9046 - val_auc: 0.9691\n",
            "Epoch 63/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2483 - binary_accuracy: 0.9019 - auc: 0.9575 - val_loss: 0.2821 - val_binary_accuracy: 0.8933 - val_auc: 0.9659\n",
            "Epoch 64/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2409 - binary_accuracy: 0.9046 - auc: 0.9600 - val_loss: 0.3034 - val_binary_accuracy: 0.8760 - val_auc: 0.9722\n",
            "Epoch 65/500\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.2413 - binary_accuracy: 0.9070 - auc: 0.9593 - val_loss: 0.3172 - val_binary_accuracy: 0.8821 - val_auc: 0.9713\n",
            "Epoch 66/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2362 - binary_accuracy: 0.9068 - auc: 0.9605 - val_loss: 0.2218 - val_binary_accuracy: 0.9169 - val_auc: 0.9732\n",
            "Epoch 67/500\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2444 - binary_accuracy: 0.9010 - auc: 0.9591 - val_loss: 0.2345 - val_binary_accuracy: 0.9133 - val_auc: 0.9712\n",
            "Epoch 68/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2315 - binary_accuracy: 0.9078 - auc: 0.9624 - val_loss: 0.2630 - val_binary_accuracy: 0.9029 - val_auc: 0.9724\n",
            "Epoch 69/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2357 - binary_accuracy: 0.9077 - auc: 0.9612 - val_loss: 0.2444 - val_binary_accuracy: 0.9139 - val_auc: 0.9722\n",
            "Epoch 70/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2326 - binary_accuracy: 0.9087 - auc: 0.9623 - val_loss: 0.2528 - val_binary_accuracy: 0.9112 - val_auc: 0.9683\n",
            "Epoch 71/500\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2404 - binary_accuracy: 0.9044 - auc: 0.9596 - val_loss: 0.2339 - val_binary_accuracy: 0.9167 - val_auc: 0.9724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "Ui_cRi7g6hp4",
        "outputId": "503b5026-9a10-4092-ec0d-e20cc06e36f9"
      },
      "source": [
        "history_df1 = pd.DataFrame(history1.history)\n",
        "\n",
        "history_df1.describe()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>binary_accuracy</th>\n",
              "      <th>auc</th>\n",
              "      <th>val_loss</th>\n",
              "      <th>val_binary_accuracy</th>\n",
              "      <th>val_auc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>71.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>71.000000</td>\n",
              "      <td>71.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.288592</td>\n",
              "      <td>0.879161</td>\n",
              "      <td>0.935296</td>\n",
              "      <td>0.298685</td>\n",
              "      <td>0.889398</td>\n",
              "      <td>0.950575</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.082076</td>\n",
              "      <td>0.051673</td>\n",
              "      <td>0.053511</td>\n",
              "      <td>0.087572</td>\n",
              "      <td>0.047972</td>\n",
              "      <td>0.048127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.233717</td>\n",
              "      <td>0.658265</td>\n",
              "      <td>0.701118</td>\n",
              "      <td>0.217162</td>\n",
              "      <td>0.720696</td>\n",
              "      <td>0.761279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.243570</td>\n",
              "      <td>0.882905</td>\n",
              "      <td>0.940174</td>\n",
              "      <td>0.245532</td>\n",
              "      <td>0.887095</td>\n",
              "      <td>0.958038</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.256413</td>\n",
              "      <td>0.898429</td>\n",
              "      <td>0.953986</td>\n",
              "      <td>0.266779</td>\n",
              "      <td>0.908252</td>\n",
              "      <td>0.967286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.294021</td>\n",
              "      <td>0.903404</td>\n",
              "      <td>0.958907</td>\n",
              "      <td>0.319374</td>\n",
              "      <td>0.913993</td>\n",
              "      <td>0.970373</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.593011</td>\n",
              "      <td>0.907905</td>\n",
              "      <td>0.962391</td>\n",
              "      <td>0.588777</td>\n",
              "      <td>0.920084</td>\n",
              "      <td>0.973177</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            loss  binary_accuracy  ...  val_binary_accuracy    val_auc\n",
              "count  71.000000        71.000000  ...            71.000000  71.000000\n",
              "mean    0.288592         0.879161  ...             0.889398   0.950575\n",
              "std     0.082076         0.051673  ...             0.047972   0.048127\n",
              "min     0.233717         0.658265  ...             0.720696   0.761279\n",
              "25%     0.243570         0.882905  ...             0.887095   0.958038\n",
              "50%     0.256413         0.898429  ...             0.908252   0.967286\n",
              "75%     0.294021         0.903404  ...             0.913993   0.970373\n",
              "max     0.593011         0.907905  ...             0.920084   0.973177\n",
              "\n",
              "[8 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 267
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k1uxdHM6hp5",
        "outputId": "83b3888d-333f-4f08-98e2-93dc46dcd7b4"
      },
      "source": [
        "train_acc = top_50_nn.evaluate(train_X, train_y)\n",
        "test_acc = top_50_nn.evaluate(val_X, val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2078/2078 [==============================] - 3s 2ms/step - loss: 0.2505 - binary_accuracy: 0.9194 - auc: 0.9709\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2520 - binary_accuracy: 0.9198 - auc: 0.9708\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1azfhxBI6hp5",
        "outputId": "125ca1f8-c0a9-4a70-ae1e-38bbb9a66878"
      },
      "source": [
        "dict(zip(top_50_nn.metrics_names, test_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'auc': 0.9707860350608826,\n",
              " 'binary_accuracy': 0.9197725653648376,\n",
              " 'loss': 0.2520207464694977}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 269
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "N01Zt4Pq6hp5",
        "outputId": "bde6c1a2-ca33-4d79-9241-8068cb821cc7"
      },
      "source": [
        "history_df1.loc[:, ['loss', 'val_loss']].plot();\n",
        "print(\"Minimum validation loss (binary_crossentropy): {}\".format(history_df1['val_loss'].min()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Minimum validation loss (binary_crossentropy): 0.21716155111789703\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "eEggKigE6hp6",
        "outputId": "ce7634f0-9d86-4a9e-aaa0-83799f76abc4"
      },
      "source": [
        "history_df1.loc[:, ['auc', 'val_auc']].plot();\n",
        "print(\"Maximum AUC: {}\".format(history_df1['val_auc'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum AUC: 0.9731769561767578\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "H59tsLbp6hp6",
        "outputId": "52b5dcba-aff2-44e1-e2a1-d7eb22723741"
      },
      "source": [
        "history_df1.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot();\n",
        "print(\"Maximum validation binary accuracy: {}\".format(history_df1['val_binary_accuracy'].max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Maximum validation binary accuracy: 0.9200842380523682\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCCsH00z6hp6",
        "outputId": "6cb1c318-3f2b-4ca9-a412-1531dd281a5a"
      },
      "source": [
        "history_df1.binary_accuracy.max() - history_df1.val_binary_accuracy.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.012179017066955566"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 273
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGAVzrB_6hp7",
        "outputId": "c2ab3e06-c67b-4acd-f1bb-7bceba1bb9e1"
      },
      "source": [
        "preds1 = top_50_nn.predict(val_X)\n",
        "preds1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.91260344],\n",
              "       [0.02085212],\n",
              "       [0.41528523],\n",
              "       ...,\n",
              "       [0.36616087],\n",
              "       [0.05954185],\n",
              "       [0.9062793 ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gh0R_FRa6hp7",
        "outputId": "db635ed7-afff-4237-9707-cd99963c431b"
      },
      "source": [
        "len(preds1[preds1 <= 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13777"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XRbc24626hp7",
        "outputId": "4341f24a-2095-4421-9b32-8495d793629a"
      },
      "source": [
        "len(preds1[preds1 > 0.5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8385"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85jNM1QD6hp8",
        "outputId": "7928578a-7d07-4d82-e3d8-54277826ae63"
      },
      "source": [
        "len(val_y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22162"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 277
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "1KU4SvtW6hp8",
        "outputId": "c6b496d0-1813-4c53-df61-c5016384efc7"
      },
      "source": [
        "preds_df = pd.DataFrame(preds1, columns = ['preds'])\n",
        "\n",
        "preds_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.912603</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.020852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.415285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.091382</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.747978</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      preds\n",
              "0  0.912603\n",
              "1  0.020852\n",
              "2  0.415285\n",
              "3  0.091382\n",
              "4  0.747978"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 278
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "QFnNR1By6hp8",
        "outputId": "350f0561-e053-429c-a049-35a70a22bf34"
      },
      "source": [
        "preds_df = pd.concat([preds_df, val_y.reset_index(drop=True), val_X.reset_index()], axis=1)\n",
        "\n",
        "preds_df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>preds</th>\n",
              "      <th>phishing</th>\n",
              "      <th>index</th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>qty_dot_directory</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>qty_at_directory</th>\n",
              "      <th>qty_and_directory</th>\n",
              "      <th>qty_space_directory</th>\n",
              "      <th>qty_tilde_directory</th>\n",
              "      <th>qty_comma_directory</th>\n",
              "      <th>qty_plus_directory</th>\n",
              "      <th>qty_asterisk_directory</th>\n",
              "      <th>qty_hashtag_directory</th>\n",
              "      <th>qty_dollar_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_dot_file</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>qty_underline_file</th>\n",
              "      <th>qty_slash_file</th>\n",
              "      <th>qty_questionmark_file</th>\n",
              "      <th>qty_equal_file</th>\n",
              "      <th>qty_at_file</th>\n",
              "      <th>qty_and_file</th>\n",
              "      <th>qty_exclamation_file</th>\n",
              "      <th>qty_space_file</th>\n",
              "      <th>qty_tilde_file</th>\n",
              "      <th>qty_comma_file</th>\n",
              "      <th>qty_plus_file</th>\n",
              "      <th>qty_asterisk_file</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>params_length</th>\n",
              "      <th>time_response</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.912603</td>\n",
              "      <td>1</td>\n",
              "      <td>62575</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.003517</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002345</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000690</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.138820</td>\n",
              "      <td>0.037377</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.989602</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.020852</td>\n",
              "      <td>0</td>\n",
              "      <td>38126</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.001012</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.786588</td>\n",
              "      <td>0.260471</td>\n",
              "      <td>0.009454</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.559770</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.415285</td>\n",
              "      <td>0</td>\n",
              "      <td>1617</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.001312</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.921059</td>\n",
              "      <td>0.388661</td>\n",
              "      <td>0.014919</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.018994</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.091382</td>\n",
              "      <td>0</td>\n",
              "      <td>8228</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.948421</td>\n",
              "      <td>0.119534</td>\n",
              "      <td>0.021672</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.292813</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.747978</td>\n",
              "      <td>1</td>\n",
              "      <td>55594</td>\n",
              "      <td>0.054944</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.054944</td>\n",
              "      <td>0.769219</td>\n",
              "      <td>0.027472</td>\n",
              "      <td>0.109888</td>\n",
              "      <td>0.384610</td>\n",
              "      <td>0.027472</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027472</td>\n",
              "      <td>0.054944</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.384610</td>\n",
              "      <td>0.027472</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.027472</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.274721</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.109888</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22157</th>\n",
              "      <td>0.000520</td>\n",
              "      <td>0</td>\n",
              "      <td>65294</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001480</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.001480</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.684142</td>\n",
              "      <td>0.658983</td>\n",
              "      <td>0.008618</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.312430</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.000087</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22158</th>\n",
              "      <td>0.016039</td>\n",
              "      <td>0</td>\n",
              "      <td>10038</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.891957</td>\n",
              "      <td>0.419925</td>\n",
              "      <td>0.166351</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.020000</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22159</th>\n",
              "      <td>0.366161</td>\n",
              "      <td>0</td>\n",
              "      <td>43642</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001710</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.001710</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.999761</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.021711</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22160</th>\n",
              "      <td>0.059542</td>\n",
              "      <td>0</td>\n",
              "      <td>73632</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000624</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.838648</td>\n",
              "      <td>0.166678</td>\n",
              "      <td>0.008666</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.518471</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22161</th>\n",
              "      <td>0.906279</td>\n",
              "      <td>1</td>\n",
              "      <td>25895</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000655</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000225</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.954190</td>\n",
              "      <td>0.051122</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.294798</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000020</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>22162 rows × 53 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          preds  phishing  ...  tls_ssl_certificate  qty_redirects\n",
              "0      0.912603         1  ...             0.000000       0.000000\n",
              "1      0.020852         0  ...             0.000039       0.000000\n",
              "2      0.415285         0  ...             0.000069       0.000138\n",
              "3      0.091382         0  ...             0.000020       0.000020\n",
              "4      0.747978         1  ...             0.000000       0.000000\n",
              "...         ...       ...  ...                  ...            ...\n",
              "22157  0.000520         0  ...             0.000087       0.000087\n",
              "22158  0.016039         0  ...             0.000067       0.000067\n",
              "22159  0.366161         0  ...             0.000000       0.000000\n",
              "22160  0.059542         0  ...             0.000000       0.000024\n",
              "22161  0.906279         1  ...             0.000000       0.000020\n",
              "\n",
              "[22162 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrD7kQRy6hp9",
        "outputId": "c69827a1-aeb4-46bc-f2b2-2a4d4da89ace"
      },
      "source": [
        "pred_classes = np.argmax(preds1, axis=1)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "cm = confusion_matrix(val_y, pred_classes)\n",
        "print(cm)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[14519     0]\n",
            " [ 7643     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvD349puz8EZ"
      },
      "source": [
        "# neural network on top 50 most important features per recursive feature elimination package [model as feature]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "2qCK18Vm0K3a",
        "outputId": "000d55f3-d1c0-4e44-a483-aae873d08db2"
      },
      "source": [
        "y = full_df.iloc[:,-1]\n",
        "\n",
        "features = ['qty_dot_url', 'qty_hyphen_url', 'qty_slash_url', 'length_url',\n",
        "       'qty_dot_domain', 'qty_vowels_domain', 'domain_length',\n",
        "       'qty_dot_directory', 'qty_hyphen_directory', 'qty_underline_directory',\n",
        "       'qty_slash_directory', 'qty_at_directory', 'qty_and_directory',\n",
        "       'qty_space_directory', 'qty_tilde_directory', 'qty_comma_directory',\n",
        "       'qty_plus_directory', 'qty_asterisk_directory', 'qty_hashtag_directory',\n",
        "       'qty_dollar_directory', 'directory_length', 'qty_dot_file',\n",
        "       'qty_hyphen_file', 'qty_underline_file', 'qty_slash_file',\n",
        "       'qty_questionmark_file', 'qty_equal_file', 'qty_at_file',\n",
        "       'qty_and_file', 'qty_exclamation_file', 'qty_space_file',\n",
        "       'qty_tilde_file', 'qty_comma_file', 'qty_plus_file',\n",
        "       'qty_asterisk_file', 'qty_hashtag_file', 'qty_dollar_file',\n",
        "       'qty_percent_file', 'file_length', 'params_length', 'time_response',\n",
        "       'asn_ip', 'time_domain_activation', 'time_domain_expiration',\n",
        "       'qty_ip_resolved', 'qty_nameservers', 'qty_mx_servers', 'ttl_hostname',\n",
        "       'tls_ssl_certificate', 'qty_redirects']\n",
        "X = full_df[features]\n",
        "\n",
        "X = tf.keras.utils.normalize(X, axis=-1, order=2)\n",
        "\n",
        "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.25, random_state=808)\n",
        "\n",
        "train_X.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qty_dot_url</th>\n",
              "      <th>qty_hyphen_url</th>\n",
              "      <th>qty_slash_url</th>\n",
              "      <th>length_url</th>\n",
              "      <th>qty_dot_domain</th>\n",
              "      <th>qty_vowels_domain</th>\n",
              "      <th>domain_length</th>\n",
              "      <th>qty_dot_directory</th>\n",
              "      <th>qty_hyphen_directory</th>\n",
              "      <th>qty_underline_directory</th>\n",
              "      <th>qty_slash_directory</th>\n",
              "      <th>qty_at_directory</th>\n",
              "      <th>qty_and_directory</th>\n",
              "      <th>qty_space_directory</th>\n",
              "      <th>qty_tilde_directory</th>\n",
              "      <th>qty_comma_directory</th>\n",
              "      <th>qty_plus_directory</th>\n",
              "      <th>qty_asterisk_directory</th>\n",
              "      <th>qty_hashtag_directory</th>\n",
              "      <th>qty_dollar_directory</th>\n",
              "      <th>directory_length</th>\n",
              "      <th>qty_dot_file</th>\n",
              "      <th>qty_hyphen_file</th>\n",
              "      <th>qty_underline_file</th>\n",
              "      <th>qty_slash_file</th>\n",
              "      <th>qty_questionmark_file</th>\n",
              "      <th>qty_equal_file</th>\n",
              "      <th>qty_at_file</th>\n",
              "      <th>qty_and_file</th>\n",
              "      <th>qty_exclamation_file</th>\n",
              "      <th>qty_space_file</th>\n",
              "      <th>qty_tilde_file</th>\n",
              "      <th>qty_comma_file</th>\n",
              "      <th>qty_plus_file</th>\n",
              "      <th>qty_asterisk_file</th>\n",
              "      <th>qty_hashtag_file</th>\n",
              "      <th>qty_dollar_file</th>\n",
              "      <th>qty_percent_file</th>\n",
              "      <th>file_length</th>\n",
              "      <th>params_length</th>\n",
              "      <th>time_response</th>\n",
              "      <th>asn_ip</th>\n",
              "      <th>time_domain_activation</th>\n",
              "      <th>time_domain_expiration</th>\n",
              "      <th>qty_ip_resolved</th>\n",
              "      <th>qty_nameservers</th>\n",
              "      <th>qty_mx_servers</th>\n",
              "      <th>ttl_hostname</th>\n",
              "      <th>tls_ssl_certificate</th>\n",
              "      <th>qty_redirects</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5676</th>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.999934</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.011480</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39002</th>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.003369</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.730210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.680199</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.000518</td>\n",
              "      <td>0.064003</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1732</th>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000737</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.698307</td>\n",
              "      <td>0.269674</td>\n",
              "      <td>0.016112</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.662860</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39668</th>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.001588</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.859057</td>\n",
              "      <td>0.261792</td>\n",
              "      <td>0.005926</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.439823</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82035</th>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000751</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.795440</td>\n",
              "      <td>0.178787</td>\n",
              "      <td>0.007212</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.579014</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       qty_dot_url  qty_hyphen_url  ...  tls_ssl_certificate  qty_redirects\n",
              "5676      0.000004             0.0  ...             0.000004       0.000000\n",
              "39002     0.000777             0.0  ...             0.000259       0.000259\n",
              "1732      0.000092             0.0  ...             0.000000       0.000000\n",
              "39668     0.000122             0.0  ...             0.000000       0.000061\n",
              "82035     0.000054             0.0  ...             0.000000       0.000000\n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 283
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AL_9I51z8Ee"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "earlystopping = callbacks.EarlyStopping(monitor = 'val_accuracy', mode = 'max',\n",
        "                                         patience = 15, restore_best_weights = True)\n",
        "                                         \n",
        "def phish_nn_top50():\n",
        "  model = keras.Sequential()\n",
        "  model.add(tf.keras.layers.InputLayer(input_shape=[50]))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=64, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=50, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.20))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.2))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=16, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.40))\n",
        "  model.add(tf.keras.layers.Dense(units=111, activation='relu'))\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  model.fit(train_X, train_y, validation_split=0.30, batch_size= 175, epochs=50, callbacks = [earlystopping], workers=8)\n",
        "  model.predict(val_X)\n",
        "  model.evaluate(val_X, val_y)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrarCNYVz8Ee"
      },
      "source": [
        "mod_top50 = KerasClassifier(build_fn=phish_nn_top50,\n",
        "                        epochs=10,\n",
        "                        batch_size=175)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCn6EsLtz8Ee",
        "outputId": "e3d36c50-22dd-4593-f753-49c9e0d36417"
      },
      "source": [
        "num_folds=5\n",
        "kfold=StratifiedKFold(n_splits=num_folds, shuffle=True)\n",
        "\n",
        "cv_results_top50 = cross_val_score(mod_top50,\n",
        "                           X, y,\n",
        "                           cv=kfold\n",
        "                           )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 5ms/step - loss: 0.6203 - accuracy: 0.6551 - val_loss: 0.5781 - val_accuracy: 0.7224\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5605 - accuracy: 0.7075 - val_loss: 0.5820 - val_accuracy: 0.7257\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5436 - accuracy: 0.7227 - val_loss: 0.5754 - val_accuracy: 0.7260\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5335 - accuracy: 0.7265 - val_loss: 0.5861 - val_accuracy: 0.7383\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5157 - accuracy: 0.7386 - val_loss: 0.4971 - val_accuracy: 0.7695\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4762 - accuracy: 0.7718 - val_loss: 0.4675 - val_accuracy: 0.8101\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4231 - accuracy: 0.8133 - val_loss: 0.5076 - val_accuracy: 0.7901\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3876 - accuracy: 0.8402 - val_loss: 0.4902 - val_accuracy: 0.8801\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3514 - accuracy: 0.8590 - val_loss: 0.4118 - val_accuracy: 0.8884\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3263 - accuracy: 0.8730 - val_loss: 0.3936 - val_accuracy: 0.8906\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3250 - accuracy: 0.8711 - val_loss: 0.3657 - val_accuracy: 0.9035\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3103 - accuracy: 0.8780 - val_loss: 0.3393 - val_accuracy: 0.8924\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3027 - accuracy: 0.8803 - val_loss: 0.3380 - val_accuracy: 0.9015\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2977 - accuracy: 0.8823 - val_loss: 0.4248 - val_accuracy: 0.8074\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3005 - accuracy: 0.8810 - val_loss: 0.4212 - val_accuracy: 0.8157\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2885 - accuracy: 0.8871 - val_loss: 0.3145 - val_accuracy: 0.8908\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2919 - accuracy: 0.8819 - val_loss: 0.2887 - val_accuracy: 0.9120\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2866 - accuracy: 0.8830 - val_loss: 0.5244 - val_accuracy: 0.7841\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2827 - accuracy: 0.8879 - val_loss: 0.2947 - val_accuracy: 0.9108\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2805 - accuracy: 0.8891 - val_loss: 0.4030 - val_accuracy: 0.8883\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2768 - accuracy: 0.8886 - val_loss: 0.2715 - val_accuracy: 0.9076\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2752 - accuracy: 0.8930 - val_loss: 0.2680 - val_accuracy: 0.9150\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2638 - accuracy: 0.8968 - val_loss: 0.2774 - val_accuracy: 0.9153\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8909 - val_loss: 0.2844 - val_accuracy: 0.9158\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2639 - accuracy: 0.8984 - val_loss: 0.3138 - val_accuracy: 0.8754\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2597 - accuracy: 0.8952 - val_loss: 0.2774 - val_accuracy: 0.9061\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2656 - accuracy: 0.8977 - val_loss: 0.2884 - val_accuracy: 0.9119\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2546 - accuracy: 0.8984 - val_loss: 0.2972 - val_accuracy: 0.9048\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2577 - accuracy: 0.8987 - val_loss: 0.2581 - val_accuracy: 0.9129\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2562 - accuracy: 0.8994 - val_loss: 0.2555 - val_accuracy: 0.9163\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2635 - accuracy: 0.8942 - val_loss: 0.2311 - val_accuracy: 0.9103\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2687 - accuracy: 0.8952 - val_loss: 0.2824 - val_accuracy: 0.9200\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2456 - accuracy: 0.9049 - val_loss: 0.3417 - val_accuracy: 0.9157\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2666 - accuracy: 0.8951 - val_loss: 0.2562 - val_accuracy: 0.9120\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2510 - accuracy: 0.8996 - val_loss: 0.2900 - val_accuracy: 0.9102\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2467 - accuracy: 0.9040 - val_loss: 0.2379 - val_accuracy: 0.9189\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2503 - accuracy: 0.9025 - val_loss: 0.3118 - val_accuracy: 0.9055\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.9017 - val_loss: 0.3164 - val_accuracy: 0.9072\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8983 - val_loss: 0.2668 - val_accuracy: 0.9185\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2424 - accuracy: 0.9056 - val_loss: 0.2658 - val_accuracy: 0.9120\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2535 - accuracy: 0.9010 - val_loss: 0.2638 - val_accuracy: 0.9133\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2487 - accuracy: 0.9001 - val_loss: 0.2916 - val_accuracy: 0.9186\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2448 - accuracy: 0.9057 - val_loss: 0.3509 - val_accuracy: 0.9096\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2521 - accuracy: 0.9011 - val_loss: 0.2841 - val_accuracy: 0.9223\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2529 - accuracy: 0.9023 - val_loss: 0.2500 - val_accuracy: 0.9153\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2452 - accuracy: 0.9027 - val_loss: 0.3184 - val_accuracy: 0.8921\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2496 - accuracy: 0.9054 - val_loss: 0.2467 - val_accuracy: 0.9189\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2505 - accuracy: 0.9025 - val_loss: 0.2430 - val_accuracy: 0.9183\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2408 - accuracy: 0.9061 - val_loss: 0.2470 - val_accuracy: 0.9100\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2424 - accuracy: 0.9052 - val_loss: 0.2630 - val_accuracy: 0.9198\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2658 - accuracy: 0.9194\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2418 - accuracy: 0.9052\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2402 - accuracy: 0.9068\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2399 - accuracy: 0.9065\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2408 - accuracy: 0.9062\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2440 - accuracy: 0.9040\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2404 - accuracy: 0.9058\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2396 - accuracy: 0.9051\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2383 - accuracy: 0.9071\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2364 - accuracy: 0.9081\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2360 - accuracy: 0.9089\n",
            "102/102 [==============================] - 0s 1ms/step - loss: 0.2456 - accuracy: 0.9222\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6277 - accuracy: 0.6493 - val_loss: 0.5818 - val_accuracy: 0.7203\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5593 - accuracy: 0.7058 - val_loss: 0.5772 - val_accuracy: 0.7236\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5434 - accuracy: 0.7236 - val_loss: 0.5785 - val_accuracy: 0.7296\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5321 - accuracy: 0.7289 - val_loss: 0.5545 - val_accuracy: 0.7394\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5144 - accuracy: 0.7393 - val_loss: 0.5048 - val_accuracy: 0.7646\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4663 - accuracy: 0.7740 - val_loss: 0.4610 - val_accuracy: 0.8322\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4261 - accuracy: 0.8093 - val_loss: 0.4325 - val_accuracy: 0.8429\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3841 - accuracy: 0.8381 - val_loss: 0.5054 - val_accuracy: 0.8082\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3666 - accuracy: 0.8468 - val_loss: 0.4618 - val_accuracy: 0.8455\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3381 - accuracy: 0.8647 - val_loss: 0.3925 - val_accuracy: 0.8558\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3280 - accuracy: 0.8681 - val_loss: 0.3316 - val_accuracy: 0.8847\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3153 - accuracy: 0.8753 - val_loss: 0.4403 - val_accuracy: 0.8666\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3047 - accuracy: 0.8783 - val_loss: 0.3788 - val_accuracy: 0.8792\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2923 - accuracy: 0.8812 - val_loss: 0.3380 - val_accuracy: 0.8934\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8869 - val_loss: 0.3427 - val_accuracy: 0.8911\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2894 - accuracy: 0.8834 - val_loss: 0.3264 - val_accuracy: 0.8849\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2834 - accuracy: 0.8837 - val_loss: 0.3265 - val_accuracy: 0.8983\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2729 - accuracy: 0.8903 - val_loss: 0.3389 - val_accuracy: 0.8991\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2772 - accuracy: 0.8871 - val_loss: 0.3094 - val_accuracy: 0.8982\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2769 - accuracy: 0.8894 - val_loss: 0.3141 - val_accuracy: 0.9050\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2712 - accuracy: 0.8906 - val_loss: 0.3112 - val_accuracy: 0.9018\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2689 - accuracy: 0.8910 - val_loss: 0.3023 - val_accuracy: 0.9066\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2697 - accuracy: 0.8898 - val_loss: 0.2561 - val_accuracy: 0.9099\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8942 - val_loss: 0.2591 - val_accuracy: 0.9124\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2561 - accuracy: 0.8975 - val_loss: 0.2659 - val_accuracy: 0.9108\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8951 - val_loss: 0.3070 - val_accuracy: 0.9125\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8990 - val_loss: 0.2936 - val_accuracy: 0.9120\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2583 - accuracy: 0.8959 - val_loss: 0.3024 - val_accuracy: 0.9025\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2604 - accuracy: 0.8932 - val_loss: 0.4097 - val_accuracy: 0.8795\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8951 - val_loss: 0.3333 - val_accuracy: 0.8879\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2610 - accuracy: 0.8963 - val_loss: 0.2336 - val_accuracy: 0.9154\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2547 - accuracy: 0.8963 - val_loss: 0.3037 - val_accuracy: 0.9082\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2544 - accuracy: 0.8967 - val_loss: 0.2845 - val_accuracy: 0.8914\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8983 - val_loss: 0.2568 - val_accuracy: 0.9182\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2448 - accuracy: 0.9022 - val_loss: 0.3254 - val_accuracy: 0.9141\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2459 - accuracy: 0.9021 - val_loss: 0.2580 - val_accuracy: 0.9157\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2427 - accuracy: 0.9030 - val_loss: 0.2691 - val_accuracy: 0.8955\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2432 - accuracy: 0.9025 - val_loss: 0.2396 - val_accuracy: 0.9158\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2406 - accuracy: 0.9034 - val_loss: 0.3105 - val_accuracy: 0.9056\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2423 - accuracy: 0.8998 - val_loss: 0.2200 - val_accuracy: 0.9176\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2409 - accuracy: 0.9025 - val_loss: 0.2654 - val_accuracy: 0.9076\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2428 - accuracy: 0.8999 - val_loss: 0.2820 - val_accuracy: 0.9098\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2409 - accuracy: 0.9051 - val_loss: 0.2195 - val_accuracy: 0.9162\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2412 - accuracy: 0.9035 - val_loss: 0.2461 - val_accuracy: 0.9129\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2442 - accuracy: 0.9038 - val_loss: 0.2553 - val_accuracy: 0.9045\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2431 - accuracy: 0.9039 - val_loss: 0.3324 - val_accuracy: 0.8910\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2441 - accuracy: 0.9010 - val_loss: 0.2601 - val_accuracy: 0.9037\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2388 - accuracy: 0.9045 - val_loss: 0.2419 - val_accuracy: 0.9134\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2327 - accuracy: 0.9071 - val_loss: 0.2676 - val_accuracy: 0.8890\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2588 - accuracy: 0.9167\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2487 - accuracy: 0.9020\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2435 - accuracy: 0.9037\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2438 - accuracy: 0.9016\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2429 - accuracy: 0.9034\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2416 - accuracy: 0.9038\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2403 - accuracy: 0.9048\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2432 - accuracy: 0.9031\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2375 - accuracy: 0.9069\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2348 - accuracy: 0.9065\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2365 - accuracy: 0.9053\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2556 - accuracy: 0.9100\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6318 - accuracy: 0.6420 - val_loss: 0.5718 - val_accuracy: 0.6592\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5586 - accuracy: 0.7004 - val_loss: 0.5692 - val_accuracy: 0.7249\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5448 - accuracy: 0.7186 - val_loss: 0.5566 - val_accuracy: 0.7296\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5344 - accuracy: 0.7251 - val_loss: 0.5328 - val_accuracy: 0.7428\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5085 - accuracy: 0.7458 - val_loss: 0.5256 - val_accuracy: 0.7502\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4612 - accuracy: 0.7806 - val_loss: 0.4345 - val_accuracy: 0.8029\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4107 - accuracy: 0.8186 - val_loss: 0.4142 - val_accuracy: 0.8509\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3795 - accuracy: 0.8457 - val_loss: 0.3728 - val_accuracy: 0.8808\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3551 - accuracy: 0.8558 - val_loss: 0.3851 - val_accuracy: 0.8664\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3360 - accuracy: 0.8667 - val_loss: 0.3366 - val_accuracy: 0.8915\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3343 - accuracy: 0.8642 - val_loss: 0.4770 - val_accuracy: 0.8777\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3121 - accuracy: 0.8754 - val_loss: 0.2895 - val_accuracy: 0.8920\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3065 - accuracy: 0.8806 - val_loss: 0.2987 - val_accuracy: 0.9109\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2971 - accuracy: 0.8818 - val_loss: 0.3049 - val_accuracy: 0.9008\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2983 - accuracy: 0.8826 - val_loss: 0.2987 - val_accuracy: 0.8921\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2927 - accuracy: 0.8831 - val_loss: 0.3217 - val_accuracy: 0.9085\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2909 - accuracy: 0.8869 - val_loss: 0.2503 - val_accuracy: 0.9130\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2767 - accuracy: 0.8898 - val_loss: 0.2617 - val_accuracy: 0.9043\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2733 - accuracy: 0.8892 - val_loss: 0.2587 - val_accuracy: 0.9052\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2716 - accuracy: 0.8936 - val_loss: 0.2528 - val_accuracy: 0.9111\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2721 - accuracy: 0.8917 - val_loss: 0.2817 - val_accuracy: 0.9031\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2716 - accuracy: 0.8881 - val_loss: 0.3353 - val_accuracy: 0.9143\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2706 - accuracy: 0.8929 - val_loss: 0.2552 - val_accuracy: 0.9127\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2683 - accuracy: 0.8930 - val_loss: 0.2701 - val_accuracy: 0.9168\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2578 - accuracy: 0.8983 - val_loss: 0.2285 - val_accuracy: 0.9143\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2611 - accuracy: 0.8956 - val_loss: 0.2659 - val_accuracy: 0.8948\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2513 - accuracy: 0.9021 - val_loss: 0.2540 - val_accuracy: 0.9164\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2600 - accuracy: 0.8985 - val_loss: 0.2317 - val_accuracy: 0.9118\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2569 - accuracy: 0.8964 - val_loss: 0.2283 - val_accuracy: 0.9154\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2614 - accuracy: 0.8940 - val_loss: 0.2679 - val_accuracy: 0.9014\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2576 - accuracy: 0.8985 - val_loss: 0.2536 - val_accuracy: 0.9142\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2525 - accuracy: 0.9010 - val_loss: 0.2341 - val_accuracy: 0.9149\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2463 - accuracy: 0.9036 - val_loss: 0.2205 - val_accuracy: 0.9200\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2415 - accuracy: 0.9040 - val_loss: 0.3059 - val_accuracy: 0.8832\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2495 - accuracy: 0.9041 - val_loss: 0.2467 - val_accuracy: 0.9114\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2434 - accuracy: 0.9050 - val_loss: 0.2465 - val_accuracy: 0.9184\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2481 - accuracy: 0.9031 - val_loss: 0.2163 - val_accuracy: 0.9174\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2462 - accuracy: 0.9028 - val_loss: 0.2370 - val_accuracy: 0.9135\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2426 - accuracy: 0.9046 - val_loss: 0.2194 - val_accuracy: 0.9095\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2378 - accuracy: 0.9060 - val_loss: 0.2549 - val_accuracy: 0.9192\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2468 - accuracy: 0.9021 - val_loss: 0.2842 - val_accuracy: 0.9111\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2414 - accuracy: 0.9069 - val_loss: 0.2097 - val_accuracy: 0.9242\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2434 - accuracy: 0.9047 - val_loss: 0.2252 - val_accuracy: 0.9159\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2486 - accuracy: 0.9022 - val_loss: 0.2507 - val_accuracy: 0.9080\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2471 - accuracy: 0.8995 - val_loss: 0.2689 - val_accuracy: 0.9161\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2440 - accuracy: 0.9048 - val_loss: 0.2110 - val_accuracy: 0.9167\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2399 - accuracy: 0.9048 - val_loss: 0.2356 - val_accuracy: 0.9180\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2426 - accuracy: 0.9033 - val_loss: 0.2500 - val_accuracy: 0.9158\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2331 - accuracy: 0.9078 - val_loss: 0.2632 - val_accuracy: 0.9124\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2449 - accuracy: 0.9035 - val_loss: 0.2094 - val_accuracy: 0.9149\n",
            "693/693 [==============================] - 1s 2ms/step - loss: 0.2149 - accuracy: 0.9123\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2389 - accuracy: 0.9051\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2355 - accuracy: 0.9067\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2355 - accuracy: 0.9070\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2383 - accuracy: 0.9057\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2366 - accuracy: 0.9060\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2351 - accuracy: 0.9068\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2323 - accuracy: 0.9076\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2332 - accuracy: 0.9082\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2285 - accuracy: 0.9095\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2302 - accuracy: 0.9089\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2087 - accuracy: 0.9150\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6347 - accuracy: 0.6365 - val_loss: 0.5540 - val_accuracy: 0.7202\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5574 - accuracy: 0.7044 - val_loss: 0.5621 - val_accuracy: 0.7248\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5453 - accuracy: 0.7130 - val_loss: 0.5373 - val_accuracy: 0.7273\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.5419 - accuracy: 0.7155 - val_loss: 0.5414 - val_accuracy: 0.7307\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.5231 - accuracy: 0.7260 - val_loss: 0.5092 - val_accuracy: 0.7490\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4835 - accuracy: 0.7604 - val_loss: 0.4303 - val_accuracy: 0.7896\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4267 - accuracy: 0.7972 - val_loss: 0.4341 - val_accuracy: 0.8384\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3885 - accuracy: 0.8272 - val_loss: 0.3840 - val_accuracy: 0.8747\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3449 - accuracy: 0.8525 - val_loss: 0.3805 - val_accuracy: 0.8823\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3352 - accuracy: 0.8606 - val_loss: 0.3612 - val_accuracy: 0.8823\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3220 - accuracy: 0.8631 - val_loss: 0.3195 - val_accuracy: 0.8951\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3116 - accuracy: 0.8651 - val_loss: 0.3302 - val_accuracy: 0.8684\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3095 - accuracy: 0.8725 - val_loss: 0.3196 - val_accuracy: 0.8802\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3003 - accuracy: 0.8777 - val_loss: 0.3844 - val_accuracy: 0.8917\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3026 - accuracy: 0.8752 - val_loss: 0.2768 - val_accuracy: 0.8916\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2852 - accuracy: 0.8815 - val_loss: 0.3172 - val_accuracy: 0.8898\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2789 - accuracy: 0.8852 - val_loss: 0.3250 - val_accuracy: 0.9142\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2774 - accuracy: 0.8860 - val_loss: 0.3000 - val_accuracy: 0.9010\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2755 - accuracy: 0.8872 - val_loss: 0.2590 - val_accuracy: 0.8979\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2619 - accuracy: 0.8941 - val_loss: 0.2984 - val_accuracy: 0.9047\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2748 - accuracy: 0.8853 - val_loss: 0.3164 - val_accuracy: 0.8941\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2742 - accuracy: 0.8864 - val_loss: 0.2445 - val_accuracy: 0.9098\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2765 - accuracy: 0.8886 - val_loss: 0.2656 - val_accuracy: 0.9085\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8938 - val_loss: 0.2587 - val_accuracy: 0.9066\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2569 - accuracy: 0.8959 - val_loss: 0.2761 - val_accuracy: 0.9074\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2561 - accuracy: 0.8967 - val_loss: 0.2563 - val_accuracy: 0.9143\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2603 - accuracy: 0.8945 - val_loss: 0.2739 - val_accuracy: 0.9151\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2598 - accuracy: 0.8959 - val_loss: 0.2417 - val_accuracy: 0.9153\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2450 - accuracy: 0.9030 - val_loss: 0.2632 - val_accuracy: 0.9143\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2549 - accuracy: 0.8987 - val_loss: 0.2601 - val_accuracy: 0.9115\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2526 - accuracy: 0.8992 - val_loss: 0.2674 - val_accuracy: 0.9151\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2622 - accuracy: 0.8939 - val_loss: 0.2530 - val_accuracy: 0.9172\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2547 - accuracy: 0.8966 - val_loss: 0.2355 - val_accuracy: 0.9170\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2537 - accuracy: 0.9004 - val_loss: 0.2293 - val_accuracy: 0.9100\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2501 - accuracy: 0.9003 - val_loss: 0.2360 - val_accuracy: 0.9054\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2451 - accuracy: 0.9039 - val_loss: 0.2947 - val_accuracy: 0.9118\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2450 - accuracy: 0.9000 - val_loss: 0.2650 - val_accuracy: 0.9155\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2446 - accuracy: 0.9041 - val_loss: 0.2291 - val_accuracy: 0.9159\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2507 - accuracy: 0.8994 - val_loss: 0.2600 - val_accuracy: 0.9096\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2529 - accuracy: 0.8980 - val_loss: 0.2506 - val_accuracy: 0.9147\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2479 - accuracy: 0.9014 - val_loss: 0.2139 - val_accuracy: 0.9185\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2435 - accuracy: 0.9006 - val_loss: 0.2623 - val_accuracy: 0.9083\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.8987 - val_loss: 0.2651 - val_accuracy: 0.9166\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.9027 - val_loss: 0.2469 - val_accuracy: 0.9194\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2427 - accuracy: 0.9025 - val_loss: 0.2452 - val_accuracy: 0.9107\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2561 - accuracy: 0.8985 - val_loss: 0.2267 - val_accuracy: 0.9182\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2431 - accuracy: 0.9021 - val_loss: 0.2181 - val_accuracy: 0.9179\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2431 - accuracy: 0.9027 - val_loss: 0.2305 - val_accuracy: 0.9181\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2437 - accuracy: 0.9042 - val_loss: 0.2211 - val_accuracy: 0.9189\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2392 - accuracy: 0.9032 - val_loss: 0.3049 - val_accuracy: 0.9040\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.3086 - accuracy: 0.9014\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2431 - accuracy: 0.9024\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2452 - accuracy: 0.9020\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2458 - accuracy: 0.9018\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2402 - accuracy: 0.9047\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2402 - accuracy: 0.9045\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2479 - accuracy: 0.8998\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2412 - accuracy: 0.9036\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2428 - accuracy: 0.9016\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2387 - accuracy: 0.9047\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2365 - accuracy: 0.9065\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.9174\n",
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 7ms/step - loss: 0.6266 - accuracy: 0.6420 - val_loss: 0.5604 - val_accuracy: 0.7206\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5596 - accuracy: 0.7044 - val_loss: 0.5686 - val_accuracy: 0.7238\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5475 - accuracy: 0.7213 - val_loss: 0.5798 - val_accuracy: 0.7270\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5348 - accuracy: 0.7267 - val_loss: 0.5447 - val_accuracy: 0.7432\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5169 - accuracy: 0.7352 - val_loss: 0.5191 - val_accuracy: 0.7643\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4757 - accuracy: 0.7677 - val_loss: 0.4481 - val_accuracy: 0.8209\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4321 - accuracy: 0.8024 - val_loss: 0.4449 - val_accuracy: 0.8126\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3965 - accuracy: 0.8256 - val_loss: 0.4013 - val_accuracy: 0.8700\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3778 - accuracy: 0.8453 - val_loss: 0.4191 - val_accuracy: 0.8491\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3427 - accuracy: 0.8662 - val_loss: 0.4353 - val_accuracy: 0.8467\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3278 - accuracy: 0.8709 - val_loss: 0.3819 - val_accuracy: 0.8944\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3218 - accuracy: 0.8756 - val_loss: 0.4152 - val_accuracy: 0.8244\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3203 - accuracy: 0.8722 - val_loss: 0.4027 - val_accuracy: 0.8377\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3186 - accuracy: 0.8774 - val_loss: 0.3750 - val_accuracy: 0.8932\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2851 - accuracy: 0.8866 - val_loss: 0.3463 - val_accuracy: 0.9128\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2986 - accuracy: 0.8797 - val_loss: 0.4713 - val_accuracy: 0.8129\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2864 - accuracy: 0.8887 - val_loss: 0.4101 - val_accuracy: 0.8638\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2888 - accuracy: 0.8827 - val_loss: 0.4227 - val_accuracy: 0.8336\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2852 - accuracy: 0.8866 - val_loss: 0.4094 - val_accuracy: 0.8241\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2806 - accuracy: 0.8889 - val_loss: 0.2682 - val_accuracy: 0.8964\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2839 - accuracy: 0.8870 - val_loss: 0.2684 - val_accuracy: 0.9116\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2748 - accuracy: 0.8901 - val_loss: 0.2720 - val_accuracy: 0.9136\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2662 - accuracy: 0.8951 - val_loss: 0.3082 - val_accuracy: 0.9099\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8939 - val_loss: 0.3090 - val_accuracy: 0.9121\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2698 - accuracy: 0.8900 - val_loss: 0.4320 - val_accuracy: 0.8714\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8934 - val_loss: 0.4286 - val_accuracy: 0.7958\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2704 - accuracy: 0.8900 - val_loss: 0.3083 - val_accuracy: 0.9122\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2606 - accuracy: 0.8964 - val_loss: 0.3058 - val_accuracy: 0.9056\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2641 - accuracy: 0.8935 - val_loss: 0.2801 - val_accuracy: 0.9111\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2643 - accuracy: 0.8943 - val_loss: 0.2797 - val_accuracy: 0.9133\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2573 - accuracy: 0.8981 - val_loss: 0.3598 - val_accuracy: 0.8844\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2672 - accuracy: 0.8926 - val_loss: 0.3237 - val_accuracy: 0.8944\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2558 - accuracy: 0.8982 - val_loss: 0.3253 - val_accuracy: 0.9006\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2558 - accuracy: 0.8975 - val_loss: 0.3894 - val_accuracy: 0.8983\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2564 - accuracy: 0.8973 - val_loss: 0.2439 - val_accuracy: 0.9109\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2566 - accuracy: 0.8984 - val_loss: 0.3191 - val_accuracy: 0.9070\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2643 - accuracy: 0.8950 - val_loss: 0.2753 - val_accuracy: 0.9150\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8951 - val_loss: 0.2439 - val_accuracy: 0.9023\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2693 - accuracy: 0.8942 - val_loss: 0.2688 - val_accuracy: 0.9169\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2612 - accuracy: 0.8958 - val_loss: 0.2838 - val_accuracy: 0.9129\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2527 - accuracy: 0.8986 - val_loss: 0.2717 - val_accuracy: 0.9165\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8968 - val_loss: 0.3162 - val_accuracy: 0.8702\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2519 - accuracy: 0.8970 - val_loss: 0.2456 - val_accuracy: 0.9161\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2706 - accuracy: 0.8910 - val_loss: 0.2647 - val_accuracy: 0.9115\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2504 - accuracy: 0.8975 - val_loss: 0.3058 - val_accuracy: 0.9132\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2523 - accuracy: 0.8991 - val_loss: 0.2442 - val_accuracy: 0.9212\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2532 - accuracy: 0.8993 - val_loss: 0.2855 - val_accuracy: 0.9173\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2513 - accuracy: 0.8985 - val_loss: 0.2745 - val_accuracy: 0.9154\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.8962 - val_loss: 0.3552 - val_accuracy: 0.8645\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2492 - accuracy: 0.9006 - val_loss: 0.2922 - val_accuracy: 0.8853\n",
            "693/693 [==============================] - 1s 2ms/step - loss: 0.2940 - accuracy: 0.8892\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2521 - accuracy: 0.8982\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2544 - accuracy: 0.8969\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2486 - accuracy: 0.8993\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2440 - accuracy: 0.9025\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2481 - accuracy: 0.9009\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2464 - accuracy: 0.9016\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2441 - accuracy: 0.9016\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2470 - accuracy: 0.9005\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2457 - accuracy: 0.9007\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2433 - accuracy: 0.9036\n",
            "102/102 [==============================] - 0s 2ms/step - loss: 0.3113 - accuracy: 0.8817\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4UP_0I0z8Ef",
        "outputId": "82cda488-b53a-4a6f-d0ef-d9d6166eb0a2"
      },
      "source": [
        "print(round(cv_results_top50.mean(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eu2lEupJz8Ef",
        "outputId": "9b273e5f-f626-4c21-c311-f4deb932840d"
      },
      "source": [
        "print(round(cv_results_top50.std(), 4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0144\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZAh1T15z8Ef",
        "outputId": "05595f04-838e-435a-8fbf-a97a93c62790"
      },
      "source": [
        "cv_results_top50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.9222222 , 0.91003948, 0.91499805, 0.91742343, 0.88166279])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVO-aORqz8Eg",
        "outputId": "8b88e0d4-c20c-4316-da98-24c2c29739f7"
      },
      "source": [
        "cv_top50_preds = cross_val_predict(mod_top50, X, y, cv=5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6221 - accuracy: 0.6462 - val_loss: 0.5685 - val_accuracy: 0.7164\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5633 - accuracy: 0.7062 - val_loss: 0.5826 - val_accuracy: 0.7219\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5464 - accuracy: 0.7184 - val_loss: 0.5684 - val_accuracy: 0.7233\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.5338 - accuracy: 0.7252 - val_loss: 0.5412 - val_accuracy: 0.7417\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5065 - accuracy: 0.7412 - val_loss: 0.4856 - val_accuracy: 0.7769\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4564 - accuracy: 0.7827 - val_loss: 0.4167 - val_accuracy: 0.8245\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.4054 - accuracy: 0.8232 - val_loss: 0.4384 - val_accuracy: 0.8493\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3699 - accuracy: 0.8480 - val_loss: 0.3901 - val_accuracy: 0.8851\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3635 - accuracy: 0.8531 - val_loss: 0.4116 - val_accuracy: 0.8626\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3318 - accuracy: 0.8692 - val_loss: 0.3808 - val_accuracy: 0.8955\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3321 - accuracy: 0.8695 - val_loss: 0.3676 - val_accuracy: 0.9094\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3153 - accuracy: 0.8777 - val_loss: 0.4326 - val_accuracy: 0.9039\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3113 - accuracy: 0.8764 - val_loss: 0.3233 - val_accuracy: 0.9094\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2890 - accuracy: 0.8863 - val_loss: 0.3571 - val_accuracy: 0.8965\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.3010 - accuracy: 0.8802 - val_loss: 0.3994 - val_accuracy: 0.8953\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3008 - accuracy: 0.8808 - val_loss: 0.2920 - val_accuracy: 0.9121\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2905 - accuracy: 0.8837 - val_loss: 0.2954 - val_accuracy: 0.8928\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2795 - accuracy: 0.8901 - val_loss: 0.5050 - val_accuracy: 0.7971\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2741 - accuracy: 0.8898 - val_loss: 0.2922 - val_accuracy: 0.9137\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2709 - accuracy: 0.8911 - val_loss: 0.4213 - val_accuracy: 0.9001\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2651 - accuracy: 0.8950 - val_loss: 0.4714 - val_accuracy: 0.8287\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2716 - accuracy: 0.8915 - val_loss: 0.2818 - val_accuracy: 0.9099\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8937 - val_loss: 0.2765 - val_accuracy: 0.9121\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2608 - accuracy: 0.8987 - val_loss: 0.2723 - val_accuracy: 0.8939\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8948 - val_loss: 0.2879 - val_accuracy: 0.9137\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2594 - accuracy: 0.8985 - val_loss: 0.2538 - val_accuracy: 0.9117\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2556 - accuracy: 0.8967 - val_loss: 0.2945 - val_accuracy: 0.9181\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2674 - accuracy: 0.8938 - val_loss: 0.2858 - val_accuracy: 0.9085\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2551 - accuracy: 0.8988 - val_loss: 0.2545 - val_accuracy: 0.9130\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2506 - accuracy: 0.9005 - val_loss: 0.2518 - val_accuracy: 0.9043\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2476 - accuracy: 0.9026 - val_loss: 0.3086 - val_accuracy: 0.8971\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2585 - accuracy: 0.8953 - val_loss: 0.2542 - val_accuracy: 0.8980\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2510 - accuracy: 0.9008 - val_loss: 0.2492 - val_accuracy: 0.9138\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2530 - accuracy: 0.9003 - val_loss: 0.2610 - val_accuracy: 0.9190\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2498 - accuracy: 0.9027 - val_loss: 0.2633 - val_accuracy: 0.9148\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2464 - accuracy: 0.9000 - val_loss: 0.2557 - val_accuracy: 0.9136\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2511 - accuracy: 0.9011 - val_loss: 0.2827 - val_accuracy: 0.9005\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2539 - accuracy: 0.8991 - val_loss: 0.2723 - val_accuracy: 0.9080\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2514 - accuracy: 0.8999 - val_loss: 0.2462 - val_accuracy: 0.9187\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2526 - accuracy: 0.8981 - val_loss: 0.5502 - val_accuracy: 0.7943\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2494 - accuracy: 0.9024 - val_loss: 0.3209 - val_accuracy: 0.9153\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2509 - accuracy: 0.8992 - val_loss: 0.3368 - val_accuracy: 0.9134\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2567 - accuracy: 0.8963 - val_loss: 0.3130 - val_accuracy: 0.9156\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2421 - accuracy: 0.9060 - val_loss: 0.3387 - val_accuracy: 0.9137\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2455 - accuracy: 0.9017 - val_loss: 0.2521 - val_accuracy: 0.9187\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2469 - accuracy: 0.9018 - val_loss: 0.2776 - val_accuracy: 0.9001\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2521 - accuracy: 0.9001 - val_loss: 0.3452 - val_accuracy: 0.8713\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2405 - accuracy: 0.9071 - val_loss: 0.2667 - val_accuracy: 0.9149\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2489 - accuracy: 0.9019 - val_loss: 0.2461 - val_accuracy: 0.9142\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2637 - accuracy: 0.9181\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2611 - accuracy: 0.8973\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2577 - accuracy: 0.8989\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2594 - accuracy: 0.8977\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2543 - accuracy: 0.9000\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2467 - accuracy: 0.9021\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2499 - accuracy: 0.9008\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2470 - accuracy: 0.9034\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2412 - accuracy: 0.9046\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2469 - accuracy: 0.9030\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2428 - accuracy: 0.9051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6245 - accuracy: 0.6527 - val_loss: 0.5785 - val_accuracy: 0.7206\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5616 - accuracy: 0.7082 - val_loss: 0.5573 - val_accuracy: 0.7238\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5470 - accuracy: 0.7183 - val_loss: 0.5646 - val_accuracy: 0.7312\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5280 - accuracy: 0.7289 - val_loss: 0.5216 - val_accuracy: 0.7572\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.4754 - accuracy: 0.7735 - val_loss: 0.4378 - val_accuracy: 0.8155\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.4241 - accuracy: 0.8130 - val_loss: 0.4338 - val_accuracy: 0.8279\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3913 - accuracy: 0.8348 - val_loss: 0.4290 - val_accuracy: 0.8673\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3601 - accuracy: 0.8551 - val_loss: 0.3793 - val_accuracy: 0.8842\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3408 - accuracy: 0.8647 - val_loss: 0.4108 - val_accuracy: 0.8815\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.3327 - accuracy: 0.8695 - val_loss: 0.3530 - val_accuracy: 0.8969\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.3278 - accuracy: 0.8705 - val_loss: 0.3368 - val_accuracy: 0.8952\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3089 - accuracy: 0.8747 - val_loss: 0.3145 - val_accuracy: 0.9080\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.3085 - accuracy: 0.8773 - val_loss: 0.3686 - val_accuracy: 0.8921\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2991 - accuracy: 0.8806 - val_loss: 0.3078 - val_accuracy: 0.9011\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2895 - accuracy: 0.8847 - val_loss: 0.3675 - val_accuracy: 0.8748\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.2907 - accuracy: 0.8831 - val_loss: 0.3654 - val_accuracy: 0.8908\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2811 - accuracy: 0.8892 - val_loss: 0.3353 - val_accuracy: 0.9030\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2726 - accuracy: 0.8899 - val_loss: 0.3617 - val_accuracy: 0.8809\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2682 - accuracy: 0.8904 - val_loss: 0.3843 - val_accuracy: 0.9078\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2715 - accuracy: 0.8919 - val_loss: 0.2843 - val_accuracy: 0.9130\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2770 - accuracy: 0.8884 - val_loss: 0.2356 - val_accuracy: 0.9085\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2656 - accuracy: 0.8973 - val_loss: 0.2926 - val_accuracy: 0.9076\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2644 - accuracy: 0.8948 - val_loss: 0.2818 - val_accuracy: 0.9030\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2703 - accuracy: 0.8923 - val_loss: 0.2370 - val_accuracy: 0.9001\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2655 - accuracy: 0.8932 - val_loss: 0.4386 - val_accuracy: 0.8783\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8963 - val_loss: 0.3301 - val_accuracy: 0.8475\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.2664 - accuracy: 0.8924 - val_loss: 0.2557 - val_accuracy: 0.9179\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2582 - accuracy: 0.8981 - val_loss: 0.2915 - val_accuracy: 0.9120\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2500 - accuracy: 0.9004 - val_loss: 0.3010 - val_accuracy: 0.9086\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8969 - val_loss: 0.3016 - val_accuracy: 0.9106\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2425 - accuracy: 0.9043 - val_loss: 0.2815 - val_accuracy: 0.9163\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2494 - accuracy: 0.9003 - val_loss: 0.2470 - val_accuracy: 0.9027\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2560 - accuracy: 0.8992 - val_loss: 0.2766 - val_accuracy: 0.9125\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2468 - accuracy: 0.9015 - val_loss: 0.3377 - val_accuracy: 0.8911\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2482 - accuracy: 0.9024 - val_loss: 0.3378 - val_accuracy: 0.9146\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2485 - accuracy: 0.9019 - val_loss: 0.2368 - val_accuracy: 0.9149\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2530 - accuracy: 0.8988 - val_loss: 0.2533 - val_accuracy: 0.9161\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2463 - accuracy: 0.9012 - val_loss: 0.3144 - val_accuracy: 0.8895\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2464 - accuracy: 0.9013 - val_loss: 0.3112 - val_accuracy: 0.9103\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2422 - accuracy: 0.9039 - val_loss: 0.2221 - val_accuracy: 0.9156\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2497 - accuracy: 0.9005 - val_loss: 0.2744 - val_accuracy: 0.9145\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.9001 - val_loss: 0.2442 - val_accuracy: 0.9176\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2584 - accuracy: 0.9139\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2722 - accuracy: 0.8912\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2577 - accuracy: 0.8967\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2597 - accuracy: 0.8965\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2547 - accuracy: 0.8983\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2510 - accuracy: 0.9000\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2521 - accuracy: 0.8994\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2471 - accuracy: 0.9035\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2514 - accuracy: 0.9005\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2433 - accuracy: 0.9025\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2491 - accuracy: 0.9021\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.6242 - accuracy: 0.6489 - val_loss: 0.5734 - val_accuracy: 0.7208\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5600 - accuracy: 0.7023 - val_loss: 0.5902 - val_accuracy: 0.7226\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5453 - accuracy: 0.7175 - val_loss: 0.5490 - val_accuracy: 0.7269\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5382 - accuracy: 0.7215 - val_loss: 0.5500 - val_accuracy: 0.7348\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5205 - accuracy: 0.7336 - val_loss: 0.5040 - val_accuracy: 0.7626\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4748 - accuracy: 0.7715 - val_loss: 0.4227 - val_accuracy: 0.8068\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4232 - accuracy: 0.8097 - val_loss: 0.4542 - val_accuracy: 0.8467\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3854 - accuracy: 0.8381 - val_loss: 0.3766 - val_accuracy: 0.8789\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3639 - accuracy: 0.8576 - val_loss: 0.3312 - val_accuracy: 0.8875\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3515 - accuracy: 0.8547 - val_loss: 0.3202 - val_accuracy: 0.8718\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3408 - accuracy: 0.8586 - val_loss: 0.4101 - val_accuracy: 0.8753\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3191 - accuracy: 0.8715 - val_loss: 0.3006 - val_accuracy: 0.8950\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3212 - accuracy: 0.8693 - val_loss: 0.3314 - val_accuracy: 0.9084\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3025 - accuracy: 0.8821 - val_loss: 0.3126 - val_accuracy: 0.9112\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2946 - accuracy: 0.8819 - val_loss: 0.3493 - val_accuracy: 0.9088\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2875 - accuracy: 0.8859 - val_loss: 0.3309 - val_accuracy: 0.9095\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2815 - accuracy: 0.8869 - val_loss: 0.3053 - val_accuracy: 0.9083\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2819 - accuracy: 0.8871 - val_loss: 0.2578 - val_accuracy: 0.8942\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2876 - accuracy: 0.8848 - val_loss: 0.2810 - val_accuracy: 0.8944\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2882 - accuracy: 0.8857 - val_loss: 0.2724 - val_accuracy: 0.9132\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2796 - accuracy: 0.8853 - val_loss: 0.2961 - val_accuracy: 0.8982\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2826 - accuracy: 0.8847 - val_loss: 0.2732 - val_accuracy: 0.9142\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2761 - accuracy: 0.8899 - val_loss: 0.2997 - val_accuracy: 0.8981\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2641 - accuracy: 0.8939 - val_loss: 0.2853 - val_accuracy: 0.9149\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2736 - accuracy: 0.8898 - val_loss: 0.3061 - val_accuracy: 0.9144\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2618 - accuracy: 0.8965 - val_loss: 0.3164 - val_accuracy: 0.9133\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.9001 - val_loss: 0.3764 - val_accuracy: 0.9049\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2649 - accuracy: 0.8928 - val_loss: 0.2830 - val_accuracy: 0.9161\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2518 - accuracy: 0.9008 - val_loss: 0.2250 - val_accuracy: 0.9144\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2512 - accuracy: 0.8997 - val_loss: 0.2608 - val_accuracy: 0.9024\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2718 - accuracy: 0.8931 - val_loss: 0.2806 - val_accuracy: 0.9152\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2600 - accuracy: 0.8963 - val_loss: 0.3175 - val_accuracy: 0.9133\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2534 - accuracy: 0.8996 - val_loss: 0.2563 - val_accuracy: 0.9165\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2562 - accuracy: 0.8953 - val_loss: 0.2571 - val_accuracy: 0.9194\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2501 - accuracy: 0.9010 - val_loss: 0.2835 - val_accuracy: 0.9127\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2570 - accuracy: 0.8975 - val_loss: 0.2308 - val_accuracy: 0.9111\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2497 - accuracy: 0.9015 - val_loss: 0.2701 - val_accuracy: 0.9123\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2470 - accuracy: 0.9016 - val_loss: 0.2683 - val_accuracy: 0.9163\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2532 - accuracy: 0.8977 - val_loss: 0.2990 - val_accuracy: 0.9141\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2506 - accuracy: 0.9007 - val_loss: 0.3086 - val_accuracy: 0.8989\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2445 - accuracy: 0.9037 - val_loss: 0.2825 - val_accuracy: 0.9158\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2439 - accuracy: 0.9023 - val_loss: 0.2450 - val_accuracy: 0.9184\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2404 - accuracy: 0.9037 - val_loss: 0.3257 - val_accuracy: 0.8872\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2514 - accuracy: 0.9006 - val_loss: 0.2619 - val_accuracy: 0.8961\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2358 - accuracy: 0.9070 - val_loss: 0.2454 - val_accuracy: 0.9121\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2418 - accuracy: 0.9049 - val_loss: 0.3115 - val_accuracy: 0.8434\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2451 - accuracy: 0.9004 - val_loss: 0.2882 - val_accuracy: 0.9153\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2518 - accuracy: 0.8981 - val_loss: 0.2897 - val_accuracy: 0.9113\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2385 - accuracy: 0.9053 - val_loss: 0.3094 - val_accuracy: 0.9040\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2588 - accuracy: 0.9192\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2564 - accuracy: 0.8997\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2530 - accuracy: 0.9012\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2443 - accuracy: 0.9036\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2484 - accuracy: 0.9007\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2482 - accuracy: 0.9027\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2423 - accuracy: 0.9034\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2475 - accuracy: 0.9015\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2454 - accuracy: 0.9015\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2445 - accuracy: 0.9035\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2423 - accuracy: 0.9042\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6295 - accuracy: 0.6459 - val_loss: 0.5666 - val_accuracy: 0.7189\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5560 - accuracy: 0.7054 - val_loss: 0.5608 - val_accuracy: 0.7259\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5510 - accuracy: 0.7105 - val_loss: 0.5554 - val_accuracy: 0.7307\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5361 - accuracy: 0.7225 - val_loss: 0.5461 - val_accuracy: 0.7370\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5153 - accuracy: 0.7316 - val_loss: 0.4830 - val_accuracy: 0.7745\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4671 - accuracy: 0.7703 - val_loss: 0.4639 - val_accuracy: 0.7982\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4286 - accuracy: 0.7939 - val_loss: 0.3914 - val_accuracy: 0.8302\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3931 - accuracy: 0.8291 - val_loss: 0.3848 - val_accuracy: 0.8756\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3612 - accuracy: 0.8507 - val_loss: 0.3676 - val_accuracy: 0.8779\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 2s 7ms/step - loss: 0.3407 - accuracy: 0.8638 - val_loss: 0.3712 - val_accuracy: 0.8784\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3427 - accuracy: 0.8621 - val_loss: 0.3365 - val_accuracy: 0.8932\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3214 - accuracy: 0.8724 - val_loss: 0.4269 - val_accuracy: 0.8998\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3086 - accuracy: 0.8772 - val_loss: 0.4735 - val_accuracy: 0.8812\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3086 - accuracy: 0.8751 - val_loss: 0.2911 - val_accuracy: 0.8989\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2968 - accuracy: 0.8801 - val_loss: 0.3146 - val_accuracy: 0.8888\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2946 - accuracy: 0.8831 - val_loss: 0.3084 - val_accuracy: 0.9060\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2932 - accuracy: 0.8828 - val_loss: 0.3010 - val_accuracy: 0.9084\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2751 - accuracy: 0.8906 - val_loss: 0.3293 - val_accuracy: 0.8972\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2854 - accuracy: 0.8859 - val_loss: 0.3095 - val_accuracy: 0.9105\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2805 - accuracy: 0.8879 - val_loss: 0.3140 - val_accuracy: 0.9006\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2861 - accuracy: 0.8864 - val_loss: 0.3055 - val_accuracy: 0.9126\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2806 - accuracy: 0.8887 - val_loss: 0.3975 - val_accuracy: 0.8857\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2703 - accuracy: 0.8926 - val_loss: 0.3373 - val_accuracy: 0.9007\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2692 - accuracy: 0.8934 - val_loss: 0.2572 - val_accuracy: 0.9163\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2678 - accuracy: 0.8942 - val_loss: 0.3674 - val_accuracy: 0.9117\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2638 - accuracy: 0.8967 - val_loss: 0.2878 - val_accuracy: 0.9141\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2615 - accuracy: 0.8978 - val_loss: 0.3651 - val_accuracy: 0.9080\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.8974 - val_loss: 0.3624 - val_accuracy: 0.9157\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8949 - val_loss: 0.3157 - val_accuracy: 0.9165\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2620 - accuracy: 0.8955 - val_loss: 0.2870 - val_accuracy: 0.9199\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 4ms/step - loss: 0.2612 - accuracy: 0.9006 - val_loss: 0.2875 - val_accuracy: 0.9022\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2628 - accuracy: 0.8963 - val_loss: 0.2728 - val_accuracy: 0.9086\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2466 - accuracy: 0.9023 - val_loss: 0.2799 - val_accuracy: 0.9069\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2561 - accuracy: 0.9008 - val_loss: 0.2830 - val_accuracy: 0.9179\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8958 - val_loss: 0.3600 - val_accuracy: 0.8896\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2579 - accuracy: 0.8971 - val_loss: 0.3123 - val_accuracy: 0.8926\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2536 - accuracy: 0.8998 - val_loss: 0.5433 - val_accuracy: 0.8098\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.9026 - val_loss: 0.3006 - val_accuracy: 0.9103\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2441 - accuracy: 0.9052 - val_loss: 0.2689 - val_accuracy: 0.9128\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.9022 - val_loss: 0.4183 - val_accuracy: 0.8965\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2489 - accuracy: 0.9021 - val_loss: 0.3049 - val_accuracy: 0.9168\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2404 - accuracy: 0.9085 - val_loss: 0.2148 - val_accuracy: 0.9217\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2434 - accuracy: 0.9061 - val_loss: 0.2831 - val_accuracy: 0.9156\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2537 - accuracy: 0.9005 - val_loss: 0.5655 - val_accuracy: 0.8226\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2466 - accuracy: 0.9032 - val_loss: 0.2922 - val_accuracy: 0.9121\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2409 - accuracy: 0.9066 - val_loss: 0.3515 - val_accuracy: 0.9128\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2520 - accuracy: 0.9033 - val_loss: 0.2683 - val_accuracy: 0.9214\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2516 - accuracy: 0.9033 - val_loss: 0.2683 - val_accuracy: 0.9170\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2463 - accuracy: 0.9033 - val_loss: 0.2480 - val_accuracy: 0.9139\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2404 - accuracy: 0.9063 - val_loss: 0.2493 - val_accuracy: 0.9183\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2524 - accuracy: 0.9155\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2500 - accuracy: 0.9029\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2446 - accuracy: 0.9047\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2455 - accuracy: 0.9047\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2417 - accuracy: 0.9057\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2448 - accuracy: 0.9044\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2421 - accuracy: 0.9070\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2413 - accuracy: 0.9054\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2447 - accuracy: 0.9034\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2363 - accuracy: 0.9072\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2385 - accuracy: 0.9062\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "266/266 [==============================] - 3s 6ms/step - loss: 0.6261 - accuracy: 0.6494 - val_loss: 0.5579 - val_accuracy: 0.7153\n",
            "Epoch 2/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5562 - accuracy: 0.7012 - val_loss: 0.5696 - val_accuracy: 0.7216\n",
            "Epoch 3/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5480 - accuracy: 0.7140 - val_loss: 0.5652 - val_accuracy: 0.7276\n",
            "Epoch 4/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5403 - accuracy: 0.7178 - val_loss: 0.5402 - val_accuracy: 0.7344\n",
            "Epoch 5/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.5251 - accuracy: 0.7263 - val_loss: 0.5287 - val_accuracy: 0.7451\n",
            "Epoch 6/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4846 - accuracy: 0.7575 - val_loss: 0.5061 - val_accuracy: 0.7765\n",
            "Epoch 7/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4473 - accuracy: 0.7866 - val_loss: 0.4414 - val_accuracy: 0.8064\n",
            "Epoch 8/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.4120 - accuracy: 0.8132 - val_loss: 0.4218 - val_accuracy: 0.8455\n",
            "Epoch 9/50\n",
            "266/266 [==============================] - 1s 6ms/step - loss: 0.3984 - accuracy: 0.8229 - val_loss: 0.3637 - val_accuracy: 0.8787\n",
            "Epoch 10/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3628 - accuracy: 0.8478 - val_loss: 0.3471 - val_accuracy: 0.8752\n",
            "Epoch 11/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3345 - accuracy: 0.8651 - val_loss: 0.3846 - val_accuracy: 0.8858\n",
            "Epoch 12/50\n",
            "266/266 [==============================] - 2s 6ms/step - loss: 0.3275 - accuracy: 0.8695 - val_loss: 0.3861 - val_accuracy: 0.8810\n",
            "Epoch 13/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3213 - accuracy: 0.8720 - val_loss: 0.3447 - val_accuracy: 0.8966\n",
            "Epoch 14/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3094 - accuracy: 0.8791 - val_loss: 0.3112 - val_accuracy: 0.8999\n",
            "Epoch 15/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2974 - accuracy: 0.8817 - val_loss: 0.3504 - val_accuracy: 0.8919\n",
            "Epoch 16/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.3070 - accuracy: 0.8760 - val_loss: 0.3136 - val_accuracy: 0.9044\n",
            "Epoch 17/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2954 - accuracy: 0.8828 - val_loss: 0.3235 - val_accuracy: 0.9130\n",
            "Epoch 18/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2910 - accuracy: 0.8848 - val_loss: 0.2939 - val_accuracy: 0.9162\n",
            "Epoch 19/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2848 - accuracy: 0.8859 - val_loss: 0.3087 - val_accuracy: 0.9084\n",
            "Epoch 20/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2876 - accuracy: 0.8878 - val_loss: 0.2799 - val_accuracy: 0.8931\n",
            "Epoch 21/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2828 - accuracy: 0.8882 - val_loss: 0.3097 - val_accuracy: 0.8966\n",
            "Epoch 22/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2711 - accuracy: 0.8939 - val_loss: 0.2686 - val_accuracy: 0.9117\n",
            "Epoch 23/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2669 - accuracy: 0.8941 - val_loss: 0.2938 - val_accuracy: 0.9143\n",
            "Epoch 24/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8950 - val_loss: 0.2622 - val_accuracy: 0.9150\n",
            "Epoch 25/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2684 - accuracy: 0.8935 - val_loss: 0.2723 - val_accuracy: 0.9189\n",
            "Epoch 26/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2694 - accuracy: 0.8948 - val_loss: 0.3057 - val_accuracy: 0.9067\n",
            "Epoch 27/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2678 - accuracy: 0.8956 - val_loss: 0.2720 - val_accuracy: 0.9190\n",
            "Epoch 28/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2584 - accuracy: 0.8972 - val_loss: 0.3509 - val_accuracy: 0.9146\n",
            "Epoch 29/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2555 - accuracy: 0.8997 - val_loss: 0.2429 - val_accuracy: 0.9097\n",
            "Epoch 30/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2670 - accuracy: 0.8951 - val_loss: 0.4075 - val_accuracy: 0.8989\n",
            "Epoch 31/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2693 - accuracy: 0.8950 - val_loss: 0.2704 - val_accuracy: 0.9051\n",
            "Epoch 32/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2640 - accuracy: 0.8969 - val_loss: 0.2400 - val_accuracy: 0.9143\n",
            "Epoch 33/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2553 - accuracy: 0.9022 - val_loss: 0.2635 - val_accuracy: 0.9212\n",
            "Epoch 34/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2579 - accuracy: 0.9005 - val_loss: 0.2402 - val_accuracy: 0.9208\n",
            "Epoch 35/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2593 - accuracy: 0.8987 - val_loss: 0.2400 - val_accuracy: 0.9147\n",
            "Epoch 36/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.9018 - val_loss: 0.2566 - val_accuracy: 0.9218\n",
            "Epoch 37/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2581 - accuracy: 0.8999 - val_loss: 0.2651 - val_accuracy: 0.9021\n",
            "Epoch 38/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2574 - accuracy: 0.8997 - val_loss: 0.2374 - val_accuracy: 0.9170\n",
            "Epoch 39/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2499 - accuracy: 0.9024 - val_loss: 0.2778 - val_accuracy: 0.9128\n",
            "Epoch 40/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2589 - accuracy: 0.8991 - val_loss: 0.2328 - val_accuracy: 0.9087\n",
            "Epoch 41/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2533 - accuracy: 0.9002 - val_loss: 0.2211 - val_accuracy: 0.9085\n",
            "Epoch 42/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2691 - accuracy: 0.8957 - val_loss: 0.2425 - val_accuracy: 0.9148\n",
            "Epoch 43/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2504 - accuracy: 0.9034 - val_loss: 0.2813 - val_accuracy: 0.9144\n",
            "Epoch 44/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2525 - accuracy: 0.9014 - val_loss: 0.2499 - val_accuracy: 0.9185\n",
            "Epoch 45/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2502 - accuracy: 0.9053 - val_loss: 0.2623 - val_accuracy: 0.8988\n",
            "Epoch 46/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2623 - accuracy: 0.8974 - val_loss: 0.2667 - val_accuracy: 0.9191\n",
            "Epoch 47/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2434 - accuracy: 0.9045 - val_loss: 0.2305 - val_accuracy: 0.9126\n",
            "Epoch 48/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2514 - accuracy: 0.9030 - val_loss: 0.2891 - val_accuracy: 0.9161\n",
            "Epoch 49/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2504 - accuracy: 0.9015 - val_loss: 0.2542 - val_accuracy: 0.9232\n",
            "Epoch 50/50\n",
            "266/266 [==============================] - 1s 5ms/step - loss: 0.2478 - accuracy: 0.9040 - val_loss: 0.2688 - val_accuracy: 0.9045\n",
            "693/693 [==============================] - 1s 1ms/step - loss: 0.2736 - accuracy: 0.9030\n",
            "Epoch 1/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2468 - accuracy: 0.9054\n",
            "Epoch 2/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2492 - accuracy: 0.9033\n",
            "Epoch 3/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2473 - accuracy: 0.9059\n",
            "Epoch 4/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2498 - accuracy: 0.9041\n",
            "Epoch 5/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2481 - accuracy: 0.9044\n",
            "Epoch 6/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2453 - accuracy: 0.9051\n",
            "Epoch 7/10\n",
            "406/406 [==============================] - 2s 5ms/step - loss: 0.2448 - accuracy: 0.9048\n",
            "Epoch 8/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2393 - accuracy: 0.9067\n",
            "Epoch 9/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2413 - accuracy: 0.9052\n",
            "Epoch 10/10\n",
            "406/406 [==============================] - 2s 4ms/step - loss: 0.2392 - accuracy: 0.9065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXIJKL_dz8Eg",
        "outputId": "3436956e-5f2a-4117-8c12-2f16d2cc8000"
      },
      "source": [
        "cm_top50 = confusion_matrix(y, cv_top50_preds)\n",
        "print(cm_top50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[52138  5862]\n",
            " [ 2262 28385]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}